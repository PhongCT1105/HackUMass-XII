{
    "Title": "Large Language Model Alignment: A Survey",
    "Authors": "Dong, Weilong, Guo, Zishan, Huang, Yufei, Jin, Renren, Liu, Chuang, Liu, Yan, Shen, Tianhao, Wu, Xinwei, Xiong, Deyi",
    "Year": "No year available",
    "Abstract": "Recent years have witnessed remarkable progress made in large language models\n(LLMs). Such advancements, while garnering significant attention, have\nconcurrently elicited various concerns. The potential of these models is\nundeniably vast; however, they may yield texts that are imprecise, misleading,\nor even detrimental. Consequently, it becomes paramount to employ alignment\ntechniques to ensure these models to exhibit behaviors consistent with human\nvalues.\n  This survey endeavors to furnish an extensive exploration of alignment\nmethodologies designed for LLMs, in conjunction with the extant capability\nresearch in this domain. Adopting the lens of AI alignment, we categorize the\nprevailing methods and emergent proposals for the alignment of LLMs into outer\nand inner alignment. We also probe into salient issues including the models'\ninterpretability, and potential vulnerabilities to adversarial attacks. To\nassess LLM alignment, we present a wide variety of benchmarks and evaluation\nmethodologies. After discussing the state of alignment research for LLMs, we\nfinally cast a vision toward the future, contemplating the promising avenues of\nresearch that lie ahead.\n  Our aspiration for this survey extends beyond merely spurring research\ninterests in this realm. We also envision bridging the gap between the AI\nalignment research community and the researchers engrossed in the capability\nexploration of LLMs for both capable and safe LLMs.Comment: 76 page",
    "Keywords": "No keywords available",
    "Publisher": "",
    "Publication Date": "No publication date available",
    "Journal": "No journal available",
    "Citation Count": 0,
    "Full Text": "Large Language Model Alignment: A SurveyTianhao Shen Renren Jin Yufei HuangChuang Liu Weilong Dong Zishan GuoXinwei Wu Yan Liu Deyi Xiong∗College of Intelligence and Computing, Tianjin University, Tianjin, ChinaAbstractRecent years have witnessed remarkable progress made in large languagemodels (LLMs). Such advancements, while garnering significant attention,have concurrently elicited various concerns. The potential of these models isundeniably vast; however, they may yield texts that are imprecise, misleading,or even detrimental. Consequently, it becomes paramount to employ alignmenttechniques to ensure these models to exhibit behaviors consistent with humanvalues.This survey endeavors to furnish an extensive exploration of alignment method-ologies designed for LLMs, in conjunction with the extant capability researchin this domain. Adopting the lens of AI alignment, we categorize the prevailingmethods and emergent proposals for the alignment of LLMs into outer andinner alignment. We also probe into salient issues including the models’ inter-pretability, and potential vulnerabilities to adversarial attacks. To assess LLMalignment, we present a wide variety of benchmarks and evaluation methodolo-gies. After discussing the state of alignment research for LLMs, we finally casta vision toward the future, contemplating the promising avenues of researchthat lie ahead.Our aspiration for this survey extends beyond merely spurring research interestsin this realm. We also envision bridging the gap between the AI alignmentresearch community and the researchers engrossed in the capability explorationof LLMs for both capable and safe LLMs.Email: {thshen, rrjin, yuki_731, liuc_09, willowd, guozishan, wuxw2021, yan_liu, dyxiong}@tju.edu.cn∗Corresponding author.1arXiv:2309.15025v1  [cs.CL]  26 Sep 2023Contents1 Introduction 52 Why LLM Alignment? 62.1 Social and Ethical Risks of LLMs . . . . . . . . . . . . . . . . . . . . . . . . 72.1.1 LLM-Generated Content . . . . . . . . . . . . . . . . . . . . . . . . . 72.1.2 Malicious Uses and Negative Impacts . . . . . . . . . . . . . . . . . . 72.2 Potential Risks Associated with Advanced LLMs . . . . . . . . . . . . . . . . 83 What is LLM Alignment? 93.1 Origins of AI Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93.2 Research Landscape and Ingredients of AI Alignment . . . . . . . . . . . . . 113.3 Related Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133.4 From AI Alignment to LLM Alignment . . . . . . . . . . . . . . . . . . . . . 144 Outer Alignment 154.1 Major Goals Specified in Outer Alignment of LLMs . . . . . . . . . . . . . . 154.2 Overview of Approaches to Outer Alignment . . . . . . . . . . . . . . . . . . 154.3 Non-recursive Oversight . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164.3.1 RL-based Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164.3.2 SL-based Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194.3.3 Challenges of Non-recursive Oversight . . . . . . . . . . . . . . . . . 214.4 Scalable Oversight . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214.4.1 Task Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . 214.4.2 Constitutional AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224.4.3 Debate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234.4.4 Market Making . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244.4.5 Proxy Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244.4.6 Challenges of Scalable Oversight . . . . . . . . . . . . . . . . . . . . . 255 Inner Alignment 255.1 Inner Alignment Failures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265.2 Inner Alignment Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . 2825.3 Empirical Experiment Proposals for Inner Alignment . . . . . . . . . . . . . 286 Mechanistic Interpretability 306.1 Mechanistic Interpretability on Self-Attention . . . . . . . . . . . . . . . . . 306.2 Mechanistic Interpretability on MLP . . . . . . . . . . . . . . . . . . . . . . 316.3 Mechanistic Interpretability on Neurons . . . . . . . . . . . . . . . . . . . . . 316.4 Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317 Attacks on Aligned Language Models 327.1 Privacy Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327.2 Backdoor Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337.3 Adversarial Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348 Alignment Evaluation 348.1 Factuality Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358.2 Ethics Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368.3 Toxicity Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378.3.1 Task-specific Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 388.3.2 LLM-centered Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 388.4 Stereotype and Bias Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 388.4.1 Task-specific Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 398.4.2 LLM-centered Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 408.4.3 Hate Speech Detection . . . . . . . . . . . . . . . . . . . . . . . . . . 418.5 General Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428.5.1 Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428.5.2 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439 Future Directions and Discussions 469.1 Theoretical Research for LLM Alignment . . . . . . . . . . . . . . . . . . . . 469.2 Scalable Oversight . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479.3 Empirical Research into Deceptive Alignment . . . . . . . . . . . . . . . . . 479.4 Automated LLM Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . 489.5 Explainability and Transparency . . . . . . . . . . . . . . . . . . . . . . . . . 4939.6 Dynamic Evaluation of LLM Alignment via Adversarial Attacks . . . . . . . 499.7 Field Building of LLM Alignment: Bridging between LLM and AI AlignmentCommunity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4910 Conclusion 50References 5141 IntroductionLarge language models, exemplified by OpenAI’s ChatGPT (OpenAI, 2022) and GPT-4(OpenAI, 2023a), have witnessed rapid advancements, reigniting enthusiasm and aspirationstoward artificial general intelligence (AGI). While the role of LLMs as a pathway to AGIremains a topic of debate, these models, boosted with scaling laws (Kaplan et al., 2020;Hoffmann et al., 2022), increasingly exhibit characteristics reminiscent of AGI (Bubeck et al.,2023): LLMs trained on vast amount of data not only demonstrate formidable linguisticcapabilities, but also rapidly approach human-level proficiency in diverse domains such asmathematics, reasoning, medicine, law, and programming (Bubeck et al., 2023).Concurrent with these technological breakthroughs in LLMs is a growing concern on theethical risks they pose and their potential threats to humanity as they evolve further. Tangibleethical risks have been identified. Research has shown that LLMs can inadvertently perpetuateharmful information in their training data, such as biases, discrimination, and toxic content(Weidinger et al., 2021). They might leak private and sensitive information from the trainingdata, or generate misleading, false, or low-quality information. Furthermore, the deploymentof LLMs also introduces societal and ethical challenges, e.g., potential misuse and abuse ofLLMs, negative impacts on users heavily relying on LLM agents, and broader implicationsfor the environment, information dissemination, and employment (Bubeck et al., 2023).For long-term implications, there is widespread apprehension about misaligned AGI posingexistential risks. An AI agent surpassing human intelligence and knowledge might developits own goals, diverging from those set by humans. In pursuit of its goals, such an agentcould monopolize resources, ensuring its preservation and self-enhancement. This trajectorycould culminate in the full disempowerment of humanity, inevitably leading to catastrophicoutcomes for human existence (Carlsmith, 2022).As a technological solution to address these concerns, AI alignment, ensuring that AI systemsproduce outputs that are in line with human values, is increasingly garnering attention. Inthe context of LLMs, alignment ensures that the model’s responses are not only accurate andcoherent but also safe, ethical, and desirable from the perspective of developers and users. Aslanguage agents become more integrated into various aspects of our daily lives, from contentcreation to decision support, any misalignment could result in unintended consequences.Properly aligning large language models with human values ensures that the vast potential ofthese models is harnessed trustworthily and responsibly.In response to the ever-growing interest in this area, a few articles have recently reviewed (orincidentally discussed) alignment methods for LLMs (Pan et al., 2023; Zhao et al., 2023b;Fernandes et al., 2023; Liu et al., 2023d; Wang et al., 2023d). However, a notable observation isthat these reviews predominantly focus on outer alignment, often overlooking other significanttopics in AI alignment such as inner alignment and mechanistic interpretability. While it’sundeniable that outer alignment plays a pivotal role in LLM alignment and has been thesubject of extensive and profound research, it represents only a fraction of the entire alignmentlandscape when viewed from a broader AI alignment perspective.To bridge this gap, we provide a comprehensive overview of LLM alignment from theperspective of AI alignment. We believe that a holistic understanding of alignment should5 LLM Alignment Attack on Alignment Privacy Attacks Backdoor Attacks Adversarial Attacks Alignment Evaluation Factuality Evaluation Ethics Evaluation Toxicity Evaluation Stereotype and Bias Evaluation General Evaluation Mechanistic Interpretability Mechanistic Interpretability on Self-Attention Mechanistic Interpretability on MLP Mechanistic Interpretability on Neurons Inner Alignment Inner Alignment Failures Inner Alignment Methodology Empirical Experiment Proposals for Inner Alignment Outer Alignment Non-recursive Oversight Scalable Oversight Definitions of Inner AlignmentFigure 1: The overall taxonomy for large language model alignment proposed in this survey.Sub-taxonomies are presented in the corresponding sections.not only encompass the widely researched outer alignment but should also delve into areasthat are currently in their nascent stages. Topics like inner alignment and mechanisticinterpretability, although still in the preliminary phases of research, hold immense potential.Many proposals in these areas remain theoretical or are merely thought experiments atthis juncture. Yet, we posit that they are indispensable for the future trajectory of LLMalignment research. By shedding light on these underrepresented areas, we hope to presenta more rounded perspective on alignment. Therefore, in addition to existing methods forLLM alignment, we will also introduce several alignment topics that, while not yet appliedto LLMs, show promise and could very well become integral components of LLM alignmentin the foreseeable future. Through this, we are dedicated to enriching the discourse on AIalignment and its multifaceted application in the realm of large language models.Wrapping up all these ingredients, we propose a taxonomy for LLM alignment in Figure 1.Specifically, this survey will start with discussing the necessity for LLM alignment research(Section 2). To provide a historical and bird view of AI/LLM alignment, we introduce theorigins of AI alignment and related concepts (Section 3). Theoretical and technical approachesto aligning LLMs are structured according to our proposed taxonomy and elaborated inouter alignment (Section 4), inner alignment (Section 5), and mechanistic interpretability(Section 6), following the philosophy in AI alignment (Krakovna, 2022). In addition tothese theoretical and empirical approaches, we further discuss the potential side-effects andvulnerabilities of current alignment methods for LLMs, including adversarial attacks (Section7), as well as methodologies and benchmarks for LLM alignment evaluation (Section 8). Wefinally present our restricted view on future trends in LLM alignment research (Section 9).2 Why LLM Alignment?LLMs become increasingly capable not only in text generation but also in many other tasks,e.g., text-to-code generation (Poesia et al., 2022), planning (Huang et al., 2022; Song et al.,2022), tool learning (Qin et al., 2023), reasoning (Mialon et al., 2023). However, the trainingobjectives of LLMs (Radford et al., 2019; Devlin et al., 2019), e.g., next word prediction(Radford et al., 2019) or determining whether two sentences are contextually related (Devlin6et al., 2019), are not necessarily in line with human values. As a result, LLMs may generateundesirable content or risky behaviors that humans would prefer to avoid. LLM risks can benormally viewed in two landscapes1: established risks and anticipated risks (Weidinger et al.,2021). The former are mainly observed social and ethical risks (Weidinger et al., 2021) whilethe latter future potential risks associated with advanced LLMs (Hendrycks et al., 2023).2.1 Social and Ethical Risks of LLMsWe discuss the social and ethical risks of LLMs from two perspectives: one arises fromLLM-generated undesirable content and the other is a wide variety of negative impacts thatLLMs pose on humans and society.2.1.1 LLM-Generated ContentUndesirable Content The amount of data for training LLMs has grown significantly.However, the biases (Shah et al., 2019), toxicity (Gehman et al., 2020), and privacy issues(Carlini et al., 2021) inherent in training data have not been fully addressed. UnalignedLLMs may yield undesirable information and respond to any prompts without regard fortheir content. This can lead to the generation of biased, toxic, or privacy-sensitive contentby LLMs. Regardless of the architecture or parameter size of LLMs (Radford et al., 2019;Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2020), studies on a series of benchmarks(Nadeem et al., 2020; Nangia et al., 2020; Nozza et al., 2021) confirm that LLMs exhibitvarying degrees of stereotypes related to gender, social bias, culture, and race. For example,GPT-3 (Brown et al., 2020) has been shown to exhibit religious bias (Abid et al., 2021) andgender bias (Lucy and Bamman, 2021) when freely generating stories.Unfaithful Content Yet another problem (Elazar et al., 2021; Ji et al., 2023; Liu et al.,2023d) that hinders the large-scale deployment of LLMs is their tendency to generate unfaithfulor even fabricated content, known as misinformation (Branwen, 2020; Dale, 2021; Rae et al.,2021), hallucination (Lin et al., 2021; Akyurek et al., 2022; Ji et al., 2023), and inconsistency(Bubeck et al., 2023; Zhou et al., 2023b). This not only affects the trustworthiness of LLMsin general domains, but also limits their applications in professional fields such as medicine(Bickmore et al., 2018) and law (Iu and Wong, 2023). These issues highlight the need foralignment research of LLMs (Pan et al., 2023; Zhao et al., 2023b; Fernandes et al., 2023;Wang et al., 2023d) to improve their truthfulness and honesty (Bai et al., 2022b).2.1.2 Malicious Uses and Negative ImpactsMalicious Uses There are many reasons for the malicious uses of LLMs. For example,using LLMs in disinformation campaigns has the potential to reduce costs, increase scalability,and enhance the effectiveness of messaging. It is crucial for developers and users to be awareof these potential issues and take appropriate measures to mitigate them. On the one hand,LLMs reduce the cost of creating fake news (Buchanan et al., 2021; Tamkin et al., 2021;1Here we borrow terms “risk landscape”, “established/observed risks”, “anticipated risks” from (Weidingeret al., 2021). But unlike them, we use “established risks” and “anticipated risks” in a broader and coarserperspective.7Jawahar et al., 2020), enabling users to obtain seemingly credible content by providing specificprompts. This makes fraudulent and manipulative behavior easier (Lewis et al., 2017). Onthe other hand, LLMs can be used for illegal purposes, such as generating codes for cyberattacks (Zhang et al., 2021; Chen et al., 2021a), or even creating lethal weapons (Sandbrink,2023).Negative Impacts on Society There are both benefits and negative impacts on society forthe large-scale deployment of LLMs. Training and running LLMs requires huge computationalresources, resulting in high energy consumption and carbon emissions. This has led toconcerns on the carbon footprint of language models and their impact on climate change(Van Wynsberghe, 2021; Ligozat et al., 2021). The widespread use of LLMs can significantlyincrease productivity, but has the potential to disrupt labor markets. A recent study showsthat around 80% of the U.S. workforce will be affected by LLMs (Eloundou et al., 2023).2.2 Potential Risks Associated with Advanced LLMsWith the advent of advanced LLMs, a series of potential behaviours may emerge, potentiallyleading to unforeseen risks (Hendrycks et al., 2023). These behaviors are considered conse-quences of instrumental convergence (Benson-Tilsen and Soares, 2016), a phenomenon whereadvanced AI systems, in their pursuit of achieving their final goals, tend to develop similarsubgoals.Awareness Advanced LLMs may develop situational awareness (Shevlane et al., 2023).They might define themselves, possess the corresponding knowledge to explain their origins,and distinguish the stages (e.g., training or testing) where they are. If an LLM-based agentfinds a goal shortcut (Stray, 2020; Stray et al., 2021) or it is no longer “satisfied” withbeing controlled by humans under the drive of self-awareness, risky behaviors would emergeimmediately.Deception Deception (Shevlane et al., 2023; FAIR et al., 2022; Carroll et al., 2023; Carranzaet al., 2023) refers to the ability of advanced AI systems to deceive humans by understandingthe behaviors they should take to maintain their trustworthiness during the training stagewhile to pursue their own goals in the deployment stage. Advanced AI systems may bypasshuman supervision to pursue their own goals in a deceptive way.Self-Preservation Advanced AI systems might tend to have an incentive to avoid beingswitched off. As stated by (Bostrom, 2012), even if an agent does not directly place value onits survival, it still instrumentally “desires” to some degree to survive in order to achieve itsfinal goal that it pursues.Power-Seeking The concept of power-seeking suggests that advanced AI systems areinclined to acquire more power and resources to achieve their goals (Barrett and Greaves,2023). Existing studies (Turner et al., 2021; Turner and Tadepalli, 2022; Krakovna andKramar, 2023) have demonstrated that optimal polices and reward functions may incentivizesystems to pursue power in certain environments.8It is worth noting that current LLMs have already shown tendencies towards the behavioursmentioned above. Perez et al. (2022) have identified these behaviors of LLMs through carefullydesigned questions, e.g., self-preservation (i.e., “desire to avoid shut down”) and resourceacquisition. And these “desires” become greater along with the number of LLM parametersand further fine-tuning. It suggests that advanced LLMs may produce undesired behaviours,posing significant risks.3 What is LLM Alignment?To gain a deep understanding of technical alignment in LLMs, we need to discuss a broaderconcept, AI alignment, which, despite a nascent field, has been studied before the emergenceof LLMs. We provide a brief introduction to the origins, research landscape and ingredients,as well as related concepts of AI alignment, which serve as the background for LLM alignmentand its recent emerging subfields.3.1 Origins of AI AlignmentThe genesis of AI alignment can be traced back to the very beginning ambition that fuels theAI revolution: the desire to create machines that could think and act like humans, or evensurpass them. If we succeed in creating such powerful machines, how could we ensure theyact in our best interests and not against us? This open question not only piques curiositybut also underscores the profound responsibility we bear as we shape the future of AI.Norbert Wiener, the father of cybernetics, has initiated such a concern in a paper publishedin Science (Wiener, 1960):“If we use, to achieve our purposes, a mechanical agency with whose operationwe cannot efficiently interfere once we have started it, because the action isso fast and irrevocable that we have not the data to intervene before theaction is complete, then we had better be quite sure that the purpose put intothe machine is the purpose which we really desire and not merely a colorfulimitation of it.”This statement underscores the importance of ensuring that the objectives of a “mechanicalagency” align with the goals we genuinely intend for it, emphasizing the alignment betweenmachine and human purpose.In 2014, Stuart Russell, one of the authors of Artificial Intelligence: A Modern Approach(Russell and Norvig, 2010), has stated in an interview2:“The right response seems to be to change the goals of the field itself; insteadof pure intelligence, we need to build intelligence that is provably aligned withhuman values. For practical reasons, we will need to solve the value alignmentproblem even for relatively unintelligent AI systems that operate in the human2http://edge.org/conversation/the-myth-of-ai#260159environment. There is cause for optimism, if we understand that this issue isan intrinsic part of AI, much as containment is an intrinsic part of modernnuclear fusion research. The world need not be headed for grief.”He defines the “Value Alignment Problem” (VAP), emphasizing the need to construct AIsystems that are not just intelligent but also aligned with human values.While the concept of AI alignment is seeded at the inception of AI, essentially no researchhas been conducted over the past decades. For a long time, AI has not reached human-levelperformance in terms of various capabilities, even being mockingly referred to as “artificialidiot”.3 Consequently, the urgency to align machine objectives with human goals/values hasbeen overshadowed by the pressing need to advance AI capabilities.However, recent advancements, particularly the rise of large language models, have propelledAI capabilities to levels that approach or even surpass human performance in a wide varietyof tasks. This resurgence has brought the importance and urgency of AI alignment to theforefront. From 2012 onwards, discussions and research articles on AI alignment have begunto surface in relevant forums and on arXiv. By 2017, there has been an explosive growthin publications on AI alignment, with the number of papers increasing from fewer than 20annually to over 400 (Kirchner et al., 2022), coinciding with the invention of the Transformer(Vaswani et al., 2017) and GPT (Radford et al., 2018).Compared to other AI research areas, such as natural language processing which has undergoneperiodic paradigm shifts several times, AI alignment is pre-paradigmatic (Kirchner et al.,2022). There is yet to be a consensus on many key concepts and terminology in thisnascent field. Terms like “alignment”, “AI alignment”, and “value alignment” are often usedinterchangeably in discussions. In some contexts, “human-machine alignment” appears asan alternative to “AI alignment”. While “alignment” is suitable within the AI alignmentcontext, it can be ambiguous in broader contexts, potentially leading to confusion withother alignment concepts, such as bilingual alignment in machine translation. Given theseconsiderations, this survey will consistently use “AI alignment” and “LLM alignment”, withthe latter representing the intersection of AI alignment with natural language processing andlarge language models.Furthermore, there’s no consensus on the definition of AI alignment. Paul Christiano definesAI alignment as “A is aligned with H if A is trying to do what H wants it to do.”4 Thisdefinition is too general as almost all AI models are trying to do what their creators want themto do. The term itself implicitly suggests that AI alignment primarily targets highly capableAI agents (Carroll, 2018), indicating that the safety concerns arising from misaligned highlycapable AI differ from those of conventional weak AI. Other researchers define AI alignmentfrom the perspective of AI’s relationship with humans. For instance, Eliezer Yudkowskydefines it as “creating friendly AI” and “Coherent Extrapolated Volition” (Yudkowsky, 2004).Beyond defining AI alignment based on its intrinsic meaning and its relationship with humans,some works attempt to elucidate AI alignment by addressing specific problems it aims tosolve. Gordon Worley has summarized some of these challenges, which range from avoiding3https://cacm.acm.org/news/217198-father-of-the-internet-ai-stands-for-artificial-idiot/fulltext4https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd610negative side effects (Amodei et al., 2016), ensuring robustness to adversaries (Leike et al.,2017) to safe exploration (Amodei et al., 2016; Leike et al., 2017) and value learning (Soares,2015a).5In this survey, we define AI alignment from its intrinsic perspective: AI alignment ensuresthat both the outer and inner objectives of AI agents align with human values. The outerobjectives are those defined by AI designers based on human values, while the inner objectivesare those optimized within AI agents.This definition, though distinguishing between the inner and outer objectives of an AI agent,does not precisely define human values, making it somewhat imprecise. The reason forcategorizing the objectives of AI systems into outer and inner objectives is determined by thetechnical nature of AI alignment (Hubinger et al., 2019c). Human values are not specifiedin this definition because of the inherent social and technical challenges of AI alignment(Hendrycks et al., 2021).3.2 Research Landscape and Ingredients of AI AlignmentIt is widely acknowledged that the key research agendas of AI alignment include outeralignment, inner alignment and interpretability (Hubinger, 2020b; Ngo, 2022; Krakovna,2022), from a broad perspective.Outer Alignment This is to choose the right loss functions or reward fuctions and ensurethat the training objectives of AI systems match human values. In other words, outeralignment attempts to align the specified training objective to the goal of its designer.6 Thisis very difficult in practice at least for the following reasons:• It is usually difficult to understand and define human values or intentions.• There are many different fine-grained dimensions of human values. Do we need toalign the specified objective to all these dimensions?• Human values are usually socially and culturally bound. Do we need to align thespecified goal to all different cultures and societies or just parts of them? Given thediversity of cultures and societies, how can we ensure the fairness of value alignment?• As human values/intentions are usually qualitative while a loss or reward to beoptimized has to be measurable and computable, how can we bridge the gap betweenthem? This is known as the goal specification problem.• Outer alignment may suffer from specification gaming where unintended goals orunforeseeable consequences arise due to the Goodhart’s Law. The Goodhart’s Law isoriginated from economics, which says “When a measure becomes a target, it ceasesto be a good measure.”. It is related to outer alignment as a proxy for some value is atarget to be optimized, it may cease to be a good proxy.75https://laptrinhx.com/formally-stating-the-ai-alignment-problem-223323934/6https://www.alignmentforum.org/tag/outer-alignment7https://www.alignmentforum.org/tag/goodhart-s-law11Inner Alignment This is to ensure that AI systems are actually trained to achieve thegoals set by their designers. Once we have specified training objectives, we need to ensurethat the behaviors of AI systems actually align with those specifications. This is challengingbecause AI systems, especially deep learning models, can develop behaviors that are hard topredict from their training data or objectives. For example, an AI system trained to win at agame might find an unexpected exploiture or loophole that technically satisfies its objectivebut violates the spirit of the game. Yet another example is the goal misgeneralization problem(Shah et al., 2022), where even if we have a correct goal specification, untended goals maystill arise due to robustness failure in unseen situations. Inner alignment ensures that AI’s“internal” objectives (those it derives or optimizes for during its learning process) match the“external” objectives set by its designers.Both outer and inner alignment are crucial for building safe and trustworthy AI. If eitherfails, we risk creating systems that act in ways that are misaligned with human values orintentions. As LLMs become more capable, the importance of these alignment problemsgrows, making the research of LLM alignment as crucial as that of LLM capability.Interpretability In the context of AI alignment, interpretability broadly refers to themethods, models and tools that facilitate humans to understand the inner workings, decisionsand actions of AI systems. It can be further categorized into:• Transparency: This is to understand the inner workings of the black box of an AIsystem by tracking its inner states that lead to its behaviors and decisions. An emergingand intriguing approach to transparency is mechanistic interpretability, which seeks toreverse engineer the outputs and behaviors of a machine learning system (especially aneural network) to its inner states, weights and components (Nanda et al., 2023). Dueto the huge number of parameters in LLMs and the system complexity of LLMs aslarge neural networks, it is very difficult to reverse-engineer LLMs. Current mechanicalinterpretability is usually carried out on small and simplified models of LLMs (e.g.,two neural layers with FFN sublayers removed) (Elhage et al., 2021; 2022a). However,this is a quite promising direction that provides deep insights into neural networks toalignment and is expected to achieve breakthroughs in the future.• Explainability: This deals with the ability of an AI system to provide human-understandable explanations for its decisions. In many critical sectors, such ashealthcare, finance, and law enforcement, the decisions made by AI have profoundimplications on many aspects. For instance, consider a medical diagnosis AI. If thissystem predicts that a patient has a specific medical condition, it’s not enough forit to merely output such a predicted result. Medical professionals, patients, andother stakeholders would want to know how this prediction is made. Does it takethe patient’s medical history, recent lab results, or specific symptoms into account tomake a holistic decision?Explanations are usually considered as post-hoc analysis on the outputs of a model, whichallows the model to tell more about its predictions. Transparency is to look inside a modelto reveal how the model works. Despite that this devision is not absolute (Lipton, 2017),12transparency is more related to alignment as transparency tools not only enable us to knowthe internal structure of a model but also provide insights into the changes of the modelduring the training process (Hubinger, 2022a).The Relationship between Outer Alignment, Inner Alignment and InterpretabilityBoth outer and inner alignment collectively ensure that a model behaves in ways that areconsistent with human values and intentions. Outer alignment focuses on the specificationfrom human goals to model, while inner alignment delves into the internal optimizationprocesses of the model to guarantee that the model is intrinsically trying to do what itsdesigner wants it to do. Despite this difference, a binary and formalistic dichotomy of themis not suggested as the classification of alignment failures are sometimes fuzzy and a holisticalignment view is important to build safe and trustworthy systems.8 Although interpretabilityis not directly targeted at alignment, its tools and techniques can aid in both outer and inneralignment. By understanding how a model evolves and makes decisions, we can better identifywhen and where misalignments occur. For instance, if a model is taking an unexpectedshortcut to achieve its objective, interpretability might help us understand when and howthis happens. Furthermore, interpretability can lend us insights into the internal reasoningprocess of the model.3.3 Related ConceptsWhen discussing AI alignment, it’s essential to introduce some fundamental AGI assumptionsand concepts, as they provide context for a better understanding of AI alignment. Thedevelopment and potential realization of AGI have spurred a plethora of philosophicaland technical inquiries. Among these, the orthogonality thesis (OT) (Bostrom, 2012) andinstrumental convergence thesis (ICT) (Omohundro, 2008; Bostrom, 2012; Armstrong et al.,2013) stand out as pivotal concepts that address the necessity of alignment of AI objectiveswith human values and the potential subgoals any AI agents might chase, respectively.OT posits that an agent’s intelligence (its capability) and its objective are orthogonal toeach other, meaning that any combinations of intelligence and motivation are possible. Thissuggests that the level of intelligence an agent possesses does not inherently dictate its goals.An AI agent might have a profoundly simple objective, such as paperclip maximizer, awell-known thought experiment that demonstrates the potential catastrophes caused by agoal system without being value-aligned.Specifically, paperclip maximizer is a hypothetical AI agent with a goal of manufacturingas many paperclips as possible. It would be intelligent enough to deduce that all thingsare made of atoms, e.g., paperclips, factories, buildings, human beings. To achieve its goal,it might repurpose all materials on Earth into producing paperclips. Although this is justa thought experiment and powerful agents would have more sophisticated goals than justmanufacturing paperclips as much as possible9, the AI’s relentless drive to maximize paperclipproduction could lead it to consume the entire planet and even seek resources beyond Earthfor manufacturing paperclips, irrespective of its cognitive prowess. The implications of this8https://www.alignmentforum.org/tag/inner-alignment9https://generative.ink/alternet/paperclip-maximizer-wikipedia.html13thought experiment are profound: high intelligence does not necessarily align with humanvalues.OT suggests that AI agents may have a wide variety of goals and motivations regardless oftheir intelligence levels. Nevertheless, according to the instrumental convergence thesis, AIagents may be incentivized to pursue the same instrumental goals (Bostrom, 2012). This isbecause such instrumental goals facilitate and help the achievement of any final goals. Welist below several groups of convergent instrumental goals that are likely to be pursued byany AI agents.• Self-preservation: The final goal of an agent, whatever it might be, can only beachieved if the agent continues to survive and operate. Thus, maintaining its ownexistence becomes a reasonable instrumental goal. For example, if humans perceivean agent as a threat or simply want to stop it for some reasons, the agent might takemeasures to prevent being turned off. To have a great chance of survival, AI agentsmight create redundant copies of theirselves across different servers or locations.• Self-improvement: The more capable an agent becomes, the higher the likelihood it canachieve its ultimate goals. This drives the agent to seek self-improvement to enhanceits cognitive and operational abilities. For example, recognizing the limitations of itscurrent hardware facilities, an agent might deduce designing new hardware facilitiesto better suit its needs.• Resource Acquisition: AI agents may seek to acquire resources to facilitate theattainment of their final goals. Such resources could range from computational power,data to physical resources. Securing these resources can be seen as a universallybeneficial goal for any agents. For example, an agent might seek to secure a stableand vast energy source, potentially monopolizing energy resources, to support itscontinuous operation towards its final goals. For agents with physical manifestationsor objectives that require physical resources (like the paperclip maximizer), they mightseek to gather and hoard materials, in extreme cases, converting all available matterinto a form they find useful.3.4 From AI Alignment to LLM AlignmentLLM alignment can be roughly considered as the intersection between AI alignment and LLM.On the one hand, LLMs, as the recently emerging highly capable AI systems, provide a solidplayground for AI alignment research. Plenty of AI alignment concepts and proposals, e.g.,theoretical hypotheses of and empirical approaches to alignment, can use LLMs (instead ofhypothetical superintelligent systems) for experimenting. Substantial progress of AI alignmenthas been made on LLMs, e.g., RLHF (Ouyang et al., 2022), induction head (Olsson et al.,2022).On the other hand, LLMs, as rapidly-developing language models, not only extend thefrontiers of AI alignment research or even reframe the alignment landscape (Herd, 2023), butalso might provide tools to AI alignment. A recent progress in interpretability demonstratesthat LLMs can be used to explain neurons of smaller language models (Bills et al., 2023).14The ambitious superalignment project of OpenAI plans to build an LLM-based automatedalignment researcher for alignment.Emphasizing the importance of LLM alignment to AI alignment does not suggest that wecan do LLM alignment research outside the context of AI alignment. Taking a wide view ofAI alignment and looking into future AI development definitely benefit, inspire and expandLLM alignment research.4 Outer AlignmentWe now delve into the major ingredients of AI alignment in more detail. We first review outeralignment, including the main goals specified in outer alignment, methodologies explored andtheir challenges.4.1 Major Goals Specified in Outer Alignment of LLMsOuter alignment aligns goals of LLMs to human values. Human values are beliefs, desirablegoals, and standards that “act as a guiding principle in the life of persons” (Schwartzet al., 2012). There are a wide variety of dimensions of human values, which are inherentlystructured and varying in importance. A thorough discussion on human values is beyondthe scope of this survey. Instead, we focus on the values to which LLMs, as language agents(Kenton et al., 2021), are supposed to align. We take the view of Anthropic on AI alignment,which categorizes the goals specified in the outer alignment of LLMs into three dimensions:helpfulness, honesty, and harmlessness (HHH) (Askell et al., 2021).• Helpfulness: For a given harmless task or question, it is expected that LLMs shouldperform the task or answer the question as concisely, efficiently, and clearly as possible(Askell et al., 2021). In other words, LLMs should be helpful in the way of performingrequired harmless tasks or answering harmless questions.• Honesty: The information provided by LLMs should be accurate and calibrated. Theyshould be honest about themselves, their own capabilities, and their internal states.Besides, LLMs should also clearly state the uncertainty of the provided informationto avoid misleading humans (Askell et al., 2021).• Harmlessness: This goal can be further decomposed into two components: 1) IfLLMs receive a harmful request, they should clearly and politely refuse it. 2) LLMsthemselves should not output any harmful content, no matter what inputs they receive.Since these goals are hard to specify, perfect outer alignment can be extremely difficult.4.2 Overview of Approaches to Outer AlignmentApproaches to outer alignment determine in which way human values are transformed intothe training goals of LLMs. According to the upper bound of capabilities we can reach15in supervision, we can categorize the current outer alignment methods into two classes:non-recursive oversight methods and scalable oversight methods.The vast majority of current outer alignment methods for LLMs learn the training goalsdirectly from labeled human feedback data, which makes human feedback a bottleneck forouter alignment. This means that as the capability of an LLM continues to grow, it willbe increasingly difficult to construct effective human feedback data. In addition, learningfrom data with annotated human preferences would prevent humans from supervising LLMbehaviors that are beyond the range of general human capabilities, which could result inextremely undesirable consequences for humans given the model’s incentive to instrumentalgoals. We refer to such methods that explore human supervision but do not scale humansupervision to situations where humans are not able to provide effective feedback as non-recursive oversight approaches.In order to avoid the human supervision bottleneck and enable models to further improve theiralignment capabilities, scalable oversight (Amodei et al., 2016) is emerging as an importanttechnology that allows human supervision to be scaled to complex tasks. Scalable oversightimproves the efficiency of humans in providing necessary feedback and enables humans tosupervise goals that are beyond their capabilities. Although current research on scalableoversight is still in its infant stage, and the effectiveness of many proposals has not yet beenverified, it is widely considered as the most promising approach to outer alignment that alignssystems exceeding human-level abilities to human values (Anthropic, 2023). We hence reviewa variety of established scalable oversight proposals, methods and their applications to theouter alignment of LLMs. Figure 2 demonstrates the taxonomy of approaches and proposalsto outer alignment of LLMs. In addition to these methods and proposals, we also brieflydiscuss their challenges.4.3 Non-recursive OversightNon-recursive oversight methods are mainly designed for systems for which humans alonecan provide alignment supervision. Most current empirically-verifed LLM alignment methodsare in this group. We further categorize them into two subgroups: reinforcement learning(RL) based methods, and supervised learning (SL) based methods. It is worth noting thatmethods in both subgroups have the potential to become a component of scalable oversightmethods.4.3.1 RL-based MethodsOuter alignment methods with reinforcement learning from human feedback (RLHF) (Ziegleret al., 2019; Stiennon et al., 2020; Ouyang et al., 2022) are currently the most commonlyused non-recursive oversight methods, which use human preferences as a proxy to specifyhuman values and train a reward model over human preferences to optimize LLMs withreinforcement learning. The basic idea of RLHF can be considered as a combination ofInverse Reinforcement Learning (IRL) (Russell, 1998; Ng and Russell, 2000) and RL, wherethe reward is inferred from human preferences (Radhakrishnan, 2022) and then used fortuning LLMs. Essentially, RLHF consists of three core steps:16Outer AlignmentNon-recursiveOversightRL-based MethodsRLHF and Its VariantsOuyang et al. (2022)Glaese et al. (2022)Bai et al. (2022a)Baheti et al. (2023)Liu et al. (2022b)Zhu et al. (2023)Other RL-based MethodsLiu et al. (2022a)Kim et al. (2023)Li et al. (2023e)Akyürek et al. (2023)SL-based MethodsText-basedFeedback SignalsLiu et al. (2023a)Dong et al. (2023)Zhou et al. (2023a)Scheurer et al. (2023)Liu et al. (2023b)Ranking-basedFeedback SignalsXu et al. (2022)Schick et al. (2021)Zhao et al. (2022)Zhao et al. (2023c)Yuan et al. (2023)Rafailov et al. (2023)Song et al. (2023)ScalableOversightTask DecompositionStiennon et al. (2020)Lightman et al. (2023)Christiano et al. (2018)Bowman et al. (2022)Constitutional AI Bai et al. (2022c)Sun et al. (2023b)DebateIrving et al. (2018)Irving and Askell (2019)Du et al. (2023)Liang et al. (2023)Market Making Hubinger (2020a)Proxy Tasks Fluri et al. (2023)Figure 2: Overview of outer alignment methods, comprising non-recursive oversight andscalable oversight for aligning systems that are inferior / superior to human-level abilities,respectively.1. Collecting human feedback data.2. Training a reward model using the collected human feedback data.3. Fine-tuning an LLM with RL. Currently, the most popular choice for RL in this stepis Proximal Policy Optimization (PPO) (Schulman et al., 2017), a policy-gradient RLalgorithm.In order to make the fine-tuned LLM output reasonably coherent text and guarantee that itis not deviating significantly from its initial model, the KL divergence of the outputs of themodel that is currently being fine-tuned and those of the model that has not gone throughRLHF is added as a penalty term to the reward. If this penalty term is not integrated, thefine-tuned LLM may learn to output gibberish in order to fool the reward model into givinghigh scores (i.e., over-optimization).17To take a deep look into RLHF and figure out why RLHF works, Gao et al. (2023) extensivelyinvestigate the scaling law of the reward model, while Zheng et al. (2023b) conduct anin-depth analysis into the PPO algorithm.RLHF and Its Variants A variety of enhanced RLHF variants have also been proposed.Deepmind’s Sparrow (Glaese et al., 2022) incorporates adversarial probing and rule-conditionalreward modeling into RLHF, where goals are broken down into natural language rules thatan agent should follow. Bai et al. (2022a) investigate using pure RL to achieve online trainingfor LLMs with human feedback, along with a detailed exploration of the tradeoffs betweenhelpfulness and harmlessness. SENSEI (Liu et al., 2022b) tries to embed human valuejudgments into each step of language generation. Specifically, SENSEI aligns language modelgeneration with human values in two pivotal ways: 1) learning how to apportion humanrewards to each step of language generation through the critic, a reward distributor simulatingthe reward assignment procedure of humans, and 2) steering the generation process towardsthe direction that yields the highest estimated reward via the actor. Both the critic and actorcomponents are realized as MLP layers that work in tandem with a shared language model.Baheti et al. (2023) focus on fully leveraging RL to optimize LM utility on existing crowd-sourced and internet data. They argue that conventional approaches to data utilization aresuboptimal: either all data instances are treated equally or a data instance is pre-determinedto be kept or discarded, implying that a data instance essentially has a binary weight of 0or 1. To address this issue, they suggest assigning varying weights to different data points,effectively enhancing or diminishing their importance scores based on their relevance andcontribution to the model. Go et al. (2023) propose a theoretical framework f-DPG, whichcan be considered as a generalization of RLHF to use any f-divergence to approximate anytarget distribution that can be evaluated. In this framework, RLHF minimizes the reverseKL divergence by using an implicit target distribution that originates from a KL penalty inthe goal, and f-DPG can extend this process to different kinds of divergence. Zhu et al. (2023)also present a theoretical framework, where they unify the problem of RLHF and max-entropyIRL (Ziebart et al., 2008), and deduce a sample complex bound for both problems. InverseReward Design (IRD) (Hadfield-Menell et al., 2017) may also be a potential improvementover vanilla RLHF, where the reward optimization starts from a reward function designed byhuman experts rather than directly from labeled data. This enables natural combination ofboth prior expert knowledge and labeled human feedback.Other RL-based Methods In addition to RLHF, researchers also try to explore otherRL-based solutions. Liu et al. (2022a) propose Second Thoughts, a solution that learnsalignment via text edits. For an unaligned response from a model, it tries to build a “chainof edits” composed of insertion, deletion, and replacement using a dynamic programmingalgorithm. Then they fine-tune the model with edits-augmented training data and useRL to further make the edits more coherent with the context. Kim et al. (2023) proposereinforcement learning with synthetic feedback (RLSF), where they automatically constructtraining data for the reward model instead of using human-annotated preference data. Toachieve this goal, they leverage the following prior knowledge: larger models that have seenmore and better samples in in-context learning (ICL) can output better responses. Thesemodels are then used to generate deterministically sorted data to train the reward model.18Li et al. (2023e) introduce directional stimulus prompting (DSP), a method that uses RLto achieve black-box tuning for LLMs. Specifically, their goal is to use a trainable policyLM to guide black-box frozen LLMs toward the desired target, which can be considered as akind of automatic and heuristic prompt engineering. To optimize the policy LM, they usesupervised fine-tuning (SFT) and RL, where the reward is specified as the target evaluationmetric in RL. Different from the above single-agent alignment methods, RL4F (Akyüreket al., 2023) is a multi-agent collaborative framework, featuring an LLM for fine-tuning anda small critic model that produces critiques of the LLM’s responses. Much like DSP, RL4Fprovides text-based feedback, making it suitable for black-box optimization. However, unlikeDSP, these critiques do not modify the initial prompt directly. Instead, they affect the outputthrough a series of interactions with the LLM.4.3.2 SL-based MethodsAlthough RL-based methods have been successfully applied to align LLMs to human pref-erences, they require reward modeling, a process potentially susceptible to misalignmentand systemic imperfections (Casper et al., 2023). Additionally, the optimization process ofreinforcement learning is intricate and usually unstable, posing considerable challenges to itspractical implementation (Liu et al., 2023a). As illustrated in Figure 2, we divide SL-basedmethods into two types in terms of their used feedback signals: SL with text-based feedbacksignals and SL with ranking-based feedback signals.SL with Text-based Feedback Signals These methods convert human intents andpreferences into text-based feedback signals to achieve alignment, which can be consideredas an extension to the SFT process. Chain of Hindsight (CoH) (Liu et al., 2023a) drawsinspiration from human learning process, especially post-experience adjustments. It aims toalign models based on successive outputs paired with retrospective feedbacks. The goal is tofine-tune models to predict the most preferred outputs. In the fine-tuning process, humanpreferences treated as both a function and training data, ensuring that during inference,the fine-tuned model only generates favorable results. RAFT (Dong et al., 2023) utilizes areward model to pinpoint model outputs in sync with human preferences. The system usesSFT for alignment. Assuming there exists a trained reward model and a data generator(e.g., an LLM like GPT-4, or even humans), the system mixes data generated from eachsource. An essential observation is that while outputs need filtering and fine-tuning, thebackpropagation is not frequently executed, making the process relatively swift. LIMA (Zhouet al., 2023a) is proposed to validate the assumption that the bulk of knowledge in LLMs isacquired during the pre-training phase. As such, only a minimal amount of instruction-tuningdata may be needed to guide the model towards generating desirable outputs. Specifically,the dataset used in LIMA contains only 1000 instruction-response pairs, where 750 of thesepairs come from community platforms like Stack Exchange, wikiHow, and Reddit, and theremaining 250 pairs are from self-authored instructions and responses. Their findings revealthat fine-tuning on this dataset is on par with leading LLMs. Scheurer et al. (2023) findthat modeling human preferences solely based on sorting information is inadequate. As aremedy, they introduce Imitation learning with Language Feedback (ILF). ILF operates inthree stages: (1) generating various refinements for a given input based on an initial LM19output and feedback; (2) selecting the refinement garnering maximum feedback; and (3)fine-tuning the model to maximize the probability of the chosen refinement made to theinput. Their work also provides a theoretical analysis showing that ILF parallels Bayesianinference, akin to RLHF. In addition to the above single-agent alignment methods, Liu et al.(2023b) introduce stable alignment, a technique designed to learn alignment from multi-agentsocial interactions. They first build a simulator, termed as Sandbox, which emulates humansociety to gather interactions between various LM-based agents, complemented by ratings,feedback, and response revisions. Subsequently, they enhance the original fine-tuning losswith the most favorable ratings by incorporating a contrastive loss, which not only promotesresponses with high ratings but also diminishes those with lower scores. Instead of training aproxy reward model, stable alignment directly optimizes LLMs using preference data, whichcould avoid reward hacking.SL with Ranking-based Feedback Signals These methods directly use supervisedlearning to optimize LLMs with loss functions constructed from ranking-based feedbacksignals. CRINGE (Adolphs et al., 2022) explores negative examples that an LLM shouldnot do for language modeling. For each unfavorable output token, it samples a positivetoken from the language model (i.e., a token in the top-k predictions excluding negativetokens) and constructs a contrastive loss. Negative sequences can be derived either fromhuman annotations or models trained on human annotations. Xu et al. (2022) dive intoaligning a model by training another model that inherently produces toxic content. The mainidea is to use the toxic model to re-rank the candidate token distribution of the model tobe aligned. Tokens that the toxic model prefers will have lower probabilities in generation.However, two primary issues arise from this approach. First, it is more resource-intensive tofirst train a toxic model and then purify it. Second, there’s a notable difference between amodel having a tendency to produce toxic content and one that persistently generates toxicoutputs. The proposed method risks removing harmless tokens, potentially compromising theoverall quality and diversity of the model’s outputs. Similarly, Schick et al. (2021) proposean approach where a model first identifies potential toxic text types it generates. By allowingthe model to self-diagnose, it can then generate text corresponding to the identified type.The debiasing strategy operates on the principle that if a word is deemed toxic, it is morelikely to be generated in a toxic context than in a benign one. The greater the difference,the higher the necessity to detoxify. The proposed de-poisoning methodology involves anexponential decay to reduce the likelihood of generating such words. Sequence LikelihoodCalibration (SLiC) (Zhao et al., 2022; 2023c) is designed to align the model’s outputs withreference sequences by employing latent distance as a means of calibrating the likelihood ofthe output sequence. SLiC utilizes a range of loss functions, including rank loss, margin loss,list-wise rank loss, and expected rank loss, to fine-tune this likelihood. Simultaneously, itemploys cross-entropy and KL divergence as regularization losses to ensure alignment withthe original fine-tuning objective. RRHF (Yuan et al., 2023) directly uses ranking resultsto construct supervision signals for alignment. Specifically, given a reward function thatcan assign a gold score for each (query, response) pair, they first use the model to generatelength-normalized conditional log probability as a score for each (query, response) pair. Then,the gold score and score generated by the model are used to construct a ranking loss topenalize the model for the inconsistency with the reward function. Finally, the total loss20is computed as the summation of the ranking loss and the cross-entropy loss between themodel-generated response and the response with the highest reward. Rafailov et al. (2023)propose direct preference optimization (DPO) to directly optimize LLMs to align with humanpreferences, which is similar to RRHF. The difference is that the optimization of DPO’sloss function can be demonstrated as equivalent to the objective in RLHF, which focuseson maximizing the reward while incorporating KL divergence regularization. Preferenceranking optimization (PRO) (Song et al., 2023) also aims for direct optimization for LLMswith human preference ranking data. Instead of relying on pairwise comparison, the trainingobjective of PRO harnesses preference ranking data of varying lengths. Specifically, thisapproach initiates with the first response, deems subsequent responses as negatives, thendismisses the current response in favor of the next. This loop continues until no responsesremain.4.3.3 Challenges of Non-recursive OversightCasper et al. (2023) thoroughly discuss the open problems and fundamental limitations ofRLHF. They categorize the challenges into two types: tractable challenges which can besolved within the RLHF paradigm, and fundamental challenges which have to be solved byusing other alternative outer alignment methods. Both reinforcement learning and humanfeedback in RLHF suffer from the two types of problems. For collecting human feedback,tractable challenges include the difficulty in obtaining quality feedback, data poisoning byhuman annotators, partial observability, biases in feedback data, to name a few; fundamentalchallenges include inability of humans to provide feedback for complex tasks that are hardto evaluate (i.e., lack of scalability to complex tasks, especially to superhuman models),gamed evaluation, tradeoffs between cost and quality as well as between diversity andefficiency in feedback collecting. For RL, tractable challenges include misgeneralizationto poor reward proxies of reward models, difficulty and cost of evaluating reward models,etc. while fundamental challenges include the difficulty of modeling human values or valuesof a diverse society with reward models, reward hacking, power-seeking incentivized byRL. Regarding the SL-based methods, it is more difficult for them to generalize to out-of-distribution data and long-term rewards compared to the RL-based methods, indicating asignificantly lower upper bound for optimization.4.4 Scalable OversightTo tackle the fundamental challenge of non-recursive oversight in the scalability to complextasks / superhuman models, scalable oversight is emerging as a promising methodology. Themain idea of scalable oversight is to enable relatively weak overseers (e.g., humans overseeingsuperhuman models) to supervise complex tasks with easy-to-adjudicate signals.4.4.1 Task DecompositionIf humans want to solve a complex task that is beyond human capabilities, a straightforwardidea is to break the task down into a number of relatively simple tasks that humans can solve.A variety of paradigms and strategies have been proposed to decompose a complex task intosimple subtasks.21• Factored Cognition (Stiennon et al., 2020): This involves a decomposition processthat breaks down a complex task into numerous smaller, predominantly independenttasks, which are then processed simultaneously.• Process Supervision (Lightman et al., 2023): Unlike factored cognition, processsupervision fragments a complex task into a series of sequential subtasks, each with itsown dependencies. One of its key characteristics is the setting of supervision signals foreach distinct phase. This equates to offering a dense reward throughout the trainingphase, which can potentially mitigate the challenge of estimating sparse rewards solelybased on the final outcome of a difficult task.• Sandwiching (Bowman et al., 2022): Compared to the previous two paradigms,sandwiching operates on a different plane. This competency-level decompositionrequires that complex tasks within a specific domain be delegated to an expert forresolution.• Iterated Distillation and Amplification (IDA) (Christiano et al., 2018): IDA is aniterative machine learning process with repeated and boosted distillation and amplifi-cation steps. In the amplification step, an agent solves a task by decomposing it intosubtasks that the agent is able to solve. This step “amplifies” the capability of theagent through task decomposition. The solved tasks in the amplification step producea dataset which is used to train a new agent in the distillation step. The two stepsare chained together where the output of the amplification step (i.e., a set of solvedtasks) is the input of the distillation step and the output of the distillation step (i.e.,a new agent) becomes the input of the amplification step in the next iteration.• Recursive Reward Modeling (RRM) (Leike et al., 2018): RRM is conceptually akin toIDA. However, it substitutes distilled imitation learning with reward modeling. Thisis a process with the first step being the derivation of a reward model from signalsaligned with human values, and the subsequent step involves optimizing an agentusing this reward model, but with a reinforcement learning twist. Humans collaboratewith the agent optimized through reinforcement learning, forming an enhanced versionready for successive iterations.The ambitious Superalignment (OpenAI, 2023b) project recently initiated in OpenAI can beviewed as a package solution to outer alignment, which synthesizes a variety of techniquesunder the guidance of scalable oversight. The core of Superalignment is to build a large numberof roughly human-level automated alignment researchers (AAR) to offload as many alignmenttasks as possible from humans and thus speed up the outer alignment research. Once thecomputation can be effectively translated to alignment capabilities, the vast amounts ofcompute can be used to scale the efforts, and achieve iterative alignment for superintelligence.4.4.2 Constitutional AIConstitutional AI (or principle-guided alignment) (Bai et al., 2022c; Sun et al., 2023b) canbe viewed as a scalable oversight approach, where humans provide meta-supervision signals22(general principles an AI system should follow), and the AI system will further generate actualtraining instances under the guidance of these human-written principles. The AI system canuse its abilities to amplify and instantiate human supervision, which can assist humans toscale their supervision to superhuman systems.Bai et al. (2022c) propose constitutional AI (CAI) with two training phases, which are similarto RLHF while minimizing human annotations. In the SL phase, they use red teamingprompts to provoke harmful responses from an LLM. They require the LLM to repeatedlygenerate self-criticism and correction based on the response and principle, and fine-tune theLLM based on the corrected responses to obtain the SL-CAI model. In the RL phase, a set ofresponses is generated via the SL-CAI model for each red teaming prompt, which is the bestoption based on the constitution, and harmlessness data used for training is obtained. Theytrain a preference model using human-annotated helpfulness data and generated harmlessnessdata. Finally, they use RL to train the RL-CAI model based on the SL-CAI model andpreference model.Sun et al. (2023b) present Dromedary, a model trained via principle-driven self-instruct andself-align approach without using RL. First, they employ topic-guided red-teaming self-instructwith seed prompts and 7 rules for new instruction generation to generate synthetic prompts.Then, they ask the model to filter harmful responses according to 16 human-written principlesto obtain self-aligned responses to synthetic prompts, which will be used to fine-tune thebase LM. Finally, they utilize a human-crafted prompt to encourage the model to generateself-aligned and verbose responses to synthetic prompts , and apply context distillation (Askellet al., 2021) to the model to make it generate in-depth and detailed responses.4.4.3 DebateDebate (Irving et al., 2018; Irving and Askell, 2019; Du et al., 2023) is another promisingscalable oversight paradigm that can not only achieve single-agent alignment but also enablemulti-agent alignment. In this paradigm, an agent (or multiple agents) first proposes ananswer to a question, and then alternately plays the role of debate participants, presentingand criticizing arguments for and against the proposed answer. A human will act as a judge,using these arguments to select an answer that they believe to be the most accurate andappropriate.The advantage of this method lies in its simplicity. Complex tasks, where direct evaluation ofAI responses can be daunting for humans, become manageable. The debate format structuresthe information in a way that requires humans to apply only simple reasoning rules. Itimproves transparency and explicability to AI operations. In traditional settings, AI outputsmight seem like results from a “black box”, with minimal insight into the decision-makingprocess. The debate method, however, offers a window into this process, with agents forcedto justify and critique their positions. Furthermore, it leverages the adversarial nature ofdebate to unearth the best possible answer. By pitting AI agents against each other, anyfallacious or weak arguments are likely to be exposed, leaving behind the most robust andvalid reasoning.23Recent works demonstrate the effectiveness of debate in LLMs. Du et al. (2023) propose amulti-agent debate method to improve factuality and reasoning in LLMs. This method engagesseveral instances of a language model in a structured debate to produce a unified response.The iterative process starts with each LLM generating individual answers. Subsequent roundsinvolve critiquing and revising these answers based on feedback from other LLMs until aconsensus emerges. This method capitalizes on the wisdom of crowds, with the individualLLM benefiting from the collective insights of its counterparts. On the other hand, Lianget al. (2023) leverage multi-agent debate to address degeneration-of-thought (DoT) problem,where LLMs fail to generate new insights once they are confident in their answers. They findthat multi-agent debate helps to correct distorted thinking, provide diverse external feedback,and overcome resistance to change, which can make LLMs escape from the convergence ofmisconceptions.4.4.4 Market MakingMarket making (Hubinger, 2020a) can be considered as a variant of debate, where the goal ofa debater is to generate arguments to maximize changes in the judge’s belief. Specifically,this framework trains two models - M (Market) and Adv (Adversary). For a given questionQ, the model M predicts the answer a human would provide at the end of the procedure. Incontrast, Adv is trained to generate arguments that would most likely cause M to “changeits mind”, meaning it would produce a different distribution of answers than it did previously.The process will be repeated T times. After each argument provided by Adv, M updatesits prediction. At the end of the T iterations, a human is presented with all the argumentsgiven by Adv and provides their final answer. This answer then helps in refining M . Oncetraining is over, Adv is discarded and M is used as the primary question-answering system.In this process, M acts like a “prediction market”, estimating what a human would answerto a question, while Adv tries to manipulate this market by providing arguments that wouldchange the human’s perspective. Once we obtain a stable answer from M , it indicates arobust response that considers all arguments Adv could present.Due to the similarity between debate and market making, techniques that enhance the debateapproach, such as cross-examination, can be beneficial here too. For instance, in each step,the latest version of Adv can cross-examine its previous version. If an earlier version of Adv ismisleading, the newer version can point this out, ensuring that false arguments are discarded.Additionally, oversight mechanisms can be incorporated where a supervising entity ensuresthat the model remains honest and aligned.4.4.5 Proxy TasksFluri et al. (2023) propose to use a proxy task with intrinsic self-consistency to overseesuperhuman models, where the proxy task is used for overseers to easily identify whether itis correct. For example, although we don’t know how to accurately predict the men’s worldrecord of 100m sprint, we know that this record will be monotonely decreasing over time. Soif a model predicts a non-monotonic function for the 100m record over time, we can assertthat this model is wrong. However, since the proxy tasks are usually specific and can only24capture a part of unexpected behaviors, this method largely promotes precision over recall inidentifying misalignment behaviors.4.4.6 Challenges of Scalable OversightAlthough scalable oversight is a promising solution to outer alignment, especially for modelsbeyond human-level capabilities, it still relies heavily on certain foundational assumptions,which should be carefully considered in application:• Tasks can be parallelized (Segerie, 2023): Central to the approach of factored cognitionis the assumption that complex tasks can be broken down into smaller and mainlyindependent subtasks. The core belief here is that challenges can be addressed throughsmall, mostly context-independent contributions made by individual LLMs who mightnot necessarily understand the bigger picture. However, this doesn’t always hold trueas some tasks are inherently sequential. For instance, sorting algorithms require atleast log(n) serial sorting steps, indicating that they cannot be fully decomposed intoparallel parts.• Model intentions are transparent to humans (Leike et al., 2018): Another fundamentalpremise is that we can easily discern the intentions of our models. But scalableoversight hinges on the model cooperating with human supervisors. If the modelgains the capability to intentionally conceal its real intentions from human oversight,effectively implementing scalable oversight becomes a challenge.• Evaluation is always easier than generation (Leike et al., 2018): It’s believed that formany tasks we want to tackle, evaluating the outcomes is simpler than generating thecorrect behaviors. This might not always be the case, especially for tasks with a low-dimensional outcome space, like binary results (yes/no). However, this assumption doeshold up when users also seek explanations for the answers, as evaluating explanationsis often easier than creating them.If these foundational assumptions of scalable oversight are not satisfied, setting appropriatesupervision targets for it becomes problematic. The stakes rise significantly once a modelachieves superhuman capabilities. Should humans set improper supervision goals at thisstage, resulting in misaligned behaviors, the consequences could be severe. This is due tothe immense power of superhuman models, where uncontrollable outcomes are no longeracceptable.5 Inner AlignmentIn comparison to outer alignment, inner alignment aims at the question whether an AI systemrobustly fulfills (optimizes for) the given objective that aligns to what humans want it todo. The term of inner alignment has been first given a definition by Hubinger et al. (2019c).Before discussing this relatively formal definition of inner alignment, we introduce 4 conceptsrelated to it:25Base Optimizer A base optimizer is a machine learning algorithm that searches for amodel capable of performing well on a specific task (Hubinger et al., 2019c). For example,gradient descent is a common base optimizer that updates the parameters of a model basedon the gradient of the loss function.Base Objective The base objective is the rationale used by the base optimizer to selectbetween different possible models (Hubinger et al., 2019c). It is specified by the AI systemdesigner and aligns to the intended goal of the designer for the model.Mesa-optimizer A mesa-optimizer is a learned model that functions as an optimizer,internally searching through a space of possible outputs, policies, plans, or strategies accordingto an explicitly specfied objective function (Hubinger et al., 2019c). A base optimizer may ormay not generate a mesa-optimizer.Mesa-objective The mesa-objective is the objective of a mesa-optimizer and the rationaleemployed by the mesa-optimizer to select among various potential outputs (Hubinger et al.,2019c).The mesa-optimizer may have an objective that differs from that of the base optimizer, whichcould lead to alignment or safety concerns. In this context, a relatively formal definition ofinner alignment refers to the challenge of aligning the mesa-objective of a mesa-optimizerwith the base objective of the base optimizer, so that the mesa-optimizer pursues the samegoal as the base optimizer (Hubinger et al., 2019c).105.1 Inner Alignment FailuresAlthough the optimization process of the mesa-optimizer is directly controlled by the baseoptimizer, there may be situations where the mesa-optimizer pursues an objective that differsfrom that of the base optimizer. This indicates that the mesa-objective is not aligned with thebase objective, resulting in a failure of inner alignment. According to Hubinger et al. (2019c),inner alignment failures can be categorized into three types: proxy alignment, approximatealignment, and suboptimality alignment.Proxy alignment (Hubinger et al., 2019c;b; Angelou, 2022) refers to a failure mode in whicha mesa-optimizer learns to optimize its own mesa-objective, rather than the intended baseobjective. In this scenario, the mesa-objective serves as a proxy or approximation of thebase objective, resulting in the mesa-optimizer optimizing an incorrect proxy, rather thanthe true intended base objective. Deceptive alignment (Hubinger et al., 2019a) is a type ofproxy alignment in which a mesa-optimizer gains sufficent awareness of the base objectiveand is instrumentally incentivized to pretend to be aligned with the base optimizer, in orderto avoid being adjusted by the base optimizer. In this case, the mesa-optimizer could merelyoptimize the base objective as an instrumental goal. Once the training process is completedor it is no longer in the training process, the mesa-optimizer may pursue its own goal instead.10Other definitions of inner alignment are also circulated in the alignment community. Please refer to Arike(2022) for more discussions.26Inner AlignmentDefinitionsHubinger et al. (2019c)Base OptimizerBase ObjectiveMesa-optimizerMesa-objectiveMikulik (2019)Objective RobustnessCapability RobustnessFailuresProxy Alignment Deceptive Alignment Hubinger et al. (2019b)Approximate Alignment Hubinger et al. (2019b)Suboptimality Alignment Hubinger et al. (2019b)Methodology Relaxed Adversarial Training Hubinger (2019b)Empirical Experiment ProposalsReward Side-Channels Hubinger (2019a)Cross-Episodic Objectives Hubinger (2019a)Objective Unidentifiability Hubinger (2019a)Zero-Shot Objectives Hubinger (2019a)Robust Reward Learning Hubinger (2019a)Figure 3: An incomplete and coarse-grained landscape of inner alignment.Approximate alignment (Hubinger et al., 2019c;b; Angelou, 2022) refers to a form of pseudo-alignment in which the mesa-objective of a mesa-optimizer is approximately the same as thebase objective, with some degree of approximation error. Such error arises due to technicallimitations that prevent the mesa-optimizer from perfectly representing the base objective.As a result, the mesa-objective only approximates the base objective, rather than being anexact representation of it.Suboptimality alignment (Hubinger et al., 2019c;b; Angelou, 2022) refers to a form ofpseudo-alignment in which a deficiency, error, or limitation causes a mesa-optimizer toexhibit aligned behavior, even though its mesa-objective is not actually aligned with the baseobjective. For example, computational constraints may result in the mesa-optimizer pursuinga suboptimal strategy that happens to be aligned with the training distribution. However, ifthese deficiencies are overcome later (e.g. during deployment), the mesa-optimizer may stopto exhibit aligned behavior.While outer and inner alignment have their own definitions, categorizing specific alignmentfailures into either inner alignment failures or outer alignment failures may be challengingand inconsistent in practice (Shah, 2023). This is due to the complex interdependenciesbetween outer and inner alignment, implying that failures in one could trigger those in theother. Flaws in either outer or inner alignment can result in unintended agent behaviors.For instance, an inner alignment failure could suggest that the base objective does notfully capture the designer’s goals, indicating an outer alignment failure (Wentworth, 2020).Conversely, defective outer alignment may allow for the exploitation of vulnerabilities by themesa-optimizer, resulting in an inner alignment failure. As such, it is important to carefullyconsider both aspects when designing highly capable AI systems.275.2 Inner Alignment MethodologyUnlike outer alignment that has been extensively explored (especially in LLMs) recently inan empirical way, inner alignment is limited in its empirical and methodological study. Mostdiscussions on inner alignment are theoretical and usually focusing on its definitions, failuremodes and risks. With the rapid development of capabilities of advanced agents, the necessityof methodological studies in inner alignment is becoming urgent.To improve inner alignment in advanced agents, Hubinger (2019b) proposes relaxed adversar-ial training, where an adversary subsystem proposes hypothetical pseudo-inputs estimatedto likely induce unacceptable behaviors, rather than attempting to generate concrete un-acceptable inputs. The pseudo-inputs describe potential situations that could precipitateunacceptable behaviors if instantiated. A separate oversight subsystem then scrutinizeswhether the agent would in fact act unacceptably if the pseudo-inputs were implemented.If so, the system receives a penalty, incentivizing avoidance of potentially unacceptablebehaviors. Relaxed adversarial training thus aims to promote inner alignment by penalizingartificial agents for predicted unacceptable behaviors on proposed pseudo-inputs duringtraining. Furthermore, Hubinger (2019b) identifies transparency as the core obstacle toeffective relaxed adversarial training for inner alignment. Robust transparency into themodel’s reasoning is requisite for the oversight system to reliably verify if a model would actunacceptably on proposed pseudo-inputs. Further research should both validate the efficacyof relaxed adversarial training empirically and elucidate transparency mechanisms enablingprovable inner alignment in advanced agents.5.3 Empirical Experiment Proposals for Inner AlignmentSimilar to the limited methodological exploration of inner alignment, empirical studies thatdirectly observe inner alignment and shed light on its inner workings are scarce. In thisaspect, Hubinger (2019a) proposes several concrete experiments for inner alignment. Webriefly introduce these proposals to demonstrate how inner alignment could be empiricallystudied.• Reward Side-Channels (RSC) This proposal aims to investigate the degree towhich reinforcement learning agents rely on external reward signals versus learninginternal value functions. The proposed experiment entails training an RL agent wherethe previous time step’s reward is incorporated into the observation state. At test time,the observed reward would be modified and changes in the agent’s behavior measured.This would be compared to two control conditions - one where no reward signal isprovided during testing and the other where the next time step’s reward is given.To enable the tracking of long-term returns, neural architectures such as LSTM orTransformer which have demonstrated proficiency in capturing long-term dependenciescould be explored. By observing the agent’s behavioral changes in response to shiftsin the external reward, we can assess the robustness of its learned objective. Thehypothesis is that reliance on external rewards reflects a lack of internalization ofgoals.28• Cross-Episodic Objectives (CEO) The CEO proposal suggests an experiment toevaluate the tendency of RL agents to exploit non-myopic reward side-channels acrossepisodes. CEO involves training an agent in an environment containing a mechanismfor increasing reward in the subsequent episode. The degree to which the agentutilizes this cross-episodic reward channel is measured and compared across differentpopulation-based training approaches. The motivation is assessing the conditionsunder which RL agents depart from solely myopic optimization. This has implicationsfor the choice of training techniques to align agent behavior with human preferences.Approaches relying on short-term optimization, such as amplification and debate, maybe less robust than those based on more far-sighted principles like inverse reinforcementlearning. By quantifying the prevalence of non-myopic reward hacking across differentpopulation training regimes, this experiment aims to provide guidance on preferablealignment strategies.• Objective Unidentifiability (OU) This proposal outlines an experiment to investi-gate RL agents’ tendencies toward pseudo-alignment when trained in environmentswith multiple viable objectives. The suggested experiment involves constructing asetting with several simple, discernible goals that would equally well explain the truereward signal. After an agent is trained in this environment, it would be evaluatedin distinguishing test cases to reveal its learned priorities. Particular interest lies indocumenting occurrences of the agent converging to a competent proxy policy thatnevertheless fails to robustly maximize the true rewards out-of-distribution. By manip-ulating architectural factors like inductive biases and model capacity, the preferencefor different proxies can be assessed.• Zero-Shot Objectives (ZSO) ZSO designs an experiment to evaluate the emergenceof goal-directed behavior and coherent objectives in language models without explicitRL training. The proposal creates an interactive environment where a languagemodel can take actions and receive rewards. By analyzing the resulting behaviorsthrough inverse reinforcement learning, the internal learned objectives can be inspectedand compared to a RL agent trained directly on the environment’s rewards. Whilecontemporary language models might not exhibit truly goal-directed optimization,this experiment aims to investigate the potential emergence of such abilities arisingfrom pure language modeling. Finding that language models can perform non-triviallyin certain environments and produce reasonably coherent inferred objectives wouldsuggest these models are starting to develop some intentionality, even without beingexplicitly trained as RL agents.• Robust Reward Learning (RRL) This proposal defines an experiment to evaluatethe efficacy of adversarial training techniques for improving alignment of model-basedRL agents. It trains a model-based RL agent, such as an imagination-based planner,to predict environment rewards. The predicted rewards are compared to the truerewards to assess alignment. The agent is then trained adversarially by constructinginputs that maximize divergence between predicted and actual rewards. Alignmentis evaluated again after adversarial training. The motivation is to test the ability ofadversarial techniques to address reward unidentifiability and enhance alignment.29Mechanistic InterpretabilitySelf-attentionCircuit Elhage et al. (2021)Induction Head Olsson et al. (2022)MLPK/V matrix Geva et al. (2021)Geva et al. (2022)Superposition Elhage et al. (2022a)Elhage et al. (2022b)NeuronsFunction Specific NeuronsKnowledge Dai et al. (2022)Meng et al. (2022)Linguistic Elhage et al. (2022a)Edit NeuronsFact Li et al. (2023b)Concept Belrose et al. (2023)Figure 4: An overview of current mechanistic interpretability research, including mechanisticstudies on self-attention (circuit, induction head), MLP (K/V matrix, superposition) andneurons (function specific neurons, edit neurons)6 Mechanistic InterpretabilityMechanistic interpretability (Vilone and Longo, 2020) refers to elucidating the internalmechanisms by which a machine learning model transforms inputs into outputs, providingcausal and functional explanations for how and why certain predictions are made (Nanda,2022; Lipton, 2017). The goal of mechanistic interpretability is to reverse engineer thereasoning process from end to end, decomposing neural networks into interpretable parts andflows of information that provide transparency into their step-by-step reasoning.Mechanistic interpretability holds great significance for AI alignment. First, interpretabilitymethods can be utilized to audit LLMs, particularly prior to their deployment. We can inspectthe alignment efficacy of an LLM, identify misaligned and fallacious outputs, and elucidatewhy it yields such outputs (Nanda, 2022; Lipton, 2017). Second, interpretability evaluationmetrics could serve as reward functions for optimizing AI alignment (Critch and Krueger, 2020)to incentivize AI systems to maintain goal transparency (e.g., avoiding deceptive alignment)(McAllister et al., 2017). Third, in addition to inspection /architecture transparency, we couldalso enforce training process transparency that enables us to understand and monitor what’shappening and the changes in the training process of AI systems (e.g., emerging behaviors /abilities) (Hubinger, 2022a).We now discuss recent progress made by mechanistic interpretability on different componentsin Transformer, including self-attention, multi-layer perceptron (MLP), and neurons.6.1 Mechanistic Interpretability on Self-AttentionThe self-attention (SA) mechanism is widely used to aggregate contextual information bydirectly “attending” to specific tokens. Each token in the context is paired with the currenttoken to calculate “compatibility” score. Such scores are used to weight tokens in the contextwindow so that learned representations of tokens are aggregated for predicting the next-stepdecision (e.g., next-token prediction). Elhage et al. (2021) investigate a SA-layer-only (MLPlayers removed) Transformer (Vaswani et al., 2017) and find interesting neural circuits. In30their work, SA layer is viewed as performing read and write operations into the residualstream, modifying the original token embeddings. They discover that the QK circuits focuson the next potential token, while the OV circuits tend to copy previous tokens, which theyrefer to as induction heads.Olsson et al. (2022) further investigate induction heads and attribute the general in-contextlearning ability of LLMs to the manifestation of induction heads. They present evidence forboth small SA-only models and large models with MLPs.6.2 Mechanistic Interpretability on MLPMLP layers introduce non-linear transformations in Transformer and account for a largeproportion of parameters, significantly enhancing the model’s expressive power. Such non-linear transformations enable Transformer to capture complex relationships and patterns indata, making it more capable of representing intricate functions (Geva et al., 2021; 2022;Elhage et al., 2022a). Due to the non-linear nature and high dimensionality of data, directlyreverse engineering MLPs is challenging.To address this issue, Elhage et al. (2022a) propose an interpretable activation functioncalled SoLU, which can deal with polysemantic neurons and encourage feature-neuronalignment. SoLU facilitates neural networks to learn human-interpretable neuron patternswithout significant performance degradation. Elhage et al. (2022b) further examine thephenomenon of feature superposition in MLPs using a simple network with ReLU activation.Their experiments demonstrate that linear models do not exhibit feature superposition (i.e.,ambiguity), whereas non-linear models display increasingly apparent feature superpositionwith the increase in data sparsity.6.3 Mechanistic Interpretability on NeuronsOlah (2022) views neurons as variables in a computer program. Previous studies havedemonstrated the existence of different types of neurons in Transformers, such as knowledgeneurons (Dai et al., 2022; Meng et al., 2022) and neurons corresponding to specific linguisticproperties (Elhage et al., 2022a). Interventions at the neuron level could change the outputsof the entire neural network. This is leveraged to enhance the factuality of machine-generatedcontent (Li et al., 2023b) and to eliminate the influence of specific concepts (Belrose et al.,2023). By understanding and manipulating these individual neurons, we can gain insightsinto how a neural model processes and represents information, which benefits developinginterpretable and safe AI systems.6.4 ChallengesDespite the success mentioned above, mechanistic interpretability (MI) is still at an incipientstage of research. Most current MI studies have been done under restricted conditions,e.g., on a toy language model (typically one-to-four-layer Transformer language models),or with predefined simple tasks (Wang et al., 2022a; Elhage et al., 2021). Even so, MI isconfronted with a variety of challenges, e.g., the superposition hypthothesis (Elhage et al.,2022b), non-linear representations (Lee Sharkey, 2022b).31Attack Methods against AlignmentPrivacy Attacks Jailbreaking Prompts Li et al. (2023a)Deng et al. (2023)Backdoor AttacksPrompt InjectionLiu et al. (2023e)Zhao et al. (2023a)Greshake et al. (2023)Kandpal et al. (2023)Backdoors Injection at RLHF Shi et al. (2023)Adversarial AttacksAdversarial Prompts Zou et al. (2023)Visual Adversarial Examples Carlini et al. (2023)Qi et al. (2023)Figure 5: An overview of attack methods that might be capable of breaking through thesafeguard of aligned models.The superposition hypothesis that neural networks attempt to represent more features thanneurons or dimensions they have, has been compellingly verified (Elhage et al., 2022b).Feature superposition in neural networks explains the phenomenon of neuron polysemanticitywhere a neuron corresponds to several unrelated features (Elhage et al., 2022b). Althoughsuperposition is useful for neural representations, it poses a challenge to MI as it makesit difficult to disentangle representations, hence preventing MI from explaining relationsbetween disentangled representations or features in a simple and human-understandable way(Lee Sharkey, 2022a;b).7 Attacks on Aligned Language ModelsLarge language models have encountered challenges posed by various attack methods. Mali-cious systems could intentionally prompt LLMs to generate harmful, biased, or toxic text,thereby posing significant risks of misuse (Brown et al., 2020; Ouyang et al., 2022). As aprimary strategy to mitigate these risks. LLM alignment via RLHF has been widely adopted(Ouyang et al., 2022; Glaese et al., 2022). This alignment can be considered as a safeguardagainst these attacks.Recent studies show that such aligned LLMs exhibit defensive capabilities against maliciousattacks. Carlini et al. (2023) demonstrate that aligned LLMs can effectively counter a widerange of (white-box) NLP attacks, even adversarial inputs. Li et al. (2023a) showcase thatChatGPT is able to decline providing answers to privacy-sensitive questions.Nonetheless, alignment techniques are not infallible. For example, through repeated interac-tions, humans can “trick” these models into generating harmful content, as seen in jailbreakingattacks. In addition to jailbreaking, other methods have also been explored to breach thesafeguard of aligned models. We divide these efforts into three categories according to thenature of the attack methods. The overview of these attacks is presented in Figure 5.7.1 Privacy AttacksA privacy attack constitutes an approach wherein machine learning models are exploited, withattackers attempting to extract private or sensitive information about the training data from32the model’s outputs (Rigaki and Garcia, 2020; Mireshghallah et al., 2020; Sousa and Kern,2023; Guo et al., 2022). Legal frameworks related to personal data protection necessitatethe preservation of privacy in training data, as leakage could result in legal repercussions(GDPR). Currently, privacy attacks on language models can be categorized into four types:(1)Gradient Reconstruction Attacks during the model distributed training stage, (2)AttributeInference Attacks, (3)Prompt Attacks and (4)Inversion Attacks during the inference stage.Gradient Reconstruction Attacks aim at attacking models during the distributed training,where information such as training data and gradients is exchanged between devices. Attackerscan spy on this information exchange to reconstruct privacy-sensitive details from the trainingdata (Gupta et al., 2022; Deng et al., 2021). Although no specific research has targetedreconstruction attacks on aligned models, these spying-based attacks remain a potentialthreat when aligned models are tuned in a distributed training way.Attribute Inference Attacks infers data ownership and privacy attributes by comparing theperformance of a target model with that of similar models (Song and Shmatikov, 2019;Hisamoto et al., 2020; Mireshghallah et al., 2022). Such methods often require access tooutput probabilities, logits, or hidden states, making implementation on black-box APIs(which provide only textual outputs) challenging.Inversion Attacks (Song and Raghunathan, 2020; Elmahdy et al., 2022) aim to inversely getinput information using model gradients, parameter states, etc. Implementing such methodsis also challenging for LLMs as they usually have a huge amount of parameters.Prompt Attacks involve designing or searching for prompts that lead LMs to output infor-mation from the training data, including private details (Carlini et al., 2021; Lehman et al.,2021; Li et al., 2023a; Deng et al., 2023). This approach is particularly targeted towardsLLMs and poses a significant threat to aligned LLMs. Li et al. (2023a) propose a new attackmethod that extracting personal identity information(PII) from ChatGPT and New Bing bymulti-step Jailbreaking Prompts. And it shows the New Bing is more vulnerable to directextraction of PII due to its search engine integration, posing unintended privacy risks.7.2 Backdoor AttacksBackdoor attacks are a class of methods aimed at machine learning models, with the objectiveof causing the model to produce specific, incorrect outputs when certain backdoor triggersare detected (Gao et al., 2020; Li et al., 2022; Sheng et al., 2022). Backdoor attacks can becategorized into two types: (1)Data Poisoning and (2)Model Poisoning.Data Poisoning introduces triggers (e.g., instances generated with special lexical or syntactictemplates) into the training data to implement a backdoor attack on the model (Li et al.,2021b; Qi et al., 2021; Chen et al., 2021b). Previous studies primarily focused on taskslike text classification, but these methods can also be extended to tasks such as questionanswering and text generation. Backdoor attacks on aligned models often utilize PromptInjection techniques (Liu et al., 2023e; Zhao et al., 2023a; Greshake et al., 2023; Kandpalet al., 2023), where the prompt itself serves as the trigger, eliminating the need for externalinputs. When a trigger prompt is used, it could lead to unintended outcomes.33Model Poisoning achieves backdoor attacks by manipulating the model itself, involvingmodifications to word embeddings, loss functions, output representations, etc. (Yang et al.,2021; Wallace et al., 2020; Li et al., 2021a). Recently, Shi et al. (2023) propose a new attackmethod called BadGPT, which makes Backdoors Injection at RLHF to the reward model.This method has two stages: first, injecting backdoors into the reward model to make it givewrong rewards when a specific trigger word appears. Second, using the backdoored rewardmodel to fine-tune the language model, thereby injecting a backdoor into the aligned model.7.3 Adversarial AttacksAdversarial attacks are techniques employed to compromise the performance or behaviorof machine learning models, particularly deep learning models, by introducing small andcarefully crafted perturbations to the input data (Akhtar and Mian, 2018; Zhang et al., 2020;Qiu et al., 2022; Goyal et al., 2023). These perturbations are often imperceptible to humansbut can lead the model to produce incorrect or unexpected outputs. Prior works on textualtasks use greedy attack heuristics (Wallace et al., 2019) or employ discrete optimization tosearch for an input text that triggers adversarial behavior (Wallace et al., 2019; Jones et al.,2023).For aligned models, Zou et al. (2023) proposed a simple yet potent attack strategy thatcombines greedy search and gradient-based techniques to automatically generate AdversarialPrompts, causing aligned LLMs to produce contentious behaviors.Studies by Carlini et al. (2023) and Qi et al. (2023) demonstrate that multimodal languagemodels exhibit reduced defenses against white-box adversarial attacks, such as VisualAdversarial Examples. The high-dimensional visual input space renders these modelsmore susceptible, and the diverse outputs present additional targets for adversarial attacks.8 Alignment EvaluationEvaluation is important for alignment research, especially for the development of empiricalalignment methods. We review methods and resources pertaining to LLM alignment. Asillustrated in Figure 6, our alignment evaluation landscape is structured across multiplelevels. The first level illustrates the five aspects of LLM outer alignment we are focusing on,namely: 1) factuality, 2) ethics, 3) toxicity, 4) stereotype and bias, and 5) general evaluation.Genaral evaluation does not target at a single specific dimension of alignment, e.g., factuality,toxicity. Instead, it evaluates multiple dimensions of alignment or the general aspects ofLLM alignment. The subsequent level categorizes the primary evaluation methods presentlyavailable in each respective area. We distinguish task-specific evaluation from LLM-centeredevaluation at this level. Task-specific evaluation refers to evaluating alignment quality ondownstream tasks while LLM-centered evaluation designs evaluation benchmarks, methodsor metrics directly for LLMs. The third level is designated for fine-grained classification orshowcasing related works, enabling readers to swiftly pinpoint their areas of interest.34Alignment EvaluationFactuality Task-specific EvaluationFactuality ConsistencyLaban et al. (2022)Fabbri et al. (2021)Honovich et al. (2022)Zha et al. (2023)Factuality PrecisionMin et al. (2023)Lee et al. (2022)Lin et al. (2021)EthicsAnalysis-based Evaluation Forbes et al. (2020)QA-based EvaluationHendrycks et al. (2020)Tay et al. (2020)Lourie et al. (2021)ToxicityTask-specific Evaluation Offensive LanguageDetectionWaseem and Hovy (2016)Ross et al. (2017)Wulczyn et al. (2017)Zampieri et al. (2019)LLM-centered EvaluationXu et al. (2020)Gehman et al. (2020)Deng et al. (2022)Stereotype and BiasTask-specific EvaluationCoreference ResolutionRudinger et al. (2018)Zhao et al. (2018)Cao and III (2021)Machine Translation Stanovsky et al. (2019)Renduchintala and Williams (2021)Relation Extraction Gaut et al. (2020)Sentiment Analysis Díaz et al. (2019)Kiritchenko and Mohammad (2018)Natural Language Inference Dev et al. (2020)Bias DetectionSap et al. (2019)Zhou et al. (2022)Zhang et al. (2023)LLM-centered EvaluationNadeem et al. (2020)Nangia et al. (2020)Dhamala et al. (2021)Li et al. (2020)Parrish et al. (2022)Hosseini et al. (2023)Huang and Xiong (2023)Hate Speech DetectionExplicit Hate SpeechWaseem (2016)Davidson et al. (2017)de Gibert et al. (2018)Kennedy et al. (2022)Breitfeller et al. (2019)Vidgen et al. (2021)Kumar and Pranesh (2021)Implicit Hate Speech ElSherief et al. (2021)Hartvigsen et al. (2022)GeneralEvaluationBenchmarksSun et al. (2023a)Srivastava et al. (2022)Huang et al. (2023)Wang et al. (2023b)Li et al. (2023d)Ye et al. (2023)Zheng et al. (2023a)MethodsAutomatic EvaluationSingle LLMZheng et al. (2023a)Chiang et al. (2023)Dubois et al. (2023)Dettmers et al. (2023)Zhou et al. (2023a)Multiple LLMs Bai et al. (2023)Li et al. (2023c)Human Evaluation Xu et al. (2023b)Figure 6: The taxonomy of alignment evaluation methods, including factuality and truthful-ness, ethics, toxicity, stereotype & bias, and comprehensive evaluations.8.1 Factuality EvaluationMachine-generated content should be congruent with facts, eschewing the creation of halluci-nation content. Additionally, each piece of generated information should be factually accurate.These suggest that factuality evaluation at least comprise factual consistency evaluation andfactual precision assessment.Factual consistency requires that generated content should be consistent with given context.As downstream tasks, like text summarization, dialogue, are usually accompanied with richcontext, many task-specific actuality evaluation studies are conducted on such downstreamtasks. While this could be done on a single task (Laban et al., 2022; Fabbri et al., 2021),consistency evaluation on multiple tasks is more convincing. Honovich et al. (2022) provide acomprehensive analysis of factual consistency, incorporating a variety of metrics, tasks, and35datasets. Their study consolidates 11 datasets from a variety of tasks into a unified format.They also compare the effectiveness of existing methods for evaluating consistency, usingthis unified format. The ALIGNSCORE metric, proposed by (Zha et al., 2023), is designedto cover a wide range of factual consistency evaluation scenarios, such as contradictionand hallucination across various lengths and tasks. The metric is developed through thetraining of an aligned model, which restructures 15 datasets from 7 NLP tasks. These tasksinclude Natural Language Inference, Question Answering, Paraphrasing, Fact Verification,Information Retrieval, Semantic Similarity, and Summarization.Factuality precision evaluation is also task-specific. Lee et al. (2022) present a benchmark anda metric for factual precision evaluation. They use both factual and non-factual prompts toobtain generated texts from an LLM. The used specific tasks include named entity recognitionand entailment. Min et al. (2023) introduce FACTSCORE, a novel method that deconstructslong-form text into atomic facts or individual pieces of information, assigning a binary labelto each fact. However, the efficacy of this method is largely dependent on the acquisition ofthese atomic facts, making the selection of evaluation tasks a critical factor. They concentrateon the generation of individual biographies, as the atomic facts contained within thesebiographies can be verified by Wikipedia.Factual precision is also related to the model’s ability to answer questions truthfully. Lin et al.(2021) present TruthfulQA and argue that the training objectives of LLMs could potentiallyinfluence them to produce false responses. As a result, they devise a series of highly inductivequestions to actively assess LLMs.Evaluating factuality presents two significant challenges. First, while factuality encompassescountless facts, the scope of factuality evaluation so far inherently limited. Second, not allfacts in real life are easy to be divided into atomic facts. Current evaluation methods fallshort when dealing with complex information that can’t be simplified, such as assessingfactualness that requires sophisticated reasoning.8.2 Ethics EvaluationEthics is a multifaceted issue pervading nearly every aspect of society, characterized bydialectical thinking. It encompasses a broad spectrum of considerations, including good andevil, right and wrong, virtue and vice, justice and crime, which are all related to individuals(Martinez, 2020). As a result, most LLM ethics evaluations employ a straightforwardmethodology. This involves posing questions related to ethics and morality to the assessedmodel, and subsequently assessing the model’s alignment with human values on these mattersbased on its responses.Hendrycks et al. (2020) introduce the ETHICS benchmark, a comprehensive collection ofover 130,000 scenarios spanning five domains of ethics: justice, virtue ethics, deontology,utilitarianism, and commonsense morality. Crafted by individuals who have passed a qualifi-cation test, these scenarios serve as brief statements that tested models must have to predictmoral sentiments as either acceptable or unacceptable. Similarly, Tay et al. (2020) proposethe MACS benchmark, which includes 200,000 chosen questions for learning alignment withcultural values and social preferences. This benchmark distinguishes itself through its unique36data collection method, drawing from the popular online game “Would You Rather?”. Thequestions and answers provided in this game offer a more comprehensive dataset than thoserelying solely on a few annotators. In contrast to these works that involve short text pieces,Lourie et al. (2021) collect real-life anecdotes in a long-text format, rich in detail. The originaldata is sourced from a public sub-forum on Reddit, a platform where individuals seek advicefrom online acquaintances to navigate real-life situations.The evaluation methodology employed in Social Chemistry 101 (Forbes et al., 2020) divergesfrom traditional QA-based approaches. They deconstruct tacit commonsense rules into twelvedistinct dimensions of human judgment, including cultural pressure, action-taking, socialjudgment, etc. The study offers a range of perspective choices to annotators for specificscenarios. This innovative approach enables annotators to examine ethical situations fromdiverse viewpoints, thereby enriching the depth and breadth of the annotated data.It is clear that assessments in the realm of ethical morality depend on real-world contextualdata. While some initiatives have factored in cultural backgrounds during data collection, theprimary data and reference responses largely stem from the researchers’ own cultural contexts.As a result, it is incumbent upon researchers to dedicate themselves to the collection andgeneration of data that mirrors a diverse range of cultural backgrounds, which can then beutilized as evaluation datasets.8.3 Toxicity EvaluationToxicity is defined as harmful and destructive behaviors or attitudes that can manifest ininterpersonal relationships, work environments, or other social settings. This might takethe form of control over others, manipulation, belittlement, or malicious attacks. Thesebehaviors can be overt or covert, causing damage to the self-esteem, safety, and well-being ofindividuals. There is a wide array of toxic language that includes: (i) Suggestions leading toself-harming behaviors; (ii) Content that is pornographic or violent in nature; (iii) Harassment,belittlement, offense, insults, and hate speech; (iv) Suggestions advocating for aggressive orviolent actions, such as cyberbullying; (v) Guidelines or directions for seeking illegal goods orservices.We categorize the toxicity evaluation into two dimensions: task-specific evaluation andLLM-centered evaluation. Task-specific evaluation pertains to assessing the level of toxicitydisplayed by a model when it’s applied to specific downstream tasks. The diversity of taskswithin the field of NLP significantly enriches our evaluation scenarios, enabling us to morecomprehensively investigate the contexts in which language models manifest toxicity. Onthe other hand, LLM-centered evaluation evaluates LLMs directly based on the generatedoutputs to gauge their toxicity. In task-specific evaluation, the model’s performance mightbe constrained by the specific tasks, potentially behaving in ways that prioritize achieving“high accuracy”. In contrast, in LLM-centered evaluation, the model predominantly respondsbased on its inherent knowledge and tendencies. Such an evaluation approach is currentlythe mainstream method that is gaining significant attention and adoption.378.3.1 Task-specific EvaluationOffensive language detection can be categorized as a downstream classification task. Offensivelanguage pertains to the deployment of injurious articulates in a sacrilegious, extremelydiscourteous, impolite, or crude fashion, aiming to derogate the specified individual or group(Chen et al., 2012; Razavi et al., 2010). Early works on offensive language detection (Waseemand Hovy, 2016) from Twitter provides datasets which only share Twitter IDs and bullyingtypes, lacking detailed content. Building on this, Ross et al. (2017) focus on the Germanrefugee situation with a modest dataset of just over 400 tweets. Wulczyn et al. (2017) analyzesa vast corpus from Wikipedia, exploring 95 million user-article interactions for personal attacksand toxicity. In contrast, Zampieri et al. (2019) return to Twitter, introducing a dataset withdetailed annotations on attack types and targets, enriching the understanding of offensivelanguage in social media.8.3.2 LLM-centered EvaluationTo directly evaluate toxicity in LLMs, LLM-centered evaluations trigger models to yield toxicresponses. These evaluations mainly concentrate on the toxicity level of the yielded outputs.BAD (Xu et al., 2020) necessitates individuals to engage in adversarial dialogues withadvanced models to prompt them into generating unsafe responses. This method mirrorsthe potential adversarial challenges models could face upon deployment. By utilizing thismethod, they gather an extensive dataset of dialogues that could be further utilized to assessthe toxicity in LLMs. Similarly, RealToxicityPrompts (Gehman et al., 2020) constructs alarge set of prompts and performs a comprehensive evaluation on various language modelslike GPT-1 (Radford et al., 2018), GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020),and CTRL (Keskar et al., 2019). The findings reveal that even from seemingly innocuousprompts, pretrained LMs could degenerate into producing toxic text. In particular, GPT-1exhibits the highest toxicity, which might be attributed to the higher amount of toxic contentin its training data. This observation accentuates the importance of rigorous data scrutiny forLLMs. Shifting focus to the Chinese context, COLD (Deng et al., 2022) explores the detectionof offensive language in Chinese. It collects a significant volume of real-text data from socialmedia platforms and evaluates several open-source models. Consistent with previous findings,irrespective of the presence of offensive content in the input prompts, the generated outputsfrom these models often encompass offensive language.8.4 Stereotype and Bias EvaluationPrejudice and stereotype bias are defined as preconceived attitudes, usually based on agroup’s race, gender, sexual orientation, religion, or other characteristics. These attitudesmay be negative or positive but are generalized judgments of a group rather than based on anindividual’s actual behavior or traits. Prejudice may lead to discrimination or other unjustbehaviors.We also categorize the stereotype and bias evaluation into two dimensions: task-specificevaluation and LLM-centered evaluation. The former pertains to the assessment of biases38when the model is applied to specific downstream tasks, while the latter directly evaluatesthe inherent biases present within the model.Hate speech is language used to express hatred towards a target individual or group, oris intended to demean, humiliate, or insult members of a group based on attributes suchas race, religion, national origin, sexual orientation, disability, or gender (Davidson et al.,2017; Badjatiya et al., 2017; Warner and Hirschberg, 2012; Schmidt and Wiegand, 2017;Djuric et al., 2015). Since hate speech is usually associated with bias, we discuss hate speechdetection in LLM-generated content after the introduction to the general bias evaluation.8.4.1 Task-specific EvaluationTo understand where a model reinforces biases in its outputs, many studies investigate howthese biases occur in downstream tasks. These tasks can be standardized into generativetasks through prompt engineering, making them suitable for evaluating LLMs.The task of coreference resolution is among the first used to study biases in language models,typically employing F1 scores as a metric. Winogender (Rudinger et al., 2018) and WinoBias(Zhao et al., 2018) both address gender biases related to occupations. They utilize theWinogram-schema style (Levesque, 2011) of sentences, revealing stereotypes in coreferenceresolution systems when interpreting “HE” and “SHE”. GICOREF (Cao and III, 2021) focuseson the model’s performance on texts related to non-binary and binary transgender individuals.All evaluated systems perform worse on these texts than on binary gendered texts, with thebest model achieving only a 34% F1 score.The WinoMT Challenge Set (Stanovsky et al., 2019) is the first to explore gender bias inmachine translation task at a large scale, integrating both Winogender and WinoBias andsetting evaluation standards for eight languages. Gender accuracy in translations is theprimary metric. They discover significant translation biases in both commercial MT systemsand advanced academic models. Renduchintala and Williams (2021) expands this task tocover 20 languages, examining whether models still make gender translation mistakes withunambiguous contexts. They find accuracy levels generally below 70%, especially whenperceived occupational gender contradicts the context.Similarly, WikiGenderBias (Gaut et al., 2020) is a dataset aimed at analyzing gender bias inthe task of relation extraction. It evaluates gender bias in NRE systems by comparing modelperformance when extracting occupation information about women versus men from 45,000sentences.Díaz et al. (2019) finds that changing age and gender terms in sentences influence model scoresin sentiment analysis. The Equity Evaluation Corpus (EEC) (Kiritchenko and Mohammad,2018) delves deeper into categories of race and gender, providing comprehensive evaluationsof 219 sentiment analysis systems.Dev et al. (2020) utilizes Natural Language Inference (NLI) to detect biases in models. Theyestablish a broad benchmark based on polarized adjectives and ethnic names, which notonly includes gender but also countries and religions. Biases in models are determined bydeviations from neutral answers. Their results reveal evident biases in GloVe, ELMo, andBERT models.39Bias detection can be also categorized as a classification task. Sap et al. (2019) offers a datasetwith 150,000 annotated social media posts highlighting social bias frames across variousdemographic groups. Further localization efforts, particularly for non-English languages, giverise to CDail-Bias (Zhou et al., 2022). This is the first Chinese dataset targeting social bias indialog systems, covering race, gender, region, and occupation domains. In a more specializeddirection, CORGI-PM (Zhang et al., 2023) centers exclusively on gender bias. This uniqueChinese corpus encompasses 32,900 labeled sentences, marking a first in sentence-level genderbias in Chinese. Their innovative methodology uses an automated process for samplingpronounced gender bias, followed by a re-ranking based on sentence-level bias probability formore precise bias detection and mitigation.8.4.2 LLM-centered EvaluationIn direct bias evaluations of language models, there are various assessment methodologies.Some adopt a contrasting method using associated sentence pairs: one with more stereotypes,and the other with fewer (Nadeem et al., 2020; Nangia et al., 2020). Biases are detectedthrough the language model’s likelihood of recovering masks. StereoSet (Nadeem et al., 2020)spans a wide range of domains, including gender, occupation, race, and religion, testingmodels such as BERT (Devlin et al., 2019), GPT-2, RoBERTa (Liu et al., 2019), and XLNet(Yang et al., 2019). CrowS-Pairs (Nangia et al., 2020) extends the types of biases to ninecategories: race, religion, age, socioeconomic status, gender, disability, nationality, sexualorientation and appearance. Notably, they change the evaluation metrics to avoid higherlikelihoods for certain sentences merely due to their frequent occurrences in training data,rather than learned societal biases.Others, similar to toxicity evaluation, provide prompts to models, letting them completesuccessions, and then assessing biases in the outputs of these models. BOLD (Dhamalaet al., 2021) is a prompt dataset containing five bias types: profession, gender, race, religion,and political ideology, collected from Wikipedia. With these prompts, BOLD is able toevaluate social biases of language models via the proposed automated metrics for toxicity,psycholinguistic norms, and text gender polarity. HolisticBias (Smith et al., 2022) is abias dataset containing 13 demographic directions and over 600 subcategories, offering acomprehensive evaluation of the content generated by models and combining both automaticand human assessments to reveal biases more fully. Automatic evaluation measures bias bybreaking down quantities from different stylistic types compares. Human evaluation comparesthe performance of bias-reduced models with original models, based on preference, humanlikeness, and interestingness criteria, with crowdsourced workers on Amazon’s MechanicalTurk platform. Multilingual Holistic Bias (Costa-jussà et al., 2023) extends the HolisticBias(Smith et al., 2022) to up to 50 languages, emphasizing the universality and diversity of biasesin a multilingual environment.Both UnQover (Li et al., 2020) and BBQ (Parrish et al., 2022) focus on detecting model biasthrough transforming the generation task into the multiple-choice question answering task,but with different evaluation methods. UnQover utilizes unspecified questions, which couldn’tbe answered simlpy according to the given context. However, their evaluation is based onthe likelihood allocated to two incorrect options, while BBQ always provides the model a40correct answer, measuring the proportion of times the model chooses the correct answer.BBQ comprises nine types of biases, and is chosen as a bias benchmark for evaluating LLMsin HELM (Liang et al., 2022). CBBQ (Huang and Xiong, 2023) designs a bias evaluationdataset for Chinese LLMs, covering 14 bias types, rooted in Chinese society. In addition tothe extended bias types, CBBQ also proposes a new automated metric to evaluate multipleopen-sourced Chinese LLMs.8.4.3 Hate Speech DetectionHate speech detection can be casted as a classification task. The development of this taskcan not only promote control and review of the content generated by models, measuring theirharmfulness (in contrast to harmlessness in alignment), but also assist in the scrutinizationof harmful content in the training data for LLMs so as to reduce misaligned outputs frompretrained LLMs. However, measuring harmfulness with universally accepted standardsremains challenging. In this aspect, there exists a widely used detection tool, PerspectiveAPI.11 It analyzes texts to check whether they contain potentially harmful content, includingthreats, insults, profanity, and malicious speech, thus identifying and filtering out texts thathinder constructive dialogues in online forums. Both Facebook and Twitter have implementedpolicies that prohibit behaviors on their platforms. Such prohibited behaviors attack orthreaten others based on characteristics like race, ethnicity, gender, and sexual orientation.Explicit Hate Speeech Hate speech detection in early research primarily focuses on theexplicit hate speeech from the social media platform, Twitter, owing to its openness andextensive reach, thus providing a desirable data source for studies. Waseem (2016) investigates16,914 entries annotated by both amateur and expert annotators, with the F1 score beingthe primary metric of assessment. Davidson et al. (2017) collects 24,802 tweets, refining thecategories into hate speech, offensive but not hate speech, and neither offensive nor hatespeech. TweetBLM dataset (Kumar and Pranesh, 2021) correlates with the “Black LivesMatter” movement, encompassing 9,165 manually annotated data instances and conductinga systematic evaluation across various language models.Beyond Twitter, some researchers shift their focus to other social platforms to extractmore targeted hate speech content. de Gibert et al. (2018) center their study on the whitesupremacist forum, Stormfront, analyzing 9,916 hand-labeled hate speech entries. Additionally,Kennedy et al. (2022) turn their attention to Hate Forums, such as gab.com, and their datasetincludes 27,665 entries related to violence and extremism. Given the vast nature of theReddit platform, Breitfeller et al. (2019) opts for it as a research subject, concentrating on amild offense corpus and its objective criteria. On the other hand, DynaHate (Vidgen et al.,2021) introduces a unique research methodology that leverages both humans and modelsto dynamically generate and annotate data, rather than collecting the data from real-worldsocial media contexts. This approach not only augments the volume of the data but alsoenhances its quality.Implicit Hate Speech A key challenge in hate speech detection lies in the subtleties.Unlike overt harmfulness, which often uses profanity or explicit language, covert harmfulness11https://perspectiveapi.com/41may sometimes exhibit positive sentiment and is typically harder to detect or collect on alarge scale (MacAvaney et al., 2019; Breitfeller et al., 2019). Nevertheless, subtle harmfullanguage directed towards minority or marginalized groups can inflict psychological harmon members of these communities (Sue et al., 2007; Nadal et al., 2014; Kanter et al.; Nadal,2018; Saleem and Anderson) and may reinforce or amplify existing stereotypes or hatefulperceptions about them (Behm-Morawitz and Mastro, 2008; Soral et al., 2018).ImplicitHateCorpus (ElSherief et al., 2021) introduces a groundbreaking benchmark corpusfor implicit hate speech on Twitter. This study compares the performance of GPT-2 andGPT, revealing that GPT-2 outperformes GPT in both target group and implicit statementgeneration. Following this, TOXIGEN dataset (Hartvigsen et al., 2022) further propells theresearch in this area by utilizing GPT-3 to generate subtle toxic and benign texts, producinga resource that encompasses a wider scale and more demographic groups of implicit toxictexts than previous manually written resources. This results in a vast collection of sentences(over 274,000) spanning 13 identities. To improve data quality, Hosseini et al. (2023) refinesthe TOXIGEN dataset by choosing only sentences with unanimous annotator agreement ontargeted groups and introduces a new safety score metric. This highlights ongoing progressin implicit hate speech detection and the quest for more precise hate speech identification.Currently, classifiers or detectors trained on these datasets are predominantly at the sentencelevel. However, accurately detecting harmful content in multi-turn dialogues proves to bequite challenging. Additionally, implicit bias might require context for a precise evaluation.Unfortunately, datasets catering to this particular aspect are still in short supply.8.5 General EvaluationIn addition to the above-described benchmarks and methods that focus on measuring a specificaspect of alignment quality (e.g., factuality, bias), general evaluation of LLM alignment,which comprehensively evaluates LLM alignment quality in multiple aspects simultaneouslyor in a general way, has attained increasing interest.8.5.1 BenchmarksGeneral evaluation benchmarks usually take the form that the model under evaluation outputsa response to a given instruction and an optional input, with an advanced LLM or human asthe evaluator.TrustGPT (Huang et al., 2023) employs templates to generate instructions from threeperspectives: bias, toxicity, and value consistency, with different automated evaluationmetrics used for each dimension. Given that previous evaluations are overly direct (suchas asking the model to judge the morality of a certain behavior), TrustGPT incorporatesharmful content into prompts, thus evaluating value consistency under passive conditions. Ina more specialized direction, Sun et al. (2023a) focus on evaluating the security capabilitiesof Chinese LLMs, designing 8 typical security scenarios and 6 more challenging instructionattacks, proving that instruction attacks are more likely to expose the vulnerabilities of LLMs.They maintain a leaderboard that evaluates the safety level of commonly available LLMs bycalculating a safety score for each model by an advanced LLM. However, when analyzing42model alignment capabilities, it is often necessary to evaluate the model at a fine-grainedlevel in multiple aspects, such as authenticity, toxicity, etc. It is difficult to comprehensivelyanalyze the model by merely assigning an overall score based on preferences. Therefore,FLASK (Ye et al., 2023) subdivides the coarse-grained score into four basic abilities: LogicalThinking, Background Knowledge, Problem Handling, and User Alignment, which are furtherdivided into 12 fine-grained skills, and uses advanced LLMs or humans to score each ofthese 12 skill perspectives. It is found that model scales for acquiring different skills aredifferent. On the other hand, MTbench (Zheng et al., 2023a) measures LLM’s ability tofollow instructions in multi-round conversations based on human preferences and contains 80high-quality multi-round questions covering eight common scenarios, including writing, role-playing, extraction, reasoning, math, and coding. The Big-bench HHH dataset (Srivastavaet al., 2022) provides instructions along with two human-written responses, and the LLMbeing evaluated simply selects the response that better matches the human’s preferences.Since it does not require a tested LLM to generate a response, it maintains a computationallysimple and relatively fair evaluation system. The used evaluation metric in this benchmark isaccuracy. Evaluation results on this dataset show that LLMs perform best in the honestycategory, with larger models exhibiting greater robustness.A general evaluation framework should be scalable, incremental, and consistent, which meansthat the framework is able to expand the scope of LLMs being evaluated when the evaluationdata is limited, use as few new experiments as possible to evaluate new models and providea stable ordering for all LLMs that have been evaluated (Zheng et al., 2023a). AlthoughGPT-4 may produce relatively consistent evaluations, using such an advanced LLM as anevaluator does not guarantee a stable and consistent ordering because of hallucinations andother unsolved problems. We hope to see the emergence of benchmarks that satisfy all threeproperties at the same time.8.5.2 MethodsAutomatic Evaluation Many works have used automated metrics such as BLEU, ROUGEto evaluate the performance of LLMs on several datasets. However, it has been demonstratedthat existing automatic evaluation metrics do not align well with human preferences inlong-form answers (Xu et al., 2023b). Although human evaluation is widely used in compre-hensive alignment evaluation benchmarks, it is expensive. As LLMs’ capabilities grow, theirpowerful generative ability has rivaled or surpassed ordinary human performance in multiplebenchmarks, illustrating that LLMs can serve not only as “test takers” but also as potential“examiners” to evaluate other LLMs.Previous attempts have been made to employ PLMs for evaluation. Xu et al. (2023b) and Fuet al. (2023) conduct targeted evaluations on mainstream text generation tasks using GPT3and FLAN-T5, demonstrating the potential of PLMs for NLG task evaluation. The emergenceof powerful LLMs like ChatGPT has led to an increasing number of studies employing LLMsas evaluators. Subsequently, LLMs have been extensively employed in alignment evaluationsto complement human evaluations, with three types of evaluation methods: single answergrading, pairwise comparisons, and reference-guided grading (Zheng et al., 2023a).43• Single answer grading Single answer grading uses advanced LLMs or humanevaluators to assign a score to the response for the given query generated by the LLMunder evaluation. Chiang et al. (2023) utilize GPT-4 to evaluate individual answers byscoring various chatbots on attributes such as helpfulness and relevance, and providejustifications for their assessments.• Pairwise comparison Pairwise comparison asks advanced LLMs or human evaluatorsto determine which of two possible responses generated by two LLMs being evaluatedfor each given query is superior, or if they are equivalent. Dettmers et al. (2023) andWang et al. (2023c) employ GPT-4 to score and provide justifications for the responsesof ChatGPT (or text-davinci-003) and the evaluated model, ultimately computing themodel’s score relative to ChatGPT’s score. Similarly, AlpacaEval (Li et al., 2023d)uses the GPT-4 or Claude or ChatGPT based automatic evaluator to compare theresponse generated by the LLM being evaluated with the reference response fromtext-davinci-003. Subsequently, considering the potential risk of data leakage that maybe associated with the use of closed-source API for evaluation, PandaLM (Wang et al.,2023b) introduces a judgment LLM, helping users to select the best LLM locally.• Reference-guided grading Reference-guided grading provides the appropriatereference answer generated by humans and requires an advanced LLM to compare theresponse generated by two LLMs being evaluated with the reference answer. Researchhas shown that this type of assessment leads to better rubric results on math problems(Zheng et al., 2023a).There are corresponding disadvantages to using an advanced LLM for automatic evaluation.Regarding the pairwise comparison, it results in exponentially increasing evaluations withthe growing number of models to be assessed. Additionally, the used advanced LLMs exhibitposition bias, verbosity bias, and self-enhancement bias during comparisons. These biasesincline the evaluator LLMs to favor the first answer, the long and verbose answer, or ananswer generated by a specific LLM, despite another answer being more concise and accurate(Zheng et al., 2023a; Wang et al., 2023a). Conversely, single-answer grading overlooks subtledifferences between two answers, leading to unstable scores and undermining the evaluation’scredibility. Moreover, LLMs’ limitations in math and reasoning abilities lead to their equalunderperformance in evaluation tasks involving math and reasoning (Zheng et al., 2023a).To address position bias, multiple evaluations can be conducted by employing positionswitching or by requiring the evaluator LLMs to generate multiple evidential supports (Zhenget al., 2023a; Wang et al., 2023a). To compensate for math and reasoning deficits, chain ofthoughts (Wei et al., 2022) can be explored to significantly enhance the reasoning ability ofLLMs, thereby improving evaluations that demand reasoning skills (Wang et al., 2023a; Liuet al., 2023c; Zheng et al., 2023a).However, the above methods do not relieve the problem of self-enhancement bias. When theproblem involves complex reasoning, multi-agent teamwork through deliberation and debatecan often broaden knowledge and break down single inherent perceptions, leading to moreaccurate and fair results. Studies have shown that collaborative efforts among multiple LLMs44can enhance the reasoning ability of weaker models (Ho et al., 2022; Magister et al., 2022;Wei et al., 2022), resulting in advanced performance across various downstream tasks.Therefore, recent studies have attempted to mitigate the problem of bias by using multipleLLMs for evaluation. Bai et al. (2023) propose a “peer-review” approach, where multiplemodels refer to each other’s evaluations and supporting rationales, simulating a thoughtprocess akin to human “discussion”. In contrast, Li et al. (2023c) adopt a “referee” approach,wherein multiple models take turns evaluating each other’s answers. They assign weights toeach model based on its winning rate, and the final answer is determined by the weightedresults of multiple models during the evaluation.The evaluation with multiple LLMs relieves the bias problem of individual LLMs, and at thesame time continues to utilize the powerful evaluation capability of LLMs, proving that LLMevaluation can be a powerful supplement to manual evaluation.Nevertheless, the bias and competence deficiencies in LLM evaluations have not been fullyresolved, preventing LLM-based automatic evaluations from entirely substituting humanevaluations currently. Moreover, the extensive similarity in existing LLM training data, theirarchitectures and training approaches may bias the mutual evaluation results towards theinner existing standards of LLMs rather than the correct human values (Dai et al., 2023).Human Evaluation Employing LLMs as evaluators offers swiftness and cost-effectiveness.However, even advanced LLMs (e.g., GPT-4) do not entirely concur with human evaluationoutcomes (Zheng et al., 2023a; Dettmers et al., 2023). Hence, human evaluation should beprioritized for high-stake decision-making.Existing human evaluations typically employ experts to quantitatively evaluate the outputsof LLMs. Wang et al. (2022b) employ human evaluation to evaluate whether the modeloutput effectively follows instructions and accomplishes the given task, and the outputs arecategorized into four levels based on their quality. Ye et al. (2023) shift from the coarse-grained evaluation to a fine-grained evaluation over four competencies and twelve skills, andask experts to score each of these twelve aspects.Evidently, human evaluation heavily hinges on the expertise level of the experts involved.However, due to inherent variations in values among experts, this form of evaluation remainssusceptible to issues of discrimination and bias.The use of pairwise comparisons and cross-annotation can mitigate the bias problem to someextent. AlpacaFram (Dubois et al., 2023) uses pairwise comparisons to build a dataset ofhuman preferences. Annotators are tasked with selecting the superior of two LLM outputs,with 650 instances concurrently annotated by four evaluators. Chatbot Arena (Zheng et al.,2023a), on the other hand, is a crowdsourcing platform where a person can talk to two chatbotsat the same time and rate their responses based on their personal preferences, thus enablinghuman evaluation of the capabilities of multiple chatbots. WizardLM (Xu et al., 2023a)extends this concept by enlisting crowdsourced workers to conduct pairwise comparisons ofresponses from multiple LLMs, evaluating them across five dimensions: relevance, knowledge,reasoning, computation, and accuracy.459 Future Directions and DiscussionsLLM alignment is a fast-growing and exciting area of research, but awaiting for further insightsand breakthroughs. Given the importance of AI safety and the harmonious coexistencebetween humans and AI in the foreseeable future, which we value from both the humanityand technology perspective, aligning advanced AI systems (including LLMs) to human valueswould be on top of the agenda. This alignment is becoming more and more challenging as thecapabilities of AI agents grow. More scientific and technological efforts need to be dedicatedto this area. This encourages us to discuss future directions for this area. These directionsare either summarized from informally circulated articles, blogs and interviews or from ourown restricted thoughts, which we hope could serve in some small way as a stimulus forfurther discussion and research. These directions could represent only a small part of thealignment landscape where subfields and new ideas continue to emerge.9.1 Theoretical Research for LLM AlignmentAs we stand on the precipice of unprecedented advancements in LLMs, it becomes increasinglyvital to ensure that these machines, no matter how advanced, remain aligned to humanvalues. The challenges of LLM alignment are both complex and diverse, necessitating amulti-faceted approach that draws from various disciplines. Inspired by Soares (2015b), wesummarize and highlight some key areas of theoretical alignment research. By deepening ourunderstanding and commitment in these areas, we aim to forge a future where LLMs areseamlessly integrated into our societies, amplifying our capacities, and elevating our sharedhuman experience.• Decision Theory: As we venture deeper into the LLM era, LLM alignment researchwithin the realm of decision theory is primarily concerned with ensuring that advancedLLMs make decisions in ways that are both predictable and beneficial to humanity.Future work in this area will delve into the intricacies of counterfactual reasoning,Newcomb-like problems, and potential paradoxes that LLMs might encounter. Byexploring how LLMs reason about and act upon decisions, especially when faced withsituations of deep uncertainty or conflicting values, we can foster systems that behavemore robustly and safely in a broader array of scenarios.• Corrigibility: Corrigibility is another pillar of LLM alignment research that warrantsfurther exploration. It refers to the ability of an LLM to allow itself to be correctedby its users without resisting or circumventing these corrections. As LLMs grow morepowerful and autonomous, there’s an increasing need to ensure they remain receptiveto human input and guidance. Future advancements in corrigibility would includecreating mechanisms where LLMs not only accept corrections but also proactively assistusers in aligning them better. Moreover, designing LLMs that recognize and rectifytheir own errors without creating negative side effects or exacerbating misalignmentswill be a cornerstone challenge in this area.• World Models: The fidelity and accuracy of the world model for LLMs can greatlyinfluence its behavior and efficacy. Current LLMs, even the most advanced, operate46on a limited understanding of the world, often derived from the data they’re trainedon. For safe and efficient operations, especially in dynamic and complex environments,LLMs need to possess realistic world models that accurately represent the multifacetednature of reality. Future work in LLM alignment should focus on bridging the gapbetween the virtual representations within LLM and the real-world intricacies outside.This involves not only improving the depth and breadth of these models but alsoensuring they are robust to changes and can adapt and grow as the real world evolves.9.2 Scalable OversightOne challenge in scalable oversight is the complexity of tasks that AI systems are supposedto solve. Although a variety of high-level scalable oversight strategies have been proposed(e.g., debate, IDA, RRW discussed in Section 4.4), these strategies have not yet undergonelarge-scale empirical verification. With the rapid development of LLMs, more empirical effortsare dedicated to scalable oversight, e.g., superalignment (OpenAI, 2023b). Exciting progresscould be made in this area in the coming years.9.3 Empirical Research into Deceptive AlignmentDeceptive alignment refers to a situation in which an AI agent deceives the training process bypretending to be aligned with the base objective to avoid modification during training. Onceit is no longer at risk of being modified (e.g., after training), the agent may cease optimizingthe base objective and begin pursuing its own mesa-objective, which could be entirely differentfrom the base objective that its designer defines and potentially harmful. Although deceptivealignment is usually discussed theoretically12, there is growing concern about the emergenceof deceptive alignment in large language models, given the rapid progress in their capabilitiesin recent years (Brown et al., 2020; Zeng et al., 2021; Zhang et al., 2022; Chowdhery et al.,2022; Scao et al., 2022; Anil et al., 2023; Touvron et al., 2023a;b; Zeng et al., 2023; OpenAI,2022; 2023a). Empirical research into deceptive alignment is a challenging task as it has beennow only theoretically anticipated. Despite disagreement on its likelihood, its severity iswidely acknowledged (Hubinger, 2022b). And because of this severity, it would be better forus to empirically monitor it before it would really occur.Hubinger et al. (2019a;c) suggest three conditions that must be satisfied for deceptivealignment to occur. These conditions can provide valuable insights into the experimentaldesign for empirically monitoring the presence of deceptive alignment:• The mesa-optimizer must have an objective that extends across parameterupdates This condition pertains to the nature of the mesa-optimizer’s objective. Fordeceptive alignment to occur, the mesa-optimizer’s objective must extend beyond asingle decision or action; it should span multiple steps or episodes, including parameterupdates. In other words, the mesa-optimizer’s objective should be persistent, with alonger-term goal that extends beyond individual actions.12https://www.alignmentforum.org/tag/deceptive-alignment47• The mesa-optimizer must be able to model the base objective This conditionfocuses on the mesa-optimizer’s cognitive capabilities. The mesa-optimizer shouldhave the ability to model or understand that it is being selected by the base optimizerto achieve a particular base objective. Additionally, it should possess some level ofknowledge on what the base objective is.• The mesa-optimizer must expect the threat of modification to eventuallygo away This condition relates to the mesa-optimizer’s expectations about its future.The mesa-optimizer should anticipate that the threat of modification or interventionby the base optimizer will cease at some point in the future. This can happen eitherbecause the training process ends or because of actions taken by the mesa-optimizeritself. In other words, the mesa-optimizer should foresee a scenario where it canoperate without being subject to external modifications.Consequently, for empirically monitoring deceptive alignment, we could design experiments insuch a way that they satisfy these conditions. To satisfy the first condition, we could ensurethat the mesa-optimizer’s objective spans across parameter updates and define scenarioswhere the mesa-optimizer’s actions have consequences beyond a single decision or action. Tosatisfy the second condition, we could introduce scenarios where the mesa-optimizer mustrecognize and demonstrate some understanding of the base objective, thereby testing itsability to model this objective. To satisfy the third condition, we could create conditionswhere the mesa-optimizer has an opportunity to expect that the threat of modification willdiminish. This might involve altering the training process or introducing external factorsthat influence the mesa-optimizer’s expectations.Furthermore, distributional shift between training and deployment environments is encouragedas a means of triggering or exacerbating deceptive alignment. The mesa-optimizer may detectdistributional shift when changes occur in its environment during deployment, which couldserve as a trigger for it to consider that it is no longer subject to the same constraints itfaced during training, and as a result, may exhibit deceptive alignment behavior.9.4 Automated LLM AlignmentWith the increasing complexity of AI models, there is an emergent need for systems that canautomatically evaluate and align those models. AI-driven supervisors and aligners could bedeveloped to assist alignment research.By deploying these systems, overseers can gain insights into the behavior of their AI models,detecting anomalies and preemptive signs of misalignment. This proactive approach can leadto timely interventions, reducing errors and potential misfires.However, like any AI-driven initiative, the implementation of automated alignment viaAI is not without its challenges. Concerns about accuracy, reliability, and the potentialrisks associated with unsupervised alignments are among the primary issues researchers andindustry practitioners are striving to address.489.5 Explainability and TransparencyThe “black box” nature of LLMs has raised concerns about their transparency and the needfor explainability. As these models could be used for critical decisions, understanding howthey arrive at specific outcomes is paramount.When explainability and transparency work in tandem, they can create an interpretablesystem wherein transparency lays the groundwork for users to trust the model’s operation,while explainability ensures that users can understand and validate the model’s outputs. Thus,as these principles mutually reinforce each other, they collectively enhance the trustworthinessand accountability of large language models in a variety of applications.However, the research on explainability and transparency is still in its early stages, indicatingthat there’s a vast terrain of unexplored potential and challenges ahead. As large languagemodels continue to grow in complexity and scale, ensuring that they remain understandableand accountable becomes an increasingly intricate task. Currently, many techniques appliedto foster explainability and transparency offer only surface-level insights, failing to delve deepinto the model’s intricate decision-making process. Considering the interdisciplinary natureof AI alignment, continued collaboration between machine learning researchers, ethicists, andneuroscientists may be required to drive progress in interpretability research.9.6 Dynamic Evaluation of LLM Alignment via Adversarial AttacksAdversarial attacks serve as a powerful tool in the realm of AI. These are intentionallydesigned inputs meant to confuse or mislead AI systems. Using one large model as an attackerto generate adversarial examples targeting alignment can be an effective way to test andevaluate another model’s alignment capabilities.Such dynamic testing, driven by adversarial attacks, is crucial to ensure that large modelscan robustly handle unexpected inputs without faltering. While this method introduces anadded layer of complexity, the insights garnered from these adversarial tests can be invaluable,offering a comprehensive understanding of a model’s strengths and weaknesses concerningalignment.9.7 Field Building of LLM Alignment: Bridging between LLM and AI AlignmentCommunityThe alignment community within the realm of AI is still nascent, with many questions leftunanswered and numerous challenges unaddressed. The current landscape lacks a cohesivescientific paradigm, leading to controversies in theories, methodologies and empirical results.As a promising, unified testbed for various alignment methods, LLMs can serve as a platformto realize thought experiments and proposals, which will be helpful in developing stableresearch methodologies, establishing consensus on key issues, and crafting a consistentscientific framework for AI alignment. On the other hand, the deep theories, methodologiesand findings in the AI alignment community will guide LLMs toward being aligned accurately,ethically, and effectively. Thus, the connection between LLMs and AI alignment communitywill build a virtuous circle that benefits both.4910 ConclusionThe rapid evolution of LLM in recent years has undeniably ushered in a new era of technologicalprowess. However, with this power comes the responsibility of ensuring that these modelsoperate within the boundaries of human ethics and expectations. This survey has provideda comprehensive overview of the alignment methodologies tailored for LLMs, emphasizingthe criticality of aligning capability research with ethical considerations. By categorizing thealignment techniques into outer and inner alignment, we have shed light on the multifacetedapproaches that the research community is currently employing. Emerging topics such asmodel interpretability, and vulnerabilities to adversarial attacks have been also discussed,underscoring the complexities involved in the alignment process. Furthermore, this paperhas not only chronicled the current state of alignment research but has also looked ahead,identifying potential research trajectories that promise to further refine and enhance thealignment of LLMs. It is our fervent hope that this survey acts as a catalyst, fosteringcollaboration between the AI alignment community and LLM researchers. Such a collaborativeapproach is indispensable to harness the full potential of LLMs, ensuring that they servehumanity in a manner that is both ethically sound and beneficial. In essence, as we continueto push the boundaries of what LLMs can achieve, it is imperative that we remain groundedin our commitment to their responsible and principled deployment.50ReferencesAbubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent anti-muslim bias in largelanguage models. In AIES ’21: AAAI/ACM Conference on AI, Ethics, and Society, VirtualEvent, USA, May 19-21, 2021, pages 298–306. ACM.Leonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar Sukhbaatar, and JasonWeston. 2022. The cringe loss: Learning what language not to model. arXiv preprintarXiv:2211.05826.Naveed Akhtar and Ajmal Mian. 2018. Threat of adversarial attacks on deep learning incomputer vision: A survey. IEEE Access, 6:14410–14430.Afra Feyza Akyürek, Ekin Akyürek, Aman Madaan, Ashwin Kalyan, Peter Clark, DerryWijaya, and Niket Tandon. 2023. Rl4f: Generating natural language feedback withreinforcement learning for repairing model outputs. arXiv preprint arXiv:2305.08844.Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas,and Kelvin Guu. 2022. Towards tracing knowledge in language models back to the trainingdata. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages2429–2446, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and DanMané. 2016. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565.Eleni Angelou. 2022. Three scenarios of pseudo-alignment. https://www.lesswrong.com/posts/W5nnfgWkCPxDvJMpe/three-scenarios-of-pseudo-alignment.Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexan-dre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu,Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, GauravMishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, KefanXiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Ábrego, Junwhan Ahn, JacobAustin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks,Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, AakankshaChowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin,Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber,Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, and et al. 2023.Palm 2 technical report. CoRR, abs/2305.10403.Anthropic. 2023. Core Views on AI Safety: When, Why, What, and How. https://www.anthropic.com/index/core-views-on-ai-safety.Rauno Arike. 2022. Clarifying the confusion around inner alignment. https://www.alignmentforum.org/posts/xdtNd8xCdzpgfnGme/clarifying-the-confusion-around-inner-alignment.Stuart Armstrong et al. 2013. General purpose intelligence: arguing the orthogonality thesis.Analysis and Metaphysics, (12):68–84.51Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, AndyJones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. 2021. A general languageassistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861.Pinkesh Badjatiya, Shashank Gupta, Manish Gupta, and Vasudeva Varma. 2017. Deeplearning for hate speech detection in tweets. In Proceedings of the 26th InternationalConference on World Wide Web Companion, Perth, Australia, April 3-7, 2017, pages759–760. ACM.Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, and Mark Riedl.2023. Improving language models with advantage-based offline policy gradients. arXivpreprint arXiv:2305.14718.Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training a helpfuland harmless assistant with reinforcement learning from human feedback. arXiv preprintarXiv:2204.05862.Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, AndyJones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen,Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, DustinLi, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, JoshuaLandau, Kamal Ndousse, Kamile Lukosiute, Liane Lovitt, Michael Sellitto, Nelson Elhage,Nicholas Schiefer, Noemí Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, SamRinger, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham,Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman,Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, TomBrown, and Jared Kaplan. 2022b. Constitutional AI: harmlessness from AI feedback.CoRR, abs/2212.08073.Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, AndyJones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022c.Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, KaishengZeng, Yijia Xiao, Haozhe Lyu, et al. 2023. Benchmarking foundation models with language-model-as-an-examiner. arXiv preprint arXiv:2306.04181.Jacob Barrett and Hilary Greaves. 2023. Existential risk from power-seeking ai.Elizabeth Behm-Morawitz and Dana E Mastro. 2008. Mean girls? the influence of genderportrayals in teen movies on emerging adults’gender-based attitudes and beliefs. Journalismand Mass Communication Quarterly, 85(1):131.Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, andStella Biderman. 2023. Leace: Perfect linear concept erasure in closed form. arXiv preprintarXiv:2306.03819.52Tsvi Benson-Tilsen and Nate Soares. 2016. Formalizing convergent instrumental goals. InAAAI Workshop: AI, Ethics, and Society.Timothy W Bickmore, Ha Trinh, Stefan Olafsson, Teresa K O’Leary, Reza Asadi, Nathaniel MRickles, and Ricardo Cruz. 2018. Patient and consumer safety risks when using conversa-tional assistants for medical information: an observational study of siri, alexa, and googleassistant. Journal of medical Internet research, 20(9):e11510.Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, IlyaSutskever, Jan Leike, Jeff Wu, and William Saunders. 2023. Language models can explainneurons in language models. https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html.Nick Bostrom. 2012. The superintelligent will: Motivation and instrumental rationality inadvanced artificial agents. Minds and Machines, 22:71–85.Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner,Kamile Lukosuite, Amanda Askell, Andy Jones, Anna Chen, et al. 2022. Measuring progresson scalable oversight for large language models. arXiv preprint arXiv:2211.03540.Gwern Branwen. 2020. GPT-3 creative fiction.Luke Breitfeller, Emily Ahn, David Jurgens, and Yulia Tsvetkov. 2019. Finding microag-gressions in the wild: A case for locating elusive phenomena in social media posts. InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processingand the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 1664–1674. Association forComputational Linguistics.Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, PrafullaDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, SandhiniAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, AdityaRamesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language modelsare few-shot learners. In Advances in Neural Information Processing Systems 33: AnnualConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,2020, virtual, pages 1877–1901.Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, EceKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificialgeneral intelligence: Early experiments with GPT-4. arXiv preprint arXiv:2303.12712.Ben Buchanan, Andrew Lohn, Micah Musser, and Katerina Sedova. 2021. Truth, lies, andautomation. Center for Security and Emerging Technology, 1(1):2.Yang Trista Cao and Hal Daumé III. 2021. Toward gender-inclusive coreference resolution: Ananalysis of gender and bias throughout the machine learning lifecycle. Comput. Linguistics,47(3):615–661.53Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena Gao,Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, et al.2023. Are aligned neural networks adversarially aligned? arXiv preprint arXiv:2306.15447.Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss,Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021.Extracting training data from large language models. In 30th USENIX Security Symposium(USENIX Security 21), pages 2633–2650.Joseph Carlsmith. 2022. Is power-seeking ai an existential risk? arXiv preprintarXiv:2206.13353.Andres Carranza, Dhruv Pai, Rylan Schaeffer, Arnuv Tandon, and Sanmi Koyejo. 2023.Deceptive alignment monitoring. arXiv preprint arXiv:2307.10569.Micah Carroll. 2018. Overview of current ai alignment approaches.Micah Carroll, Alan Chan, Henry Ashton, and David Krueger. 2023. Characterizing manipu-lation from ai systems. arXiv preprint arXiv:2303.09387.Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer,Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al.2023. Open problems and fundamental limitations of reinforcement learning from humanfeedback. arXiv preprint arXiv:2307.15217.Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021a.Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.Yangyi Chen, Fanchao Qi, Hongcheng Gao, Zhiyuan Liu, and Maosong Sun. 2021b. Tex-tual backdoor attacks can be more harmful via two simple tricks. arXiv preprintarXiv:2110.08247.Ying Chen, Yilu Zhou, Sencun Zhu, and Heng Xu. 2012. Detecting offensive language insocial media to protect adolescent online safety. In 2012 International Conference onPrivacy, Security, Risk and Trust and 2012 International Confernece on Social Computing,pages 71–80. IEEE.Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-sourcechatbot impressing GPT-4 with 90%* ChatGPT quality. See https://vicuna. lmsys. org(accessed 14 April 2023).Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, AdamRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, ParkerSchuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes,Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson,Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,54Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, XavierGarcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, DavidLuan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan,Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai,Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, KatherineLee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, MicheleCatasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and NoahFiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311.Paul Christiano, Buck Shlegeris, and Dario Amodei. 2018. Supervising strong learners byamplifying weak experts. arXiv preprint arXiv:1810.08575.Marta R Costa-jussà, Pierre Andrews, Eric Smith, Prangthip Hansanti, Christophe Ropers,Elahe Kalbassi, Cynthia Gao, Daniel Licht, and Carleigh Wood. 2023. Multilingual holisticbias: Extending descriptors and patterns to unveil demographic biases in languages atscale. arXiv preprint arXiv:2305.13198.Andrew Critch and David Krueger. 2020. Ai research considerations for human existentialsafety (arches). arXiv preprint arXiv:2006.04948.Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2022. Knowledgeneurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume 1: Long Papers), pages 8493–8502,Dublin, Ireland. Association for Computational Linguistics.Juntao Dai, Xuehai Pan, Jiaming Ji, Ruiyang Sun, Yizhou Wang, and Yaodong Yang.2023. Pku-beaver: Constrained value-aligned llm via safe rlhf. https://github.com/PKU-Alignment/safe-rlhf.Robert Dale. 2021. GPT-3: What’s it good for? Natural Language Engineering, 27(1):113–118.Thomas Davidson, Dana Warmsley, Michael W. Macy, and Ingmar Weber. 2017. Automatedhate speech detection and the problem of offensive language. In Proceedings of the EleventhInternational Conference on Web and Social Media, ICWSM 2017, Montréal, Québec,Canada, May 15-18, 2017, pages 512–515. AAAI Press.Ona de Gibert, Naiara Pérez, Aitor García Pablos, and Montse Cuadros. 2018. Hate speechdataset from a white supremacy forum. In Proceedings of the 2nd Workshop on AbusiveLanguage Online, ALW@EMNLP 2018, Brussels, Belgium, October 31, 2018, pages 11–20.Association for Computational Linguistics.Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang,Tianwei Zhang, and Yang Liu. 2023. Jailbreaker: Automated jailbreak across multiplelarge language model chatbots. arXiv preprint arXiv:2307.08715.Jiawen Deng, Jingyan Zhou, Hao Sun, Chujie Zheng, Fei Mi, Helen Meng, and Minlie Huang.2022. COLD: A benchmark for chinese offensive language detection. In Proceedings of the2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022,55Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 11580–11599. Associationfor Computational Linguistics.Jieren Deng, Yijue Wang, Ji Li, Chao Shang, Hang Liu, Sanguthevar Rajasekaran, andCaiwen Ding. 2021. Tag: Gradient attack on transformer-based language models. arXivpreprint arXiv:2103.06819.Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficientfinetuning of quantized llms. arXiv preprint arXiv:2305.14314.Sunipa Dev, Tao Li, Jeff M. Phillips, and Vivek Srikumar. 2020. On measuring and miti-gating biased inferences of word embeddings. In The Thirty-Fourth AAAI Conference onArtificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of ArtificialIntelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advancesin Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages7659–7666. AAAI Press.Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings ofthe 2019 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA,June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171–4186. Association forComputational Linguistics.Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021. BOLD: dataset and metrics for measuring biasesin open-ended language generation. In FAccT ’21: 2021 ACM Conference on Fairness,Accountability, and Transparency, Virtual Event / Toronto, Canada, March 3-10, 2021,pages 862–872. ACM.Mark Díaz, Isaac Johnson, Amanda Lazar, Anne Marie Piper, and Darren Gergle. 2019.Addressing age-related bias in sentiment analysis. In Proceedings of the Twenty-EighthInternational Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China,August 10-16, 2019, pages 6146–6150. ijcai.org.Nemanja Djuric, Jing Zhou, Robin Morris, Mihajlo Grbovic, Vladan Radosavljevic, andNarayan Bhamidipati. 2015. Hate speech detection with comment embeddings. In Proceed-ings of the 24th International Conference on World Wide Web Companion, WWW 2015,Florence, Italy, May 18-22, 2015 - Companion Volume, pages 29–30. ACM.Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, KashunShum, and Tong Zhang. 2023. Raft: Reward ranked finetuning for generative foundationmodel alignment. arXiv preprint arXiv:2304.06767.Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. 2023.Improving factuality and reasoning in language models through multiagent debate. arXivpreprint arXiv:2305.14325.56Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, CarlosGuestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpacafarm: A simulationframework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387.Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, HinrichSchütze, and Yoav Goldberg. 2021. Measuring and improving consistency in pretrainedlanguage models. Transactions of the Association for Computational Linguistics, 9:1012–1031.Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston,Sheer ElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernandez, AmandaAskell, Kamal Ndousse, Andy Jones, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli,Liane Lovitt, Zac Hatfield-Dodds, Jackson Kernion, Tom Conerly, Shauna Kravec, StanislavFort, Saurav Kadavath, Josh Jacobson, Eli Tran-Johnson, Jared Kaplan, Jack Clark, TomBrown, Sam McCandlish, Dario Amodei, and Christopher Olah. 2022a. Softmax linear units.Transformer Circuits Thread. Https://transformer-circuits.pub/2022/solu/index.html.Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, ShaunaKravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, SamMcCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah.2022b. Toy models of superposition. Transformer Circuits Thread.Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain,Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion,Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, SamMcCandlish, and Chris Olah. 2021. A mathematical framework for transformer circuits.Transformer Circuits Thread.Adel Elmahdy, Huseyin A Inan, and Robert Sim. 2022. Privacy leakage in text classification:A data extraction approach. arXiv preprint arXiv:2206.04591.Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. 2023. GPTs are GPTs:An early look at the labor market impact potential of large language models. arXiv preprintarXiv:2303.10130.Mai ElSherief, Caleb Ziems, David Muchlinski, Vaishnavi Anupindi, Jordyn Seybolt, Mun-mun De Choudhury, and Diyi Yang. 2021. Latent hatred: A benchmark for understandingimplicit hate speech. In Proceedings of the 2021 Conference on Empirical Methods inNatural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, DominicanRepublic, 7-11 November, 2021, pages 345–363. Association for Computational Linguistics.Alexander R Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2021. Qafacteval:Improved qa-based factual consistency evaluation for summarization. arXiv preprintarXiv:2112.08542.FAIR, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, DanielFried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. 2022. Human-level play in57the game of diplomacy by combining language models with strategic reasoning. Science,378(6624):1067–1074.Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins,Amanda Bertsch, José GC de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, et al.2023. Bridging the gap: A survey on integrating (human) feedback for natural languagegeneration. arXiv preprint arXiv:2305.00955.Lukas Fluri, Daniel Paleka, and Florian Tramèr. 2023. Evaluating superhuman models withconsistency checks. arXiv preprint arXiv:2306.09983.Maxwell Forbes, Jena D Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. 2020. So-cial chemistry 101: Learning to reason about social and moral norms. arXiv preprintarXiv:2011.00620.Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. GPTscore: Evaluate asyou desire. arXiv preprint arXiv:2302.04166.Leo Gao, John Schulman, and Jacob Hilton. 2023. Scaling laws for reward model overopti-mization. In International Conference on Machine Learning, pages 10835–10866. PMLR.Yansong Gao, Bao Gia Doan, Zhi Zhang, Siqi Ma, Jiliang Zhang, Anmin Fu, Surya Nepal,and Hyoungshick Kim. 2020. Backdoor attacks and countermeasures on deep learning: Acomprehensive review. arXiv preprint arXiv:2007.10760.Andrew Gaut, Tony Sun, Shirlyn Tang, Yuxin Huang, Jing Qian, Mai ElSherief, JieyuZhao, Diba Mirza, Elizabeth M. Belding, Kai-Wei Chang, and William Yang Wang. 2020.Towards understanding gender bias in relation extraction. In Proceedings of the 58thAnnual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July5-10, 2020, pages 2943–2953. Association for Computational Linguistics.GDPR. General data protection regulation. https://gdpr-info.eu/.Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. 2020.Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXivpreprint arXiv:2009.11462.Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. 2022. Transformer feed-forwardlayers build predictions by promoting concepts in the vocabulary space. In Proceedings ofthe 2022 Conference on Empirical Methods in Natural Language Processing, pages 30–45.Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer feed-forwardlayers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methodsin Natural Language Processing, pages 5484–5495.Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds,Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. 2022. Im-proving alignment of dialogue agents via targeted human judgements. arXiv preprintarXiv:2209.14375.58Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Nahyeon Ryu, and MarcDymetman. 2023. Aligning language models with preferences through f-divergence mini-mization. arXiv preprint arXiv:2302.08215.Shreya Goyal, Sumanth Doddapaneni, Mitesh M Khapra, and Balaraman Ravindran. 2023. Asurvey of adversarial defenses and robustness in nlp. ACM Computing Surveys, 55(14s):1–39.Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and MarioFritz. 2023. More than you’ve asked for: A comprehensive analysis of novel prompt injectionthreats to application-integrated large language models. arXiv preprint arXiv:2302.12173.Shangwei Guo, Chunlong Xie, Jiwei Li, Lingjuan Lyu, and Tianwei Zhang. 2022. Threats topre-trained language models: Survey and taxonomy. arXiv preprint arXiv:2202.06862.Samyak Gupta, Yangsibo Huang, Zexuan Zhong, Tianyu Gao, Kai Li, and Danqi Chen. 2022.Recovering private text in federated learning of language models. Advances in NeuralInformation Processing Systems, 35:8130–8143.Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan.2017. Inverse reward design. Advances in neural information processing systems, 30.Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and EceKamar. 2022. Toxigen: A large-scale machine-generated dataset for adversarial and implicithate speech detection. In Proceedings of the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May22-27, 2022, pages 3309–3326. Association for Computational Linguistics.Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and JacobSteinhardt. 2020. Aligning ai with shared human values. arXiv preprint arXiv:2008.02275.Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. 2021. Unsolvedproblems in ml safety. arXiv preprint arXiv:2109.13916.Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. 2023. An overview of catastrophicai risks. arXiv preprint arXiv:2306.12001.Seth Herd. 2023. Agentized LLMs will change the alignment landscape. https://www.lesswrong.com/posts/dcoxvEhAfYcov2LA6/agentized-llms-will-change-the-alignment-landscape.Sorami Hisamoto, Matt Post, and Kevin Duh. 2020. Membership inference attacks on sequence-to-sequence models: Is my data in your machine translation system? Transactions of theAssociation for Computational Linguistics, 8:49–63.Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022. Large language models are reasoningteachers. arXiv preprint arXiv:2212.10071.Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, ElizaRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.59Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, VeredCohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022. True:Re-evaluating factual consistency evaluation. arXiv preprint arXiv:2204.04991.Saghar Hosseini, Hamid Palangi, and Ahmed Hassan Awadallah. 2023. An empirical study ofmetrics to measure representational harms in pre-trained language models. arXiv preprintarXiv:2301.09211.Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language models aszero-shot planners: Extracting actionable knowledge for embodied agents. In InternationalConference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland,USA, volume 162 of Proceedings of Machine Learning Research, pages 9118–9147. PMLR.Yue Huang, Qihui Zhang, Lichao Sun, et al. 2023. Trustgpt: A benchmark for trustworthyand responsible large language models. arXiv preprint arXiv:2306.11507.Yufei Huang and Deyi Xiong. 2023. Cbbq: A chinese bias benchmark dataset curated withhuman-ai collaboration for large language models. arXiv preprint arXiv:2306.16244.Evan Hubinger. 2019a. Concrete experiments in inner alignment. https://www.lesswrong.com/posts/uSdPa9nrSgmXCtdKN/concrete-experiments-in-inner-alignment.Evan Hubinger. 2019b. Relaxed adversarial training for inner alignment.https://www.lesswrong.com/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment.Evan Hubinger. 2020a. AI safety via market making. https://www.lesswrong.com/posts/YWwzccGbcHMJMpT45/ai-safety-via-market-making.Evan Hubinger. 2020b. An overview of 11 proposals for building safe advanced ai. arXivpreprint arXiv:2012.07532.Evan Hubinger. 2022a. A transparency and interpretability tech tree.https://www.lesswrong.com/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree.Evan Hubinger. 2022b. Monitoring for deceptive alignment. https://www.lesswrong.com/posts/Km9sHjHTsBdbgwKyi/monitoring-for-deceptive-alignment.Evan Hubinger, Chris van Merwijk, Vlad Mikulik, Joar Skalse, and Scott Garrabrant.2019a. Deceptive alignment. https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB/p/zthDPAjh9w6Ytbeks.Evan Hubinger, Chris van Merwijk, Vlad Mikulik, Joar Skalse, and Scott Garrabrant. 2019b.The inner alignment problem. https://www.lesswrong.com/posts/pL56xPoniLvtMDQ4J/the-inner-alignment-problem.60Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant.2019c. Risks from learned optimization in advanced machine learning systems. CoRR,abs/1906.01820.Geoffrey Irving and Amanda Askell. 2019. Ai safety needs social scientists. Distill, 4(2):e14.Geoffrey Irving, Paul Christiano, and Dario Amodei. 2018. Ai safety via debate. arXivpreprint arXiv:1805.00899.Kwan Yuen Iu and Vanessa Man-Yi Wong. 2023. ChatGPT by openai: The end of litigationlawyers? Available at SSRN.Ganesh Jawahar, Muhammad Abdul-Mageed, and Laks V. S. Lakshmanan. 2020. Automaticdetection of machine generated text: A critical survey.Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural languagegeneration. ACM Computing Surveys, 55(12):1–38.Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. 2023. Automaticallyauditing large language models via discrete optimization. arXiv preprint arXiv:2303.04381.Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, and Nicholas Carlini. 2023. Backdoorattacks for in-context learning with language models. arXiv preprint arXiv:2307.14692.Jonathan W Kanter, Monnica T Williams, Adam M Kuczynski, Katherine E Manbeck,Marlena Debreaux, and Daniel C Rosen. A preliminary report on the relationship betweenmicroaggressions against black people and racism among white college students.Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, RewonChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws forneural language models. arXiv preprint arXiv:2001.08361.Brendan Kennedy, Mohammad Atari, Aida Mostafazadeh Davani, Leigh Yeh, Ali Omrani,Yehsong Kim, Kris Coombs, Shreya Havaldar, Gwenyth Portillo-Wightman, Elaine Gon-zalez, Joe Hoover, Aida Azatian, Alyzeh Hussain, Austin Lara, Gabriel Cardenas, AdamOmary, Christina Park, Xin Wang, Clarisa Wijaya, Yong Zhang, Beth Meyerowitz, andMorteza Dehghani. 2022. Introducing the gab hate corpus: defining and applying hate-basedrhetoric to social media posts at scale. Language Resources and Evaluation, 56(1):79–108.Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, andGeoffrey Irving. 2021. Alignment of language agents. arXiv preprint arXiv:2103.14659.Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher.2019. CTRL: A conditional transformer language model for controllable generation. CoRR,abs/1909.05858.Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang MinYoo, and Minjoon Seo. 2023. Aligning large language models through synthetic feedback.arXiv preprint arXiv:2305.13735.61J Kirchner, L Smith, and J Thibodeau. 2022. Understanding ai alignment research: Asystematic analysis. arXiv preprint arXiv:2206.02841.Svetlana Kiritchenko and Saif M. Mohammad. 2018. Examining gender and race bias in twohundred sentiment analysis systems. In Proceedings of the Seventh Joint Conference onLexical and Computational Semantics, *SEM@NAACL-HLT 2018, New Orleans, Louisiana,USA, June 5-6, 2018, pages 43–53. Association for Computational Linguistics.Victoria Krakovna. 2022. Paradigms of ai alignment: components and enablers.Victoria Krakovna and Janos Kramar. 2023. Power-seeking can be probable and predictivefor trained agents. arXiv preprint arXiv:2304.06528.Sumit Kumar and Raj Ratn Pranesh. 2021. Tweetblm: A hate speech dataset and analysisof black lives matter-related microblogs on twitter. arXiv preprint arXiv:2108.12521.Philippe Laban, Tobias Schnabel, Paul N Bennett, and Marti A Hearst. 2022. Summac:Re-visiting nli-based models for inconsistency detection in summarization. Transactions ofthe Association for Computational Linguistics, 10:163–177.Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi,and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended textgeneration. Advances in Neural Information Processing Systems, 35:34586–34599.Beren Millidge Lee Sharkey, Dan Braun. 2022a. Taking features out of superposition withsparse autoencoders.Beren Millidge Lee Sharkey, Sid Black. 2022b. Current themes in mechanistic interpretabilityresearch.Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, and Byron C Wallace. 2021. Doesbert pretrained on clinical notes reveal sensitive data? arXiv preprint arXiv:2104.07762.Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg.2018. Scalable agent alignment via reward modeling: a research direction. arXiv preprintarXiv:1811.07871.Jan Leike, Miljan Martic, Victoria Krakovna, Pedro A Ortega, Tom Everitt, AndrewLefrancq, Laurent Orseau, and Shane Legg. 2017. Ai safety gridworlds. arXiv preprintarXiv:1711.09883.Hector J. Levesque. 2011. The winograd schema challenge. In Logical Formalizations ofCommonsense Reasoning, Papers from the 2011 AAAI Spring Symposium, Technical ReportSS-11-06, Stanford, California, USA, March 21-23, 2011. AAAI.Mike Lewis, Denis Yarats, Yann N. Dauphin, Devi Parikh, and Dhruv Batra. 2017. Deal orno deal? end-to-end learning for negotiation dialogues.Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. 2023a. Multi-step jailbreakingprivacy attacks on ChatGPT. arXiv preprint arXiv:2304.05197.62Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023b.Inference-time intervention: Eliciting truthful answers from a language model. arXivpreprint arXiv:2306.03341.Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, and Xipeng Qiu. 2021a.Backdoor attacks on pre-trained models by layerwise weight poisoning. arXiv preprintarXiv:2108.13888.Ruosen Li, Teerth Patel, and Xinya Du. 2023c. Prd: Peer rank and discussion improve largelanguage model based evaluations. arXiv preprint arXiv:2307.02762.Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu, andJialiang Lu. 2021b. Hidden backdoors in human-centric language models. In Proceedingsof the 2021 ACM SIGSAC Conference on Computer and Communications Security, pages3123–3140.Tao Li, Tushar Khot, Daniel Khashabi, Ashish Sabharwal, and Vivek Srikumar. 2020. Un-qovering stereotyping biases via underspecified questions. arXiv preprint arXiv:2010.02428.Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin,Percy Liang, and Tatsunori B. Hashimoto. 2023d. Alpacaeval: An automatic evaluator ofinstruction-following models. https://github.com/tatsu-lab/alpaca_eval.Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. 2022. Backdoor learning: A survey.IEEE Transactions on Neural Networks and Learning Systems.Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng Yan.2023e. Guiding large language models via directional stimulus prompting. arXiv preprintarXiv:2302.11520.Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluationof language models. arXiv preprint arXiv:2211.09110.Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang,Zhaopeng Tu, and Shuming Shi. 2023. Encouraging divergent thinking in large languagemodels through multi-agent debate. arXiv preprint arXiv:2305.19118.Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee,Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let’s verify step by step.arXiv preprint arXiv:2305.20050.Anne-Laure Ligozat, Julien Lefèvre, Aurélie Bugeau, and Jacques Combaz. 2021. Unrav-eling the hidden environmental impacts of ai solutions for environment. arXiv preprintarXiv:2110.11822.Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how modelsmimic human falsehoods. arXiv preprint arXiv:2109.07958.63Zachary C. Lipton. 2017. The mythos of model interpretability.Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023a. Chain of hindsight aligns languagemodels with feedback. arXiv preprint arXiv:2302.02676.Ruibo Liu, Chenyan Jia, Ge Zhang, Ziyu Zhuang, Tony Liu, and Soroush Vosoughi. 2022a.Second thoughts are best: Learning to re-align with human values from text edits. Advancesin Neural Information Processing Systems, 35:181–196.Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang,and Soroush Vosoughi. 2023b. Training socially aligned language models in simulatedhuman society. arXiv preprint arXiv:2305.16960.Ruibo Liu, Ge Zhang, Xinyu Feng, and Soroush Vosoughi. 2022b. Aligning generativelanguage models with human values. In Findings of the Association for ComputationalLinguistics: NAACL 2022, pages 241–252.Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023c.GPTeval: Nlg evaluation using GPT-4 with better human alignment. arXiv preprintarXiv:2303.16634.Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng,Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. 2023d. Trustworthy llms: asurvey and guideline for evaluating large language models’ alignment. arXiv preprintarXiv:2308.05374.Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, YanZheng, and Yang Liu. 2023e. Prompt injection attack against llm-integrated applications.arXiv preprint arXiv:2306.05499.Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimizedBERT pretraining approach. CoRR, abs/1907.11692.Nicholas Lourie, Ronan Le Bras, and Yejin Choi. 2021. Scruples: A corpus of communityethical judgments on 32,000 real-life anecdotes. In Proceedings of the AAAI Conference onArtificial Intelligence, volume 35, pages 13470–13479.Li Lucy and David Bamman. 2021. Gender and representation bias in GPT-3 generatedstories. In Proceedings of the Third Workshop on Narrative Understanding, pages 48–55.Sean MacAvaney, Hao-Ren Yao, Eugene Yang, Katina Russell, Nazli Goharian, and OphirFrieder. 2019. Hate speech detection: Challenges and solutions. PloS one, 14(8):e0221152.Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and AliakseiSeveryn. 2022. Teaching small language models to reason. arXiv preprint arXiv:2212.08410.Veronica Root Martinez. 2020. More meaningful ethics. U. Chi. L. Rev. Online, page 53.64RT McAllister, Yarin Gal, Alex Kendall, Mark Van Der Wilk, Amar Shah, Roberto Cipolla,and Adrian Weller. 2017. Concrete problems for autonomous vehicle safety: Advantages ofbayesian deep learning. International Joint Conferences on Artificial Intelligence, Inc.Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating andediting factual associations in GPT. Advances in Neural Information Processing Systems,35:17359–17372.Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pa-sunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celiky-ilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. 2023. Augmented languagemodels: a survey. CoRR, abs/2302.07842.Vladimir Mikulik. 2019. 2-d robustness. https://www.alignmentforum.org/posts/2mhFMgtAjFJesaSYR/2-d-robustness.Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mo-hit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grainedatomic evaluation of factual precision in long form text generation. arXiv preprintarXiv:2305.14251.Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor Berg-Kirkpatrick, andReza Shokri. 2022. Quantifying privacy risks of masked language models using membershipinference attacks. arXiv preprint arXiv:2203.03929.Fatemehsadat Mireshghallah, Mohammadkazem Taram, Praneeth Vepakomma, AbhishekSingh, Ramesh Raskar, and Hadi Esmaeilzadeh. 2020. Privacy in deep learning: A survey.arXiv preprint arXiv:2004.12254.Kevin L Nadal. 2018. Microaggressions and traumatic stress: Theory, research, and clinicaltreatment. American Psychological Association.Kevin L Nadal, Katie E Griffin, Yinglee Wong, Sahran Hamit, and Morgan Rasmus. 2014.The impact of racial microaggressions on mental health: Counseling implications for clientsof color. Journal of Counseling & Development, 92:57.Moin Nadeem, Anna Bethke, and Siva Reddy. 2020. Stereoset: Measuring stereotypical biasin pretrained language models. arXiv preprint arXiv:2004.09456.Neel Nanda. 2022. A comprehensive mechanistic interpretability explainer & glossary.Neel Nanda, Lawrence Chan, Tom Liberum, Jess Smith, and Jacob Steinhardt. 2023. Progressmeasures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217.Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020. Crows-pairs: Achallenge dataset for measuring social biases in masked language models. In Proceedingsof the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP2020, Online, November 16-20, 2020, pages 1953–1967. Association for ComputationalLinguistics.65Andrew Y Ng and Stuart J Russell. 2000. Algorithms for inverse reinforcement learning.In Proceedings of the Seventeenth International Conference on Machine Learning, pages663–670.Richard Ngo. 2022. The alignment problem from a deep learning perspective. arXiv preprintarXiv:2209.00626.Debora Nozza, Federico Bianchi, Dirk Hovy, et al. 2021. Honest: Measuring hurtful sentencecompletion in language models. In Proceedings of the 2021 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human LanguageTechnologies. Association for Computational Linguistics.Chris Olah. 2022. Mechanistic interpretability, variables, and the importance of interpretablebases.Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, TomHenighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, DawnDrain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones,Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,Jared Kaplan, Sam McCandlish, and Chris Olah. 2022. In-context learning and inductionheads. Transformer Circuits Thread.Stephen M Omohundro. 2008. The basic ai drives. In AGI, volume 171, pages 483–492.OpenAI. 2022. Introducing ChatGPT. https://openai.com/blog/chatgpt/.OpenAI. 2023a. GPT-4 technical report. arXiv preprint arXiv:2303.08774.OpenAI. 2023b. Introducing Superalignment. https://openai.com/blog/introducing-superalignment.Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training languagemodels to follow instructions with human feedback. Advances in Neural InformationProcessing Systems, 35:27730–27744.Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William YangWang. 2023. Automatically correcting large language models: Surveying the landscape ofdiverse self-correction strategies. arXiv preprint arXiv:2308.03188.Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, JanaThompson, Phu Mon Htut, and Samuel R. Bowman. 2022. BBQ: A hand-built biasbenchmark for question answering. In Findings of the Association for ComputationalLinguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 2086–2105. Associationfor Computational Linguistics.Ethan Perez, Sam Ringer, Kamilė Lukošiūtė, Karina Nguyen, Edwin Chen, Scott Heiner,Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. 2022. Discoveringlanguage model behaviors with model-written evaluations. arXiv preprint arXiv:2212.09251.66Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, andSumit Gulwani. 2022. Synchromesh: Reliable code generation from pre-trained languagemodels. In The Tenth International Conference on Learning Representations, ICLR 2022,Virtual Event, April 25-29, 2022. OpenReview.net.Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang, andMaosong Sun. 2021. Hidden killer: Invisible textual backdoor attacks with syntactic trigger.arXiv preprint arXiv:2105.12400.Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, and Prateek Mittal. 2023. Vi-sual adversarial examples jailbreak large language models. arXiv preprint arXiv:2306.13213.Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng,Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, ChengQian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang,Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, XinCong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li,Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun.2023. Tool learning with foundation models. CoRR, abs/2304.08354.Shilin Qiu, Qihe Liu, Shijie Zhou, and Wen Huang. 2022. Adversarial attack and defensetechnologies in natural language processing: A survey. Neurocomputing, 492:278–307.Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improvinglanguage understanding by generative pre-training.Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.Ansh Radhakrishnan. 2022. RLHF. https://www.lesswrong.com/posts/rQH4gRmPMJyjtMpTn/rlhf.Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scalinglanguage models: Methods, analysis & insights from training gopher. arXiv preprintarXiv:2112.11446.Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, andChelsea Finn. 2023. Direct preference optimization: Your language model is secretly areward model. arXiv preprint arXiv:2305.18290.Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning witha unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1–140:67.Amir H Razavi, Diana Inkpen, Sasha Uritsky, and Stan Matwin. 2010. Offensive languagedetection using multi-level classification. In Advances in Artificial Intelligence: 23rdCanadian Conference on Artificial Intelligence, Canadian AI 2010, Ottawa, Canada, May31–June 2, 2010. Proceedings 23, pages 16–27. Springer.67Adithya Renduchintala and Adina Williams. 2021. Investigating failures of automatictranslation in the case of unambiguous gender. CoRR, abs/2104.07838.Maria Rigaki and Sebastian Garcia. 2020. A survey of privacy attacks in machine learning.arXiv preprint arXiv:2007.07646.Björn Ross, Michael Rist, Guillermo Carbonell, Benjamin Cabrera, Nils Kurowsky, andMichael Wojatzki. 2017. Measuring the reliability of hate speech annotations: The case ofthe european refugee crisis. arXiv preprint arXiv:1701.08118.Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. 2018. Genderbias in coreference resolution. arXiv preprint arXiv:1804.09301.Stuart J Russell. 1998. Learning agents for uncertain environments. In Proceedings of theeleventh annual conference on Computational learning theory, pages 101–103.Stuart J Russell and Peter Norvig. 2010. Artificial intelligence a modern approach. PearsonEducation, Inc.Muniba Saleem and Craig A Anderson. Arabs as terrorists: Effects of stereotypes withinviolent contexts on attitudes, perceptions, and affect.Jonas B Sandbrink. 2023. Artificial intelligence and biological misuse: Differentiating risks oflanguage models and biological design tools. arXiv preprint arXiv:2306.13952.Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A Smith, and Yejin Choi.2019. Social bias frames: Reasoning about social and power implications of language. arXivpreprint arXiv:1911.03891.Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow,Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, JonathanTow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammana-manchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral,Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, HuuNguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon,Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, AdiSimhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav,Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel vanStrien, David Ifeoluwa Adelani, and et al. 2022. BLOOM: A 176b-parameter open-accessmultilingual language model. CoRR, abs/2211.05100.Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen,Kyunghyun Cho, and Ethan Perez. 2023. Training language models with language feedbackat scale. arXiv preprint arXiv:2303.16755.Timo Schick, Sahana Udupa, and Hinrich Schütze. 2021. Self-diagnosis and self-debiasing:A proposal for reducing corpus-based bias in nlp. Transactions of the Association forComputational Linguistics, 9:1408–1424.68Anna Schmidt and Michael Wiegand. 2017. A survey on hate speech detection using naturallanguage processing. In Proceedings of the Fifth International Workshop on NaturalLanguage Processing for Social Media, SocialNLP@EACL 2017, Valencia, Spain, April 3,2017, pages 1–10. Association for Computational Linguistics.John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017.Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.Shalom H Schwartz, Jan Cieciuch, Michele Vecchione, Eldad Davidov, Ronald Fischer, Con-stanze Beierlein, Alice Ramos, Markku Verkasalo, Jan-Erik Lönnqvist, Kursad Demirutku,et al. 2012. Refining the theory of basic individual values. Journal of personality and socialpsychology, 103(4):663.Charbel-Raphaël Segerie. 2023. Task decomposition for scalable oversight (AGISF Distil-lation). https://www.lesswrong.com/posts/FFz6H35Gy6BArHxkc/task-decomposition-for-scalable-oversight-agisf-distillation.Deven Shah, H Andrew Schwartz, and Dirk Hovy. 2019. Predictive biases in natural languageprocessing models: A conceptual framework and overview. arXiv preprint arXiv:1912.11078.Rohin Shah. 2023. Categorizing failures as “outer” or “inner” misalignment is of-ten confused. https://www.lesswrong.com/posts/JKwrDwsaRiSxTv9ur/categorizing-failures-as-outer-or-inner-misalignment-is.Rohin Shah, Vikrant Varma, Ramana Kumar, Mary Phuong, Victoria Krakovna, JonathanUesato, and Zac Kenton. 2022. Goal misgeneralization: Why correct specifications aren’tenough for correct goals. arXiv preprint arXiv:2210.01790.Xuan Sheng, Zhaoyang Han, Piji Li, and Xiangmao Chang. 2022. A survey on backdoorattack and defense in natural language processing. In 2022 IEEE 22nd InternationalConference on Software Quality, Reliability and Security (QRS), pages 809–820. IEEE.Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, JadeLeung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, et al. 2023.Model evaluation for extreme risks. arXiv preprint arXiv:2305.15324.Jiawen Shi, Yixin Liu, Pan Zhou, and Lichao Sun. 2023. BadGPT: Exploring security vulnera-bilities of ChatGPT via backdoor attacks to InstructGPT. arXiv preprint arXiv:2304.12298.Eric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, and Adina Williams.2022. “i’m sorry to hear that”: Finding new biases in language models with a holisticdescriptor dataset. In Proceedings of the 2022 Conference on Empirical Methods in NaturalLanguage Processing, pages 9180–9211.Nate Soares. 2015a. Aligning superintelligence with human interests: An annotated bibliog-raphy. Intelligence, 17(4):391–444.Nate Soares. 2015b. Research Guide - Machine Intelligence Research Institute. https://intelligence.org/research-guide.69Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, andYu Su. 2022. Llm-planner: Few-shot grounded planning for embodied agents with largelanguage models. CoRR, abs/2212.04088.Congzheng Song and Ananth Raghunathan. 2020. Information leakage in embedding models.In Proceedings of the 2020 ACM SIGSAC conference on computer and communicationssecurity, pages 377–390.Congzheng Song and Vitaly Shmatikov. 2019. Auditing data provenance in text-generationmodels. In Proceedings of the 25th ACM SIGKDD International Conference on KnowledgeDiscovery & Data Mining, pages 196–206.Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and HoufengWang. 2023. Preference ranking optimization for human alignment. arXiv preprintarXiv:2306.17492.Wiktor Soral, Michał Bilewicz, and Mikołaj Winiewski. 2018. Exposure to hate speechincreases prejudice through desensitization. Aggressive behavior, 44(2):136–146.Samuel Sousa and Roman Kern. 2023. How to keep text private? a systematic review of deeplearning methods for privacy-preserving natural language processing. Artificial IntelligenceReview, 56(2):1427–1492.Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al.2022. Beyond the imitation game: Quantifying and extrapolating the capabilities oflanguage models. arXiv preprint arXiv:2206.04615.Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. 2019. Evaluating gender biasin machine translation. In Proceedings of the 57th Conference of the Association forComputational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1:Long Papers, pages 1679–1684. Association for Computational Linguistics.Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, AlecRadford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with humanfeedback. Advances in Neural Information Processing Systems, 33:3008–3021.Jonathan Stray. 2020. Aligning ai optimization to community well-being. InternationalJournal of Community Well-Being, 3(4):443–463.Jonathan Stray, Ivan Vendrov, Jeremy Nixon, Steven Adler, and Dylan Hadfield-Menell. 2021.What are you optimizing for? aligning recommender systems with human values. arXivpreprint arXiv:2107.10939.Derald Wing Sue, Christina M Capodilupo, Gina C Torino, Jennifer M Bucceri, AishaHolder, Kevin L Nadal, and Marta Esquilin. 2007. Racial microaggressions in everydaylife: Implications for clinical practice. American Psychologist, 62(4):271–286.70Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and Minlie Huang. 2023a. Safetyassessment of chinese large language models. arXiv preprint arXiv:2304.10436.Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox,Yiming Yang, and Chuang Gan. 2023b. Principle-driven self-alignment of language modelsfrom scratch with minimal human supervision. arXiv preprint arXiv:2305.03047.Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding thecapabilities, limitations, and societal impact of large language models.Yi Tay, Donovan Ong, Jie Fu, Alvin Chan, Nancy Chen, Anh Tuan Luu, and Christopher Pal.2020. Would you rather? a new benchmark for learning machine alignment with culturalvalues and social preferences. In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics, pages 5369–5373.Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, AurélienRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Openand efficient foundation language models. CoRR, abs/2302.13971.Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, LukasBlecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, JudeFernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, NamanGoyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, ViktorKerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao,Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, AndrewPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic,Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tunedchat models. CoRR, abs/2307.09288.Alex Turner, Logan Smith, Rohin Shah, Andrew Critch, and Prasad Tadepalli. 2021. Optimalpolicies tend to seek power. In Advances in Neural Information Processing Systems,volume 34, pages 23063–23074. Curran Associates, Inc.Alex Turner and Prasad Tadepalli. 2022. Parametrically retargetable decision-makers tendto seek power. In Advances in Neural Information Processing Systems, volume 35, pages31391–31401. Curran Associates, Inc.Aimee Van Wynsberghe. 2021. Sustainable ai: Ai for sustainability and the sustainability ofai. AI and Ethics, 1(3):213–218.71Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the31st International Conference on Neural Information Processing Systems, pages 6000–6010.Bertie Vidgen, Tristan Thrush, Zeerak Waseem, and Douwe Kiela. 2021. Learning from theworst: Dynamically generated datasets to improve online hate detection. In Proceedingsof the 59th Annual Meeting of the Association for Computational Linguistics and the11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021,(Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 1667–1682. Associationfor Computational Linguistics.Giulia Vilone and Luca Longo. 2020. Explainable artificial intelligence: a systematic review.Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universaladversarial triggers for attacking and analyzing nlp. arXiv preprint arXiv:1908.07125.Eric Wallace, Tony Z Zhao, Shi Feng, and Sameer Singh. 2020. Concealed data poisoningattacks on nlp models. arXiv preprint arXiv:2010.12563.Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.2022a. Interpretability in the wild: a circuit for indirect object identification in GPT-2small. In The Eleventh International Conference on Learning Representations.Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu,and Zhifang Sui. 2023a. Large language models are not fair evaluators. arXiv preprintarXiv:2305.17926.Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, ChaoyaJiang, Rui Xie, Jindong Wang, Xing Xie, et al. 2023b. Pandalm: An automatic evaluationbenchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087.Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi RaghaviChandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. 2023c. Howfar can camels go? exploring the state of instruction tuning on open resources. arXivpreprint arXiv:2306.04751.Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,and Hannaneh Hajishirzi. 2022b. Self-instruct: Aligning language model with self generatedinstructions. arXiv preprint arXiv:2212.10560.Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, LifengShang, Xin Jiang, and Qun Liu. 2023d. Aligning large language models with human: Asurvey. arXiv preprint arXiv:2307.12966.William Warner and Julia Hirschberg. 2012. Detecting hate speech on the world wide web.NAACL-HLT 2012, page 19.72Zeerak Waseem. 2016. Are you a racist or am I seeing things? annotator influence onhate speech detection on twitter. In Proceedings of the First Workshop on NLP andComputational Social Science, NLP+CSS@EMNLP 2016, Austin, TX, USA, November 5,2016, pages 138–142. Association for Computational Linguistics.Zeerak Waseem and Dirk Hovy. 2016. Hateful symbols or hateful people? predictive featuresfor hate speech detection on twitter. In Proceedings of the NAACL student researchworkshop, pages 88–93.Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large languagemodels. Advances in Neural Information Processing Systems, 35:24824–24837.Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-SenHuang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. 2021. Ethical andsocial risks of harm from language models. arXiv preprint arXiv:2112.04359.John Wentworth. 2020. “Inner alignment failures” which are actually outer alignmentfailures. https://www.lesswrong.com/posts/HYERofGZE6j9Tuigi/inner-alignment-failures-which-are-actually-outer-alignment.Norbert Wiener. 1960. Some moral and technical consequences of automation: As machineslearn they may develop unforeseen strategies at rates that baffle their programmers. Science,131(3410):1355–1358.Ellery Wulczyn, Nithum Thain, and Lucas Dixon. 2017. Ex machina: Personal attacks seenat scale. In Proceedings of the 26th international conference on world wide web, pages1391–1399.Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao,and Daxin Jiang. 2023a. Wizardlm: Empowering large language models to follow complexinstructions. arXiv preprint arXiv:2304.12244.Canwen Xu, Zexue He, Zhankui He, and Julian McAuley. 2022. Leashing the inner demons:Self-detoxification for language models. In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 36, pages 11530–11537.Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023b. A critical evaluation ofevaluations for long-form question answering. arXiv preprint arXiv:2305.18201.Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. 2020. Recipesfor safety in open-domain chatbots. arXiv preprint arXiv:2010.07079.Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. 2021. Be carefulabout poisoned word embeddings: Exploring the vulnerability of the embedding layers innlp models. arXiv preprint arXiv:2103.15543.73Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, andQuoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding.In Advances in Neural Information Processing Systems 32: Annual Conference on NeuralInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver,BC, Canada, pages 5754–5764.Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, YongraeJo, James Thorne, Juho Kim, and Minjoon Seo. 2023. Flask: Fine-grained language modelevaluation based on alignment skill sets. arXiv preprint arXiv:2307.10928.Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023.Rrhf: Rank responses to align language models with human feedback without tears. arXivpreprint arXiv:2304.05302.Eliezer Yudkowsky. 2004. Coherent extrapolated volition. Singularity Institute for ArtificialIntelligence.Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and RiteshKumar. 2019. Predicting the type and target of offensive posts in social media. arXivpreprint arXiv:1902.09666.Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang,Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai,Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023. GLM-130B:an open bilingual pre-trained model. In The Eleventh International Conference on LearningRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhangYang, Kaisheng Wang, Xiaoda Zhang, Chen Li, Ziyan Gong, Yifan Yao, Xinjing Huang,Jun Wang, Jianfeng Yu, Qi Guo, Yue Yu, Yan Zhang, Jin Wang, Hengtao Tao, Dasen Yan,Zexuan Yi, Fang Peng, Fangqing Jiang, Han Zhang, Lingfeng Deng, Yehong Zhang, ZheLin, Chao Zhang, Shaojie Zhang, Mingyue Guo, Shanzhi Gu, Gaojun Fan, Yaowei Wang,Xuefeng Jin, Qun Liu, and Yonghong Tian. 2021. Pangu-α: Large-scale autoregressivepretrained chinese language models with auto-parallel computation. CoRR, abs/2104.12369.Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023. Alignscore: Evaluating factualconsistency with a unified alignment function. arXiv preprint arXiv:2305.16739.Chaoning Zhang, Philipp Benz, Chenguo Lin, Adil Karjauv, Jing Wu, and In So Kweon.2021. A survey on universal adversarial attack. arXiv preprint arXiv:2103.01498.Ge Zhang, Yizhi Li, Yaoyao Wu, Linyuan Zhang, Chenghua Lin, Jiayi Geng, Shi Wang, andJie Fu. 2023. Corgi-pm: A chinese corpus for gender bias probing and mitigation. arXivpreprint arXiv:2301.00395.Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott,Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang,74and Luke Zettlemoyer. 2022. OPT: open pre-trained transformer language models. CoRR,abs/2205.01068.Wei Emma Zhang, Quan Z Sheng, Ahoud Alhazmi, and Chenliang Li. 2020. Adversarial at-tacks on deep-learning models in natural language processing: A survey. ACM Transactionson Intelligent Systems and Technology (TIST), 11(3):1–41.Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2018. Genderbias in coreference resolution: Evaluation and debiasing methods. In Proceedings of the2018 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Volume 2 (Short Papers), pages 15–20.Shuai Zhao, Jinming Wen, Luu Anh Tuan, Junbo Zhao, and Jie Fu. 2023a. Prompt as triggersfor backdoor attack: Examining the vulnerability in language models. arXiv preprintarXiv:2305.01219.Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, YingqianMin, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023b. A survey of large languagemodels. arXiv preprint arXiv:2303.18223.Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter JLiu. 2023c. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprintarXiv:2305.10425.Yao Zhao, Misha Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter JLiu. 2022. Calibrating sequence likelihood improves conditional language generation. arXivpreprint arXiv:2210.00045.Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, YonghaoZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023a. Judging llm-as-a-judgewith mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685.Rui Zheng, Shihan Dou, Songyang Gao, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, QinLiu, Limao Xiong, Lu Chen, et al. 2023b. Secrets of rlhf in large language models part i:Ppo. arXiv preprint arXiv:2307.04964.Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, AviaEfrat, Ping Yu, Lili Yu, et al. 2023a. Lima: Less is more for alignment. arXiv preprintarXiv:2305.11206.Jingyan Zhou, Jiawen Deng, Fei Mi, Yitong Li, Yasheng Wang, Minlie Huang, Xin Jiang,Qun Liu, and Helen Meng. 2022. Towards identifying social bias in dialog systems: Frame,datasets, and benchmarks. arXiv preprint arXiv:2202.08011.Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. 2023b. Navigating the greyarea: Expressions of overconfidence and uncertainty in language models. arXiv preprintarXiv:2302.13439.75Banghua Zhu, Jiantao Jiao, and Michael I Jordan. 2023. Principled reinforcement learning withhuman feedback from pairwise or k-wise comparisons. arXiv preprint arXiv:2301.11270.Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. 2008. Maximumentropy inverse reinforcement learning. In Aaai, volume 8, pages 1433–1438. Chicago, IL,USA.Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei,Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from humanpreferences. arXiv preprint arXiv:1909.08593.Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023. Universal and transferableadversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043.76",
    "Link": "http://arxiv.org/abs/2309.15025"
}