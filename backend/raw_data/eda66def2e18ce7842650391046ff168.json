{
    "Title": "LLaSM: Large Language and Speech Model",
    "Authors": "Chen, Guangyao, Dong, Siwei, Huang, Wenhao, Shi, Daochen, Shi, Yemin, Shu, Yu, Xiang, Qiqi, Zhang, Ruihua",
    "Year": "No year available",
    "Abstract": "Multi-modal large language models have garnered significant interest\nrecently. Though, most of the works focus on vision-language multi-modal models\nproviding strong capabilities in following vision-and-language instructions.\nHowever, we claim that speech is also an important modality through which\nhumans interact with the world. Hence, it is crucial for a general-purpose\nassistant to be able to follow multi-modal speech-and-language instructions. In\nthis work, we propose Large Language and Speech Model (LLaSM). LLaSM is an\nend-to-end trained large multi-modal speech-language model with cross-modal\nconversational abilities, capable of following speech-and-language\ninstructions. Our early experiments show that LLaSM demonstrates a more\nconvenient and natural way for humans to interact with artificial intelligence.\nSpecifically, we also release a large Speech Instruction Following dataset\nLLaSM-Audio-Instructions. Code and demo are available at\nhttps://github.com/LinkSoul-AI/LLaSM and\nhttps://huggingface.co/spaces/LinkSoul/LLaSM. The LLaSM-Audio-Instructions\ndataset is available at\nhttps://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions",
    "Keywords": "No keywords available",
    "Publisher": "",
    "Publication Date": "No publication date available",
    "Journal": "No journal available",
    "Citation Count": 0,
    "Full Text": "LLASM:LARGE LANGUAGE AND SPEECH MODELYu Shu1*, Siwei Dong1*, Guangyao Chen1,2*, Wenhao Huang3,Ruihua Zhang, Daochen Shi, Qiqi Xiang & Yemin Shi1†1LinkSoul.AI, 2Peking University, 301.ai* Equal contribution† Corresponding author: ymshi@linksoul.aiABSTRACTMulti-modal large language models have garnered significant interest recently. Though, mostof the works focus on vision-language multi-modal models providing strong capabilities in fol-lowing vision-and-language instructions. However, we claim that speech is also an importantmodality through which humans interact with the world. Hence, it is crucial for a general-purposeassistant to be able to follow multi-modal speech-and-language instructions. In this work, wepropose Large Language and Speech Model (LLaSM). LLaSM is an end-to-end trained largemulti-modal speech-language model with cross-modal conversational abilities, capable of follow-ing speech-and-language instructions. Our early experiments show that LLaSM demonstratesa more convenient and natural way for humans to interact with artificial intelligence. Specifi-cally, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions.Code and demo are available at https://github.com/LinkSoul-AI/LLaSM and https://huggingface.co/spaces/LinkSoul/LLaSM. The LLaSM-Audio-Instructions dataset is availableat https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.1 IntroductionSpeech contains semantic information and contains paralinguistic information like intonation at the same time, it carriesmore quantity of information than text. Additionally, speech is a more convenient and natural way for humans tointeract with artificial intelligence. Therefore, following speech-and-language instructions is crucial when developing ageneral-purpose assistant.However, most large language models [1, 2, 3] receive text input only, which restricts the ability of large languagemodels. Vision-and-language multi-modal models [4, 5, 6, 7, 8, 9] offer the ability to understand the vision information,making a huge step toward general artificial intelligence (AGI), but it is still inconvenient for humans to input the tasksby typing a text instruction. The cascading paradigm methods [10, 11] use an automatic speech recognition (ASR)model to convert the speech input into the text input, then the model can process the task with the text input. However, itstill leads to information consumption during the modal transformation from speech to text and might import mistakesof the ASR system. Recently, speech-language multi-modal models [12, 13] focusing on processing and generatingspeech and text with a large language model are capable of understanding and generating multi-modal content. Thespeech signals are encoded into discrete tokens, and then discrete speech tokens are expanded into the vocabulary of theLLM. In this way, the LLM needs to be retrained with plenty of multi-modal data and huge computing resources.In this paper, we propose LLaSM, a large speech-and-language model with cross-modal conversational abilities, capableof understanding and following speech-and-language instructions. Following the manner of LLaVA [6], we leveragethe well-trained speech modal encoder and the LLM, which makes LLaSM more resource-friendly. Specifically, weuse Whisper [14] as a speech encoder to encode the speech signals into embeddings. Then a modal adaptor learns toarXiv:2308.15930v3  [cs.CL]  16 Sep 2023A PREPRINT - SEPTEMBER 19, 2023align speech embeddings with the input text embeddings of the large language model. The speech embeddings andthe text embeddings are concatenated together to form interleaved sequences, then the interleaved sequences are inputto the LLM for supervised fine-tuning. The training process is divided into two stages. In the first stage, we use thepublic ASR datasets for the modality adaptation pre-training. The speech encoder and the LLM are frozen, only themodal adaptor is trained to align the speech and text embeddings. As most of the model parameters remain frozen,only a small part of the parameters from the modal adaptor is trained during this stage, it is not resource-consuming. Inthe second stage, we use cross-modal instruction data for training to provide the model with the capacity to processcross-modal conversations and handle multi-modal instructions. The speech encoder is frozen while the parametersof the modal adaptor and the language model are updated for cross-modal instruction fine-tuning. Worth noting thatexisting open-source speech-text cross-modal instruction-following datasets are scarce, so we build and release a speech-text cross-modal instruction-following dataset LLaSM-Audio-Instructions. The dataset is constructed by carefullyselecting dialogues from GPT4-LLM [15], ShareGPT [16], WizardLM [17], and using text-to-speech technology togenerate a large amount of dialogue audio data. In total, it contains 199k conversations, in which there are 80k Chineseaudio samples and 428k English audio samples, which is the largest Chinese and English speech-text cross-modalinstruction-following dataset to our knowledge.Our paper makes the following contributions:• We build a speech-language multi-modal model that can understand and follow speech-language instructions,which provides a more convenient and natural way for humans to interact with artificial intelligence.• We construct and release LLaSM-Audio-Instrustions, a large-scale Chinese and English speech-text cross-modal instruction-following dataset. We release the data in https://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.• We release the code in https://github.com/LinkSoul-AI/LLaSM and the demo is shown in https://huggingface.co/spaces/LinkSoul/LLaSM.2 Related WorkVision Large Language Model has gained significant traction [4, 5, 6, 7, 8, 9] recently. Most of them leverage thepre-trained LLMs and vision encoders to perform vision tasks. Flamingo [18] aligns a pre-trained vision encoderand language model using gated cross-attention and is trained on billions of image-text pairs. BLIP-2 [19] employs aFlan-T5 [20] with a Q-Former to efficiently align visual features with the language model. Palm-E [5], featuring 562billion parameters, integrates the 540B PaLM [2] and 22B Vision Transformer [21] into the largest vision-languagemodel. LLaVA [6] leverages pre-trained CLIP [22] visual encoder and LLaMA [3] and conducts instruct tuning onGPT4-assisted visual instruction data. GPT-4 [4] also shows powerful visual understanding and reasoning abilities. Thesuccess of the multi-modal large language model in the visual domains has brought a lot of inspiration to the research inthe speech domains as well.Speech Large Language Model has gained more and more interest, for the success of the vision multi-modal LLMs.The cascading paradigm methods [10, 11] use an automatic speech recognition (ASR) model to convert the speechinput into the text input, which still leads to information consumption and might import mistakes of the ASR system.Recently, speech-language multi-modal models [12, 13] focusing on processing and generating speech and text with alarge language model are capable of understanding and generating multi-modal content. The speech signals are encodedinto discrete tokens, and then discrete speech tokens are expanded into the vocabulary of the LLM. In this way, theLLM needs to be retrained with plenty of multi-modal data and huge computing resources.3 Approach3.1 ModelThe focus of training multi-modal models is to fuse cross-modal complementary information of multi-modalities andeffectively exploit the capabilities of well-trained large language models. The LLaSM model architecture is shown inFigure 1. We use Whisper [14] to encode the raw audio data into embeddings first, then a modal adaptor is trainedduring the pre-training stage to align the audio embeddings and the text embeddings. The audio embeddings and thetext embeddings are concatenated together to form interleaved input sequences to input to the large language model. Wechoose Chinese-LLAMA2-7B [23] as our LLM, for its capabilities in both Chinese and English. During the cross-modalinstruction fine-tuning stage, the modal adaptor and the LLM are trained with multi-tasks.2A PREPRINT - SEPTEMBER 19, 2023Figure 1: Model framework of the LLaSMThe pre-training stage. During this stage, the modal encoder and the LLM remain frozen. To enable the LLMto understand the audio embeddings from the modal encoder, the modal adaptor is trained with public ASR datato align the text and the audio embeddings. The data sample (audio data, text label) of ASR data is formatted as atuple of (simple instruction, audio data, text label), in which the simple instruction is an automatic speech recognitioninstruction. According to the different languages of the audio data, an English simple instruction listed in Figure 2or a Chinese simple instruction listed in Figure 3 will be chosen. The unified format of the pre-training multi-modalsequence Xsample is shown in Figure 4. Each data sample is formatted as Xsample, then we will replace the audiopatch embeddings from the text sequence with the audio embeddings of the modal adaptor. The final interleaved inputembeddings will be input to the large language model. The training target is to predict the text label of each data sample.Figure 2: English simple instructions. Figure 3: Chinese simple instructions.The cross-modal instruction fine-tuning. During this stage, only the modal encoder is frozen, the modal adaptor andthe LLM are joint-trained with multi-tasks. We build complex cross-modal instructions using several conversationaldata. The questions from humans are generated to audio data by using Microsoft Azure text-to-speech API, thenthe training target is to predict the responses from the chatbot. A round of question and answer will be processedinto a multi-modal sequence Xsample, and multiple rounds of question and answer will be concatenated with theEOS token. The unified format of the cross-modal instruction fine-tuning sequence is shown in Figure 5. As theeffectiveness of text-only conversational data with multi-task instructions has been demonstrated in several open-sourcelanguage-only instruction-tuning works [15, 16, 17], the cross-modal instructions are able to improve the capacity offollowing multi-modal instructions.3.2 Data CollectionTo enable the LLM to understand the audio signals, we collect several public ASR data sets to form the ModalityAdaptation Pre-training Data with simple instructions of automatic speech recognition. And, for cross-modal instructiontuning, we use several open-source language-only instruction-tuning data sets to build the Cross-modal InstructionFine-Tuning Data by generating the speech data. The details are as follows.Modality Adaptation Pre-training Data. To align the embeddings of text and audio, we collect several public ASRdata sets in both English and Chinese, including Aishell [24], LibriSpeech [25], Magicdata [26] and Primewords [27].The data sample of ASR data usually consists of a pair of speech audio and text utterances, especially, when we add asimple instruction to the data sample as the task instruction. These simple instructions are listed in Figure 2 and Figure3, which are different representations of the automatic speech recognition task in both English and Chinese. Whilepre-training, the simple instruction and the audio data are input to the model to predict the text label of the audio data.3A PREPRINT - SEPTEMBER 19, 2023Figure 4: The sample sequence format for the pre-training. We follow the manner of Llama-2, and B_INST = ’[INST]’,E_INST = ’[/INST]’, B_SYS = ’<<SYS>> \\n’, E_SYS = ’\\n <</SYS>> \\n\\n’. The SYSTEM = ’You are ahelpful language and speech assistant. You are able to understand the speech content that the user provides, and assistthe user with a variety of tasks using natural language.’, and the TEXT_LABEL is the text label of the ASR data sample.The audio_token_len is set to 64 by default. Special audio tokens are used, AUDIO_START_TOKEN = ’<au_start>’,AUDIO_END_TOKEN = ’<au_end>’, AUDIO_PATCH_TOKEN = ’<au_patch>’. The contentuser consists of theaudiotoken and the Isimple, in which Isimple is a simple instruction and is randomly put before or after the audiotoken.While training the BOS token and the EOS token will be added to each sample at the beginning and the end of thesequence, only the green tokens are used to compute the loss.Figure 5: The sample sequence format for the cross-modal instruction fine-tuning. We follow the manner of Llama-2,and B_INST = ’[INST]’, E_INST = ’[/INST]’, B_SYS = ’<<SYS>> \\n’, E_SYS = ’\\n <</SYS>> \\n\\n’. The SYS-TEM = ’You are a helpful language and speech assistant. You are able to understand the speech content that the user pro-vides, and assist the user with a variety of tasks using natural language.’, and the TEXT_RESPONSE is the text responsefrom the chatbot. The audio_token_len is set to 64 by default. Special audio tokens are used, AUDIO_START_TOKEN= ’<au_start>’, AUDIO_END_TOKEN = ’<au_end>’, AUDIO_PATCH_TOKEN = ’<au_patch>’. The contentuser isthe audiotoken which will be replaced by the audio embeddings during training. Each round of question and answerwill be formatted as Xsample, which will be concatenated together with the EOS token. While training the BOS tokenwill be added at the beginning of the sequence, and the EOS token will be added at the end of the sequence, only thegreen tokens are used to compute the loss.Cross-modal Instruction Fine-Tuning Data. As the effectiveness of the open-source language-only instruction-tuningdata sets has been demonstrated in previous works[15, 16, 17], a natural idea is to generate audio data of theselanguage-only data sets to build a cross-modal instruction-tuning data. In the process of building this dataset, wefirst carefully filtered all the conversation data, by removing the conversations that are not suitable for vocalization,including codes, a large number of symbols, URLs, and other non-readable text. To ensure the data quality, inthe second stage, all the answers from chat-bots in the conversations are filtered again. Those that do not containvaluable information are dropped. In the third stage, we use Microsoft Azure text-to-speech API [28] to generatespeech data from humans in these data sets. The speech data of humans are used as the complex instructions andthe responses from the chatbot are predicted during the instruction fine-tuning. Specifically, 80k conversation datawhich contains 160k samples is selected from WizardLM [17], 23k conversation data which contains 155k samples isselected from ShareGPT [16] and 96k conversation data which contains 192k samples is selected from GPT-4-LLM[15]. Table 1 shows the specific details of the dataset, which contains 199k conversation data and 508k samples intotal. Several examples of the dataset are shown in Figure 6. We release this dataset as LLaSM-Audio-Instructions athttps://huggingface.co/datasets/LinkSoul/LLaSM-Audio-Instructions.4A PREPRINT - SEPTEMBER 19, 2023Table 1: LLaSM-Audio-Instructions Data.LLaSM-Audio-InstructionsSource Conversations Samples English Samples Chinese SamplesWizardLM 80k 160k 159k <1kShareGPT 23k 155k 140k 15kGPT-4-LLM 96k 192k 128k 64kTotal 199k 508k 428k 80kFigure 6: Data samples of the LLaSM-Audio-Instructions.5A PREPRINT - SEPTEMBER 19, 2023Figure 7: Examples of experiments.6A PREPRINT - SEPTEMBER 19, 20234 ExperimentsAs shown in Figure 7, our proposed model, LLaSM, can adaptively recognize and respond to speech in Chineseand English. Figure 7 further demonstrates the effectiveness of LLaSM in a bilingual setting. Unlike conventionalmodels that rely on speech-to-text conversion as a preprocessing step, LLaSM can directly process speech inputs,which improves its execution efficiency. Furthermore, LLaSM can support multiple languages and scenarios, whichexpands its application range. Therefore, LLaSM is a promising model for convenient and interactive human-artificialintelligence communication.5 ConclusionThis work presents LLaSM, a large language model with cross-modal conversational abilities, capable of understandingand following speech-and-language instructions. Experiments show that LLaSM demonstrates a more convenient andnatural way for humans to interact with artificial intelligence. Specifically, to alleviate the scarcity of cross-modal speech-and-language instructions data, we build a large Speech Instruction Following data set LLaSM-Audio-Instructions. It isthe largest Chinese and English speech-text cross-modal instruction-following data set to our knowledge. Finally, byadopting a visual modal encoder that can easily provide LLaSM with visual capabilities, we will explore combiningboth vision and audio modalities in future work.References[1] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, SamMcCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.[2] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, SashaTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, XavierGarcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Olek-sandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, MicheleCatasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scalinglanguage modeling with pathways, 2022.[3] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, MoyaChen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, PushkarMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, AlanSchelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, MelanieKambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:Open foundation and fine-tuned chat models, 2023.[4] OpenAI. Gpt-4 technical report, 2023.[5] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, DanielDuckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, IgorMordatch, and Pete Florence. Palm-e: An embodied multimodal language model, 2023.[6] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.[7] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-languageunderstanding with advanced large language models, 2023.7A PREPRINT - SEPTEMBER 19, 2023[8] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, andIshan Misra. Imagebind: One embedding space to bind them all, 2023.[9] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu,Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality, 2023.[10] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving aitasks with chatgpt and its friends in hugging face, 2023.[11] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong,Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, and Shinji Watanabe. Audiogpt: Understanding and generatingspeech, music, sound, and talking head, 2023.[12] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt:Empowering large language models with intrinsic cross-modal conversational abilities, 2023.[13] Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chau-mont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, Hannah Muckenhirn, Dirk Padfield,James Qin, Danny Rozenberg, Tara Sainath, Johan Schalkwyk, Matt Sharifi, Michelle Tadmor Ramanovich, MarcoTagliasacchi, Alexandru Tudor, Mihajlo Velimirović, Damien Vincent, Jiahui Yu, Yongqiang Wang, Vicky Zayats,Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas Zilka, and Christian Frank. Audiopalm: A large languagemodel that can speak and listen, 2023.[14] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speechrecognition via large-scale weak supervision, 2022.[15] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4, 2023.[16] Dom Eccleston. Sharegpt. https://github.com/domeccleston/sharegpt, 2023.[17] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.Wizardlm: Empowering large language models to follow complex instructions, 2023.[18] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, ArthurMensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, ZhitaoGong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Ne-matzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, andKaren Simonyan. Flamingo: a visual language model for few-shot learning, 2022.[19] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training withfrozen image encoders and large language models, 2023.[20] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, XinyunChen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang,Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi,Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetunedlanguage models, 2022.[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. Animage is worth 16x16 words: Transformers for image recognition at scale, 2021.[22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visualmodels from natural language supervision, 2021.[23] Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, and Yemin Shi. Chinesellama2 7b. arXiv, 2023.[24] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng. Aishell-1: An open-source mandarin speech corpusand a speech recognition baseline. In Oriental COCOSDA 2017, page Submitted, 2017.[25] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based onpublic domain audio books. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE InternationalConference on, pages 5206–5210. IEEE, 2015.[26] Magic data technology co., ltd., 2019.[27] Ltd. Primewords Information Technology Co. Primewords chinese corpus set 1, 2018. https://www.primewords.cn.8A PREPRINT - SEPTEMBER 19, 2023[28] Microsoft. Microsoft azure text-to-speech api. https://azure.microsoft.com/en-us/products/ai-services/ai-speech.9",
    "Link": "http://arxiv.org/abs/2308.15930"
}