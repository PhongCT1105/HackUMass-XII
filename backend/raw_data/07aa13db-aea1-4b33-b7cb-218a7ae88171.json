{
    "Title": "Closing the loop between neural network simulators and the OpenAI Gym",
    "Authors": "Jordan, Jakob, Morrison, Abigail, Weidel, Philipp",
    "Year": "No year available",
    "Abstract": "Since the enormous breakthroughs in machine learning over the last decade,\nfunctional neural network models are of growing interest for many researchers\nin the field of computational neuroscience. One major branch of research is\nconcerned with biologically plausible implementations of reinforcement\nlearning, with a variety of different models developed over the recent years.\nHowever, most studies in this area are conducted with custom simulation scripts\nand manually implemented tasks. This makes it hard for other researchers to\nreproduce and build upon previous work and nearly impossible to compare the\nperformance of different learning architectures. In this work, we present a\nnovel approach to solve this problem, connecting benchmark tools from the field\nof machine learning and state-of-the-art neural network simulators from\ncomputational neuroscience. This toolchain enables researchers in both fields\nto make use of well-tested high-performance simulation software supporting\nbiologically plausible neuron, synapse and network models and allows them to\nevaluate and compare their approach on the basis of standardized environments\nof varying complexity. We demonstrate the functionality of the toolchain by\nimplementing a neuronal actor-critic architecture for reinforcement learning in\nthe NEST simulator and successfully training it on two different environments\nfrom the OpenAI Gym",
    "Keywords": "No keywords available",
    "Publisher": "",
    "Publication Date": "No publication date available",
    "Journal": "No journal available",
    "Citation Count": 0,
    "Full Text": "Closing the loop between neural network simulators and the\nOpenAI Gym\nJakob Jordan1,2,3∗† Philipp Weidel2,1,3∗ Abigail Morrison2,1,3\n1 Institute of Neuroscience and Medicine (INM-6), Research Centre Jülich, Jülich, Germany\n2 Institute for Advanced Simulation (IAS-6), Research Centre Jülich, Jülich, Germany\n3 JARA-Institute Brain Structure Function Relationship (JBI 1 / INM-10), Research Centre Jülich, Jülich, Germany\nAbstract\nSince the enormous breakthroughs in machine learning over the last decade, functional neural network\nmodels are of growing interest for many researchers in the field of computational neuroscience. One major\nbranch of research is concerned with biologically plausible implementations of reinforcement learning,\nwith a variety of different models developed over the recent years. However, most studies in this area are\nconducted with custom simulation scripts and manually implemented tasks. This makes it hard for other\nresearchers to reproduce and build upon previous work and nearly impossible to compare the performance of\ndifferent learning architectures. In this work, we present a novel approach to solve this problem, connecting\nbenchmark tools from the field of machine learning and state-of-the-art neural network simulators from\ncomputational neuroscience. This toolchain enables researchers in both fields to make use of well-tested\nhigh-performance simulation software supporting biologically plausible neuron, synapse and network\nmodels and allows them to evaluate and compare their approach on the basis of standardized environments\nof varying complexity. We demonstrate the functionality of the toolchain by implementing a neuronal\nactor-critic architecture for reinforcement learning in the NEST simulator and successfully training it on\ntwo different environments from the OpenAI Gym.\n1 Introduction\nThe last decade has witnessed major progress in the field of machine learning, moving from small-scale toy\nproblems to large-scale real-world applications including image [1] and speech recognition [2], complex\nmotor-control tasks [3] and playing (video) games at super-human performance [4, 5]. This progress has been\ndriven mainly by an increase in computing power, especially by training deep networks on graphics processing\nunits [6], and conceptual breakthroughs like layer-wise pretraining [7, 8] or dropout [9]. Even so, this rate\nof progress would not have been possible without the wide availability of high-performance ready-to-use\ntools, e.g., Torch [10], Theano [11], Caffe [12], TensorFlow [13] and standardized benchmarks and learning\nenvironments, such as the MNIST [14], CIFAR [15] and ImageNET [16] datasets, and the MuJoCo [17], ALE\n[18] and OpenAI Gym [19] toolkits. While ready-to-use tools allow researchers to focus on important aspects\nrather than basic implementation details, standardized benchmarks can guide the community as a whole\n∗These authors contributed equally to this study.\n†Corresponding author: j.jordan@fz-juelich.de\n1\nar\nX\niv\n:1\n70\n9.\n05\n65\n0v\n1 \n [q\n-b\nio.\nNC\n]  \n17\n Se\np 2\n01\n7\ntowards promising approaches, as for example in the case of convolutional networks through the ImageNET\ncompetition [20].\nSimilarly, researchers in the field of computational neuroscience have benefited from the increase of\ncomputational power and achieved many conceptual breakthroughs over the last decade, with a plethora\nof new neuron, synapse and network models being developed. Thanks to a variety of software projects,\nresearchers have access to simulators for all scales of neural systems from molecular simulations [21] over\ncomplex neuron [22, 23] and network models [24–26] to whole brain simulations using neural fields [27].\nHowever, in computational neuroscience no generally accepted set of benchmarks exist so far (but see [28]).\nWhile it is desirable to compare different neural network models with respect to biological plausibility,\nexplanatory power and functional performance, only the latter lends itself to the definition of quantitative\nbenchmarks.\nOne particular area in which the lack of standardized benchmarks is apparent is research into reinforcement\nlearning (RL) in neurobiological substrates. Inspired by behavioural experiments, RL is concerned with\nthe ability of organisms to learn from previous experiences to optimize their behavior in order to maximize\nreward and avoid punishment (see, e.g., [29]). RL has a long tradition in the field of machine learning which\nhas led to several powerful algorithms, such as SARSA and Q-learning [30]. Similarly, a large variety of\nneurobiological models have been proposed in recent years [31–44]. However, only a small proportion of\nthese rely on publicly available simulators and all of them employ custom built environments. Even for fairly\nsimple environments, this has led to a situation where different network models are difficult to compare\nand reproduce, thus creating a fragmentation of research efforts. Instead of building upon and extending\nexisting models, researchers are forced to spend too much time on recreating basic methods for custom\nimplementations.\nThis issue has led to the Human Brain Project (HBP) [45] dedicating significant resources of a subproject\n(SP10, Neurorobotics) to the development of the necessary infrastructure that allows users to conduct robotic\nexperiments in virtual environments and connect these to their neural network implementations with an\neasy to use web interface. This approach however, specifically addresses the need of researchers developing\nneuronal controllers for robotic applications.\nIn contrast, the OpenAI Gym [19] provides a rich and generic collection of standardized RL environ-\nments developed to support the machine learning community in evaluating and comparing algorithms. All\nenvironments are accessible via a simple, unified interface, that requires an agent to supply an action and\nreturns an observation and reward for its current state. The toolkit includes a range of different environments\nwith varying levels of complexity ranging from low-dimensional fully discrete (e.g., FrozenLake) to high-\ndimensional fully continuous tasks (e.g., Humanoid). Consistency of the OpenAI Gym environments across\ndifferent releases supports researchers in reproduction and extension of previous work and allows systematic\nbenchmarking and comparison of learning algorithms and their implementations. The easy accessibility\nof different tasks fosters progress by allowing researchers to focus on learning algorithms instead of basic\nimplementation details of particular environments and provokes researchers to evaluate the performance of\ntheir algorithms on many different tasks.\nTo make this comprehensive resource available to the computational neuroscience community, we\ndeveloped a toolchain to interface neural network simulators with the OpenAI Gym. Using this toolchain,\nresearchers can rely on well-tested, high-performance simulation engines to power their models and evaluate\nthem against a curated set of standardized environments, allowing more time to focus on neurobiological\nquestions.\nIn the next section we introduce additional pre-existing components on which our toolchain relies, and\nafterwards discuss how it links the different tools. We demonstrate its functionality by implementing a neural\nactor-critic in NEST ([24], NEural Simulation Tool) and successfully training it on two different environments\n2\nObservationRLToolkit\nNeural-\nnetwork\nsimulator\nZM\nQ\nM\nU\nSIC\nAction\nReward\nFigure 1: Interfacing RL toolkits with neural network simulators. The RL toolkit (left) is responsible for\nemulating an environment that provides observations and rewards which are communicated via ZeroMQ\nsockets and MUSIC adapters (middle) to a neural network simulator (right). Concurrently, the activity of the\nsimulated neural network is transformed to an action and fed back to the RL toolkit.\nfrom the OpenAI Gym.\n2 Pre-existing components\nNEST is a neural simulator designed for the efficient simulation of large-scale networks of simple neuron\nmodels with biophysically realistic connectivity. The simulation kernel scales from small simulations on\na laptops to super computers, with the largest simulation containing about 109 neurons and 1013 synapses,\ncorresponding to about 10% of the human cortex at the resolution of individual cells and connections [46].\nNEST is actively developed and maintained by the NEST initiative in collaboration with the community and\nfreely available under the GPLv2 and is supported by the HBP with the explicit aim of widespread long-term\navailability and maintainability.\nWhile the network implementation that we present in the results section relies on the NEST simulator,\nthe toolchain can also be used with other simulators that support the MUSIC library, for example NEURON\n[22]. The MUlti-SImulation Coordinator is a multi-purpose middleware for neural network simulators built\non top of MPI (Message Passing Interface) that enables online interaction of different simulation engines\n[47]. MUSIC provides named MPI channels, referred to as MUSIC ports, which allow the user to set up\ncommunication streams between several processes. While originally intented to distribute a single neural\nnetwork model across different simulators, the MUSIC library can also be used to connect neural simulators\nwith robotic applications.\nFor this purpose the ROS-MUSIC Toolchain (RMT) [48] was recently developed, providing an interface\nfrom MUSIC to the Robotic Operating System (ROS) [49]. ROS is the most popular middleware in the\nrobotic community which can interact with many robotic simulators and hardware platforms. The RMT\nallows exchange of well-defined messages between ROS and MUSIC via stand-alone executables, so called\nadapters, that were designed with a focus on modularity. The toolchain contains several different adapters\neach performing a rather simple operation on streams of inputs (e.g., filtering). By concatinating several\nadapters, the overall transformation of the original data can become more complex, for example converting\nhigh-dimensional continuous data (e.g., sensory data) to low-dimensional discrete data (e.g., action potentials)\n3\nor vice-versa. More information and introductory examples can be found on GitHub.1\n3 Results\nTo enable the online interaction of neural network simulators and the OpenAI Gym we rely on two different\nlibraries, MUSIC, to interface with the neural simulator, and ZeroMQ [50] to exchange messages with the\nenvironment simulated in the OpenAI Gym. In the following we describe these two parts of the toolchain\nand demonstrate their functionality by interfacing a neural network simulation in NEST with two different\nenvironments.\n3.1 Extending the ROS - MUSIC toolchain\nWe extended the RMT by adding adapters that support communication via ZeroMQ following a publish-\nsubscribe pattern. ZeroMQ is a messaging library that allows applications to exchange messages at runtime\nvia sockets. Continuously developed by a large community, it offers bindings for a variety of languages\nincluding C++ and Python, and supports most operating systems. A single communication adapter of the RMT\nsends (receives) data via a ZeroMQ socket and receives (sends) data via a MUSIC port. While the adapters\ncan handle arbitrary data, we defined a set of specialized messages in JSON format (see supplementary\nmaterial) specifically designed to communicate observations, rewards and actions as discrete or continuous\nreal-valued variables of arbitrary dimensions, as used in the OpenAI Gym. We chose the JSON format due to\nits simplicity, easy serialization and broad platform support.\nIn addition to the ZeroMQ adapters dedicated for communication with MUSIC, we developed several\nfurther adapters that can perform specific transformations of the data. As discussed above, environments can\nbe defined in continuous or discrete spaces with arbitrary dimensionality. To generate the required closed-loop\nfunctionality, the observations provided by the environment must be consistently transformed to a format that\ncan be fed into neural network simulations. Conversely, the activity of the neural network must be interpreted\nand transformed into valid actions which can be executed in the environment.\nA standard way to address the first issue is to introduce so called place cells. Each of these cells is tuned\nto a preferred (multidimensional) observation, i.e., is highly active for a specific input and less active for other\ninputs [37]. The dependence of the activity of a single place cell on observations is described by its tuning\ncurve, often chosen as a multidimensional Gaussian. To perform the transformation of observations to activity\nof place cells, we implemented a discretize adapter that allows users to specify the position and width of the\ntuning curves of an arbitrary number of place cells. While having a certain biological plausibility [51], one\ndisadvantage of this approach is that the number of place cells required to cover the whole observation space\nevenly scales exponentially in the number of dimensions of the observation. For observations with a small\nnumber of dimensions, however, this approach is very suitable.\nTo perform action selection, we added several adapters that can, respectively, select the most active\nneuron (argmax adapter), threshold the activity across neurons to create a binary vector (threshold adapter) or\nlinearly combine the activity of neurons across many input channels (linear decoder). Depending on the type\nof action required (discrete/continuous) by the environment, the user can select a single one or a combination\nof these. See the documentation of the RMT for detailed specifications of the adapters.\nIn general, we followed the design principle behind the RMT and developed modular adapters. This\nmakes each individual adapter easy to understand and enables users to quickly extend the toolchain with their\n1https://github.com/incf-music/ros-music-adapters\n4\nown adapters. By combining several adapters, the RMT allows arbitrarily complex transformations of the\ndata and can hence be applied to many use-cases.\n3.2 ZeroMQ wrapper for the OpenAI Gym\nThe second part of the toolchain is a Python wrapper around the OpenAI Gym that exposes ZeroMQ sockets\nfor communicating actions, observations and rewards (see figure 1). An environment in the OpenAI Gym is\nupdated in steps. In each step, an agent needs to provide an action and receives an observation and reward for\nits current state. The wrapper consists of four different threads that coordinate: (i) performing steps in an\nenvironment, (ii) receiving actions via a ZeroMQ SUB socket, (iii) publishing observations via a ZeroMQ\nPUB socket and (iv) publishing rewards via a ZeroMQ PUB socket. Before spawning the threads, the wrapper\nstarts a user-specified environment and creates the necessary communication buffers. The thread coordinating\nthe environment reads actions from the corresponding buffer, performs single steps in the environment and\nupdates the observation and reward buffers based on the return values of the environment. Upon detecting\nthat a single episode has ended, e.g., by an agent reaching a certain goal position, it resets the environment\nand allows a user-specified break before starting another episode. The communication threads continuously\nsend(receive) messages via ZeroMQ and read from(write to) the corresponding buffers. All threads can be\nrun with different update intervals, for example, to slow down movement of the agent by performing steps on\na coarse time grid whilst continuously receiving action choices from the neural network simulation running\non a fine time grid. The user can specify a variety of parameters via a configuration file in JSON format (see\nsupplementary material). See the documentation for detailed specifications of the wrapper.\n3.3 Applications\nTo demonstrate the functionality of the toolchain, we implemented a neural network model with actor-critic\narchitecture in NEST and trained it on two different environments simulated in the OpenAI Gym. In the first\ntask, the agent needs to learn to perform a sequence of actions in order to reach the top of a hill in a continous\nenvironment. The second task is a classical grid-world in which an agent needs to learn to navigate to a\ngoal position in a two-dimensional discrete environment with obstacles. We first describe the neural network\narchitecture and learning rule and afterwards discuss the network’s performance on the two tasks.\n3.3.1 Neural network implementation\nWe consider a temporal-difference learning algorithm [29] implemented as an actor-critic architecture,\noriginally using populations of spiking neurons [37]. We translated the spike-based implementation to rate\nneurons, mainly to simplify the implementation by avoiding issues arising from noise introduced by spiking\nneuron models [52, 37]. The neuron dynamics we considered here are determined by the following stochastic\ndifferential equation:\nτ\ndzi(t)\ndt\n= −zi(t) + µi + f (hi(t)− θi) + ξi(t) , (1)\nwhere τ is some positive time constant, µi a baseline activity level, f(·) some (arbitrary) activation function,\nhi(t) a time dependent input field, θi an input threshold and ξi(t) white noise with a certain standard deviation\nσξ. The input field hi(t) is determined by the activity of other neurons according to hi(t) =\n∑\nj wijzj(t),\nwith wij denoting the strength of the connection (weight) from neuron j to neuron i. Here we will exclusively\nconsider activation functions of the form f(x) = x (linear case), and f(x) = Θ(x)x (threshold-linear case,\n5\nObservationReward\nPlace cells\nActor\nM\nM\nCritic\nPrediction error\nd\nAction\nM\nMUSIC port\nthreshold-linear unit\nlinear unit\nexcitatory connection\ninhibitory connection\nplastic connection\nMUSIC communication\nM\nneuromodulation\nFigure 2: Actor-critic architecture for RL with rate neurons. Observations are communicated via a\nMUSIC input port to a population of place cells. These project on the one hand to a critic unit and on the\nother hand to actor units arranged in a winner-take-all circuit. The critic and an additional MUSIC input port\nproject to a unit representing the reward-prediction error that modulates the plasticity between place cells and\ncritic and actors, respectively. The actor units project to a MUSIC output port encoding the selected action.\n“relu”). These models have recently been added to the NEST simulator. Their dynamics are solved on a\nfixed time-grid by a stochastic-exponential-Euler method with a step size determined by the resolution of the\nsimulation. For more details on the model implementation see [53].\nThe input layer is a population of threshold-linear rate neurons which receive inputs through MUSIC\nand encode observations from the environment (see figure 2). Via plastic connections these place cells\nproject to a single neuron representing the value that the network assigns to the current state (the “critic”).\nAn additional neuron calculates the reward-prediction error by combining the reward received from the\nenvironment with input from the critic. Plasticity of the projections from inputs to the critic is modulated by\nthis reward-prediction error (see below).\nIn addition, neurons in the input layer project to a population of neurons representing the available actions\n(the “actor”). To enforce selection of a specific action, the actor units are arranged in a winner-take-all (WTA)\ncircuit. This is implemented by recurrent connections between actor units that correspond to short-range\nexcitation and long-range inhibition, depending on the similarity of the action that actor units encode. The\nactivity of actor units is transformed to a valid action and communicated to the environment via the RMT.\nTo derive a learning rule for the critic, we followed similar steps as in [37] applied to rate models (equation\n(1)). The critic activity should approximate a continous-time value function defined by [54]\nV pi(s(t)) :=\n∫ ∞\nt′\nr(spi(t′))e−\nt′−t\nτr dt′ . (2)\nHere s(t) denotes the state of the agent at time t, r(spi(t)) denotes the reward obtained in state s(t), τr a\ndiscounting factor for future rewards and pi the agent’s policy. To achieve this, we define the following\nobjective function which should be minimized by gradient descent on the weights from inputs to the critic:\nE(t) =\n1\n2\n(V pi(t)− z(t))2 , (3)\n6\nwhere z(t) represents the activity of the critic unit. By performing gradient descent on equation (3), using\na self-consistency equation for V pi(t) from the derivative of equation (2) and bootstrapping on the current\nprediction for the value (see supplementary material and [54, 37]), we obtain the following local Hebbian\nthree-factor learning rule that approximately minimizes the objective function (equation (3)):\n∆wj = ηδ(t)xj(t)Θ (z(t)− θpost) , (4)\nwhere η is a learning rate, xj(t) represents the activity of the jth place cell, Θ(·) the Heaviside function and\nθpost a parameter that accounts for noise on the postsynaptic unit (see supplementary material for details). The\nterm δ(t) = v˙(t) + r(t)− 1τr v(t) corresponds to the activity of the reward-prediction error unit, acting as a\nneuromodulatory signal for the Hebbian plasticity between the presynaptic (xj) and postsynaptic (z) units.\nTo avoid explicit calculation of the derivative, we rewrite δ(t) as:\nδ(t) ≈\n(\n1\nd\n− 1\nτr\n)\nv(t)− 1\nd\nv(t− d) + r(t) . (5)\nTo approximate the derivative we hence implement two connections from the critic to the reward-prediction\nerror unit: one instantaneous, and one with delay d > 0.\nTo learn an optimal policy, we exploit that the actor units follow the same dynamics as the critic. Similar\nto [37], we hence apply the same learning rule to the connections between the inputs and the actor units. In\norder to assure that at least one actor unit is active, thus preventing a deadlock, we introduce a minimal weight\nfor each connection between input and output units and add input noise to the actor units.\n3.3.2 Mountain Car\nAs an example of an environment with continous states, we consider the MountainCar environment. The task\nis to steer a toy vehicle that starts at a valley between two hills to the top of the right one (figure 3A, inset).\nTo make the task more challenging, the car’s engine is not strong enough to reach the top in one go, so the\nagent needs to learn to gain momentum by swinging back and forth between the two hills. A single episode\nin this environment starts when the agent is placed in the valley and ends when it reaches the final position on\nthe top of the right hill. The state of the agent is described by two continous variables: the x-position x(t)\nand the x-velocity x˙(t). The agent can choose from three different discrete actions that affect the velocity of\nthe vehicle (accelerate left, no acceleration, accelerate right). It receives punishment from the environment\nin every step; the goal is to minimize the total punishment collected over the whole episode. Since this is\ndifficult to implement in an actor-critic architecture [52], we provide additional reward when the agent reaches\nthe final position.\nTo translate the agent’s current state into neuronal activity, we distribute 25 place cells evenly across the\ntwo dimensional plane of possible positions and velocities using the discretize adapter of the RMT. The actor\nis implemented by a WTA circuit of three units as described in section 3.3.1. The activity of these units is\ntransformed into an action via the argmax adapter (section 3.1).\nInitially, the agent explores the environment by selecting random actions. Due to the WTA circuit\ndynamics, a single action stays active over an extended period of time. The constant punishment gradually\ndecreases the weights from the place cells to the corresponding actor unit, eventually leading to another actor\nunit becoming active (figure 3B, left). After a while, the agent reaches the goal by performing actions that\nhave not been significantly punished. For this task the stable nature of the WTA is advantageous, causing\nthe agent to perform the same action repeatedly allowing efficient exploration of the state space. After the\nagent has found the goal once, the number of steps spent on exploring actions in the following episodes is\nmuch smaller. From the sixth episode on, the performance of the agent is already close to optimal (figure 3A).\n7\nA B\nFigure 3: Network performance on an environment with continuous states and discrete actions. A:\nReward obtained by the agent per episode averaged over 10 simulations with different seeds (solid orange\nline). Orange band indicates ± one standard deviation. Light gray line marks average reward per episode\nfor which the environment is considered solved. Inset: screenshot of the environment with agent (stylized\nvehicle), environment with valley and two hills (black solid line) and goal position (yellow flag). The agent\nis close to the starting position at the trough. B: Activity traces of place cells (bottom), actor units (second\nfrom bottom), critic unit (second from top) and reward-prediction-error unit (top). Shown are neural activities\nduring 6.5 s early (left) and late (right) in the simulation.\nAfter learning for about 10 episodes, the agent’s performance has converged. The value of the final state is\nsuccessfully propagated backwards over different states, leading to a ramping of activity of the critic unit\nfrom the start of an episode to the end (figure 3B, right).\nSince the OpenAI Gym offers a variety of environments, we trained the same network model on an\nadditional task with different requirements.\n3.3.3 Frozen Lake\nAs a second application, we chose the FrozenLake environment consisting of a discrete set of 16 states\narranged in a four-by-four grid (figure 4A, inset). Each state is either a start state (S), a goal state (G), a hole\n(H) or a frozen state (F). From the start position, the agent has to reach the rewarded state by navigating\nover the frozen states without falling into holes which would reset the agent to the starting position. In each\nstep, the agent can choose from four different actions: move west, move north, move east and move south.\nUsually, the tiles are “slippery”, i.e., there is a chance that a random action is executed irrespective of the\naction chosen by the agent. However, to simplify learning for demonstration purposes, we turned this feature\noff. Upon reaching the goal, the agent receives a reward of magnitude one. Since the optimal path involves\nsix steps from start to goal, the theoretical optimal reward per step is ∼ 0.16. To encourage exploration, the\nagent receives a small punishment in each state, and additionally, to speed up learning, the agent is punished\nfor falling into holes.\nUnlike in the continuous MountainCar environment, the tuning curves of place cells do not overlap in the\ndiscrete case, leading to sharp transitions in the network activity. This leads to severe issues for associating\nvalues and actions with the respective states. To address this problem we introduced a simple eligibility trace\nby evaluating the activity of the pre- and post synaptic units in the learning rule with a small delay δt (see\nsupplementary material). With this addition, the network model is able to find the optimal solution for this\n8\nA B\nFigure 4: Network performance on a grid-world environment. A: Average reward collected by the agent\nover the next 500 steps (orange solid line) averaged over 5 simulations. Orange band indicates ± one standard\ndeviation. Gray line: theoretical optimum. Inset: screenshot of the environment with start state (S), frozen\nstates (F), holes (H) and goal state (G). The position of the agent is indicated in pink. B: The learned policy\nand value map of the environment. Red colors indicate positive, blue colors negative values. Arrows indicate\nthe preferred direction of movement.\ntask within roughly 2000 steps (figure 4A). It also learns to associate holes with punishment and frozen states\nwith reward if they are on the path to the goal (figure 4B). Although there are two possible paths to the goal,\nthe agent prefers the path with less corners, since it is easier to navigate using a WTA circuit.\n4 Conclusion\nIn this manuscript, we have presented a toolchain that closes the loop between the OpenAI Gym and neural\nnetwork simulators. We demonstrated the functionality of the toolchain by implementing an actor-critic\narchitecture in NEST and evaluating its performance on two different environments. The performance of the\nnetwork quickly reached near-optimal performance on these two tasks.\nCombining neural network simulators with RL toolkits responds to the growing need of researchers to\nprovide neural network models with rich, dynamic input. Compared to creating customized environments to\nthis end, using readily available tools is easier, often computationally more efficient, and most importantly,\nsupports reproducible science. In addition, having the OpenAI Gym environments as common benchmarks in\nboth fields encourages comparison between traditional machine learning and biologically plausible implemen-\ntations. In contrast to models presented in previous studies, our toolchain makes it easy for other researchers\nto extend our implementation of an actor-critic architecture to other environments, replace neurons models or\nexplore alternative learning rules.\nWhile the toolchain currently only supports the OpenAI Gym, the extension to other toolkits is simple\ndue to a modular design of the wrapper. The RMT can be found on GitHub and is available under the GPLv3.\nThe OpenAI Gym ZeroMQ wrapper is also available via GitHub under the MIT license. A complementary\ndevelopment to the work presented here is provided by SPORE, a framework for reward-based learning with\nspiking neurons in the NEST simulator.2 It provides support for synapse models with time-driven updates,\n2https://github.com/IGITUGraz/spore-nest-module\n9\nadditional support for recording and evaluating traces of neuronal state variables and introduces MUSIC ports\nfor communicating rewards to a running simulation.\nWith the work presented here we enable researchers to build more easily upon previous studies and\nevaluate novel models. We hope this boosts the progress in computational neuroscience in uncovering the\nbiophysical mechanisms involved in learning complex tasks from delayed rewards.\nAcknowledgments\nWe acknowledge partial support by the German Federal Ministry of Education through our German-Japanese\nComputational Neuroscience Project (BMBF Grant 01GQ1343), EuroSPIN, the Helmholtz Alliance through\nthe Initiative and Networking Fund of the Helmholtz Association and the Helmholtz Portfolio theme “Super-\ncomputing and Modeling for the Human Brain” and the European Union Seventh Framework Programme\n(FP7/2007-2013) under grant agreement no. 604102 (HBP). All network simulations carried out with NEST\n(http://www.nest-simulator.org).\nReferences\n[1] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural\nnetworks. In Advances in neural information processing systems, pp. 1097–1105.\n[2] Hinton, G., Deng, L., Yu, D., Dahl, G. E., Mohamed, A.-r., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath,\nT. N., et al. (2012a). Deep neural networks for acoustic modeling in speech recognition: The shared views of four\nresearch groups. IEEE Signal Processing Magazine 29(6), 82–97.\n[3] Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., & Kavukcuoglu, K. (2016).\nAsynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pp.\n1928–1937.\n[4] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. (2015). Human-level control through deep reinforcement learning. Nature 518(7540),\n529–533.\n[5] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., et al. (2016). Mastering the game of go with deep neural networks and tree search.\nNature 529(7587), 484–489.\n[6] Raina, R., Madhavan, A., & Ng, A. Y. (2009). Large-scale deep unsupervised learning using graphics processors. In\nProceedings of the 26th annual international conference on machine learning, pp. 873–880. ACM.\n[7] Hinton, G. E., & Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural networks. Sci-\nence 313(5786), 504–507.\n[8] Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H., et al. (2007). Greedy layer-wise training of deep networks.\nAdvances in neural information processing systems 19, 153.\n[9] Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. R. (2012b). Improving neural\nnetworks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.\n[10] Collobert, R., Bengio, S., & Mariéthoz, J. (2002). Torch: a modular machine learning software library. Technical\nreport, Idiap.\n10\n[11] James, B., Olivier, B., Frédéric, B., Pascal, L., & Razvan, P. (2010). Theano: a cpu and gpu math expression\ncompiler. In Proceedings of the Python for Scientific Computing Conference (SciPy).\n[12] Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., & Darrell, T. (2014). Caffe:\nConvolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093.\n[13] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin,\nM., et al. (2016). Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint\narXiv:1603.04467.\n[14] LeCun, Y., Cortes, C., & Burges, C. J. (1998). The mnist database of handwritten digits.\n[15] Krizhevsky, A., & Hinton, G. (2009). Learning multiple layers of features from tiny images.\n[16] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). ImageNet: A Large-Scale Hierarchical Image\nDatabase. In CVPR09.\n[17] Todorov, E., Erez, T., & Tassa, Y. (2012). Mujoco: A physics engine for model-based control. In Intelligent Robots\nand Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026–5033. IEEE.\n[18] Bellemare, M., Naddaf, Y., Veness, J., & Bowling, M. (2015). The arcade learning environment: An evaluation\nplatform for general agents. In Twenty-Fourth International Joint Conference on Artificial Intelligence.\n[19] Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba, W. (2016). OpenAI\nGym. ArXiv e-prints.\n[20] Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein,\nM., Berg, A. C., & Fei-Fei, L. (2015). ImageNet Large Scale Visual Recognition Challenge. International Journal of\nComputer Vision (IJCV) 115(3), 211–252.\n[21] Wils, S., & De Schutter, E. (2009). STEPS: modeling and simulating complex reaction-diffusion systems with\nPython. Front. Neuroinformatics 3(15).\n[22] Carnevale, T., & Hines, M. (2006). The NEURON Book. Cambridge: Cambridge University Press.\n[23] Bower, J. M., & Beeman, D. (2007). GENESIS (simulation environment). Scholarpedia 2(3), 1383.\n[24] Gewaltig, M.-O., & Diesmann, M. (2007). NEST (NEural Simulation Tool). Scholarpedia 2(4), 1430.\n[25] Goodman, D. F. M., & Brette, R. (2009). The Brian simulator. Front. Neurosci. 3(2), 192–197. doi:\n10.3389/neuro.01.026.2009.\n[26] Bekolay, T., Bergstra, J., Hunsberger, E., DeWolf, T., Stewart, T. C., Rasmussen, D., Choo, X., Voelker, A. R.,\n& Eliasmith, C. (2013). Nengo: a python tool for building large-scale functional brain models. Frontiers in\nneuroinformatics 7.\n[27] Sanz Leon, P., Knock, S., Woodman, M., Domide, L., Mersmann, J., McIntosh, A., & Jirsa, V. (2013). The virtual\nbrain: a simulator of primate brain network dynamics. Front. Neuroinform. 7, 10.\n[28] Gerstner, W., & Naud, R. (2009). How good are neuron models? Science 326(5951), 379–380.\n[29] Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. Adaptive Computation and Machine\nLearning. The MIT Press.\n[30] Watkins, C. J. C. H. (1989). Learning from delayed rewards. Ph. D. thesis, University of Cambridge England.\n11\n[31] Vasilaki, E., Frémaux, N., Urbanczik, R., Senn, W., & Gerstner, W. (2009). Spike-based reinforcement learning\nin continuous state and action space: When policy gradient methods fail. PLoS Comput. Biol. 5(12), e1000586.\ndoi:10.1371/journal.pcbi.1000586.\n[32] Izhikevich, E. M. (2007). Solving the distal reward problem through linkage of STDP and dopamine signaling.\nCereb. Cortex 17(10), 2443–2452.\n[33] Urbanczik, R., & Senn, W. (2009). Reinforcement learning in populations of spiking neurons. Nature neuro-\nscience 12(3), 250.\n[34] Potjans, W., Morrison, A., & Diesmann, M. (2009). A spiking neural network model of an actor-critic learning\nagent. Neural Comput. 21, 301–339.\n[35] Frémaux, N., Sprekeler, H., & Gerstner, W. (2010). Functional requirements for reward-modulated spike-timing-\ndependent plasticity. J. Neurosci. 30(40), 13326–13337.\n[36] Jitsev, J., Morrison, A., & Tittgemeyer, M. (2012). Learning from positive and negative rewards in a spiking neural\nnetwork model of basal ganglia. In Neural Networks (IJCNN), The 2012 International Joint Conference on, pp. 1–8.\nIEEE.\n[37] Frémaux, N., Sprekeler, H., & Gerstner, W. (2013). Reinforcement learning using a continuous time actor-critic\nframework with spiking neurons. PLoS Comput Biol 9(4), e1003024.\n[38] Rasmussen, D., & Eliasmith, C. (2014). A neural model of hierarchical reinforcement learning. In CogSci.\n[39] Friedrich, J., Urbanczik, R., & Senn, W. (2014). Code-specific learning rules improve action selection by populations\nof spiking neurons. International Journal of Neural Systems 24(05), 1450002.\n[40] Rombouts, J. O., Bohte, S. M., & Roelfsema, P. R. (2015). How attention can create synaptic tags for the learning\nof working memories in sequential tasks. PLoS Comput Biol 11(3), e1004060.\n[41] Baladron, J., & Hamker, F. H. (2015). A spiking neural network based on the basal ganglia functional anatomy.\nNeural Networks 67, 1–13.\n[42] Aswolinskiy, W., & Pipa, G. (2015). Rm-sorn: a reward-modulated self-organizing recurrent neural network.\nFrontiers in computational neuroscience 9, 36.\n[43] Friedrich, J., & Lengyel, M. (2016). Goal-directed decision making with spiking neurons. Journal of Neuro-\nscience 36(5), 1529–1546.\n[44] Rueckert, E., Kappel, D., Tanneberg, D., Pecevski, D., & Peters, J. (2016). Recurrent spiking networks solve\nplanning tasks. Scientific reports 6.\n[45] Human Brain Project (2014). Project website. Available at: http://www.humanbrainproject.eu.\n[46] Kunkel, S., Schmidt, M., Eppler, J. M., Masumoto, G., Igarashi, J., Ishii, S., Fukai, T., Morrison, A., Diesmann, M.,\n& Helias, M. (2014). Spiking network simulation code for petascale computers. Front. Neuroinformatics 8, 78.\n[47] Djurfeldt, M., Hjorth, J., Eppler, J. M., Dudani, N., Helias, M., Potjans, T. C., Bhalla, U. S., Diesmann, M., Kotaleski,\nJ. H., & Ekeberg, O. (2010). Run-time interoperability between neuronal simulators based on the music framework.\nNeuroinformatics 8. doi:10.1007/s12021-010-9064-z.\n[48] Weidel, P., Djurfeldt, M., Duarte, R. C., & Morrison, A. (2016). Closed loop interactions between spiking neural\nnetwork and robotic simulators based on music and ros. Frontiers in neuroinformatics 10.\n12\n[49] Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., Wheeler, R., & Ng, A. Y. (2009). Ros: an\nopen-source robot operating system. In ICRA workshop on open source software, Volume 3, pp. 5. Kobe.\n[50] Hintjens, P. (2013). ZeroMQ: Messaging for Many Applications. \"O’Reilly Media, Inc.\".\n[51] Moser, E. I., Kropff, E., & Moser, M.-B. (2008). Place cells, grid cells, and the brain’s spatial representation system.\nAnnu. Rev. Neurosci. 31, 69–89.\n[52] Potjans, W., Diesmann, M., & Morrison, A. (2011). An imperfect dopaminergic error signal can drive temporal-\ndifference learning. PLoS Comput. Biol. 7(5), e1001133.\n[53] Hahne, J., Dahmen, D., Schuecker, J., Frommer, A., Bolten, M., Helias, M., & Diesmann, M. (2016). Integration of\ncontinuous-time dynamics in a spiking neural network simulator. arXiv. 1610.09990 [q–bio.NC].\n[54] Doya, K. (2000). Reinforcement learning in continuous time and space. Neural Comput. 12(1), 219–245.\n[55] Nordlie, E., Gewaltig, M.-O., & Plesser, H. E. (2009). Towards reproducible descriptions of neuronal network\nmodels. PLoS Comput. Biol. 5(8), e1000456.\nSupplementary material\nA Derivation of the learning rule\nWe consider continuous time, continuous states and continuous actions and follow similar steps as [54, 37]. Starting from\nthe continuous-time value function\nV pi(s(t)) :=\n∫ ∞\nt\nr(spi(t′))e−\nt′−t\nτr dt′ , (6)\nwe take the derivative with respect to t to arrive at a self-consistency equation for V :\nV˙ pi(t) :=\ndV pi(t)\ndt\n= −r(t) + 1\nτr\nV pi(t) . (7)\nTo implement temporal-difference learning in a neural network architecture, we would like to approximately represent\nthe true value V pi(t) of the state at time t by the rate zi(t) of a critic neuron. This activity will depend on the activity of\ninput units, i.e., place cells, and the weights between inputs and critic. With initially random weights the self-consistency\ncriterion will not be fulfilled, and will have a finite error δ(t):\nδ(t) = z˙i(t) + r(t)− 1\nτr\nzi(t) . (8)\nWe now define an objective function that should be minimized by gradient descent on the weights:\nE(t) =\n1\n2\n(V pi(t)− zi(t))2 . (9)\nWe take derivative with respect to wij and use the self-consistency equation (7):\n∂E(t)\n∂wij\n= −(V pi(t)− zi(t))∂zi(t)\n∂wij\n= −(τrV˙ pi(t) + τrr(t)− zi(t))∂zi(t)\n∂wij\n≈ − (τr z˙(t) + τrr(t)− zi(t))︸ ︷︷ ︸\nτrδ(t)\n∂zi(t)\n∂wij\n(10)\n13\nHere we have replaced V˙ pi(t) with z˙i(t) in the last line. For a discussion of the validity of this approximation and the\nconvergence of the learning rule, see [37]. To perform gradient descent on the objective function, we hence need to\nchange the weights according to\n∆wij :=− η′ ∂E(t)\n∂wij\n=ηδ(t)\n∂zi(t)\n∂wij\n, (11)\nwhere we introduced η = η′τr . We hence need to determine the derivative of the critic activity with respect to the weights\nbetween inputs and critic ∂zi(t)\n∂wij\n.\nWe start from the differential equation describing the dynamics of a threshold-linear rate neuron, and assume the\nnoise to be small, i.e., we drop the term ξi(t). Without loss of generality, we assume µi = 0, θi = 0. The dynamics are\nthen given by the solution to\nτ\ndzi(t)\ndt\n= −zi(t) + gΘ\n(∑\nj\nwijxj(t)\n)(∑\nj\nwijxj(t)\n)\n. (12)\nVariation of constants yields the general solution as a convolution equation:\nzi(t) = g\n((\nΘ (. . . )\n(∑\nj\nwijxj(·)\n))\n∗ κ(·)\n)\n(t) , (13)\nwhere we have introduced the filter kernel κ(t) = 1\nτ\ne−\nt\nτ Θ(t). We now take the derivative with respect to wij . While the\nderivative at\n∑\nj wijxj(t) = 0 is technically not defined, we follow the usual convention and set it to zero. This yields\n∂zi(t)\n∂wij\n=\n{\ng (xj ∗ κ) (t) if ∑j wijxj(t) > 0\n0 else\n(14)\nBy combining equation (11) with equation (14), we obtain the following learning rule:\n∆wij =\n{\nηδ(t)g(xj ∗ κ)(t) if ∑j wijxj(t) > 0\n0 else\n(15)\nBy choosing the time constant of critic and actors small, we effectively remove the filtering of the presynaptic activity\n(limτ→0 κ(t) = δ(t)) and hence ignore it. To simplify this equation further, we rewrite it as a condition on the rate of the\npostsynaptic neuron by observing that zi(t) > 0 iff\n∑\nj wijxj(t) > 0. To implement exploration for similar inputs to all\noutput units we add noise to the activity of the actor units. We only consider a postsynaptic neuron active, if its activity is\nlarger than some threshold θpost. This leads to the following form for the learning rule:\n∆wij = ηδ(t)gxj(t)Θ(zi(t)− θpost) , (16)\nwhere we Θ(·) denotes the Heaviside step function defined as:\nΘ(x) =\n{\n1 x > 0\n0 else\nTo implement a simple type of eligibility trace, we introduce an additional parameter δt that can delay the activity of\nthe pre- and post-synaptic units in the learning rule:\n∆wij = ηδ(t)gxj(t− δt)Θ(zi(t− δt)− θpost) , (17)\n14\nA Model summary\nPopulations Seven\nTopology None\nConnectivity Population specific\nNeuron model Linear & threshold-linear rate\nChannel models None\nSynapse model Instantaneous & delayed continuous coupling\nPlasticity Three-factor Hebbian\nExternal input Continuous MUSIC ports\nExternal output Continuous MUSIC ports\nMeasurements Rates of all neurons\nB Populations\nName Elements Size\nObservation MUSIC in port 1\nReward MUSIC in port 1\nAction MUSIC out port 1\nPlace cells Threshold-linear 16(25)\nCritic Threshold-linear 1\nActor Threshold-linear 4(3)\nPrediction error Linear 1\nC Connectivity\nSource Target Pattern\nObservation Place cells One-to-one (by MUSIC channel), instantaneous, static, weight wo\nReward Prediction error One-to-one (by MUSIC channel), instantaneous, static, weight wr\nActor Action One-to-one (by MUSIC channel), instantaneous, static, weight wa\nPlace cells Critic All-to-all, instantaneous, plastic, initial weight wpc\nPlace cells Actor All-to-all, instantaneous, plastic, initial weight wpa\nCritic Prediction error One-to-one, instantaneous, static, weight 1/d− 1/τr\nCritic Prediction error One-to-one, delay d, static, weight −1/d\nActor Actor All-to-all, instantaneous, static, weight α exp(−∆a/σ) + β\nD Neuron and synapse model\nType Linear rate neuron\nDynamics τ dz(t)dt = −z(t) + µ+ (h(t)− θ) + ξ(t)\nType Threshold-linear rate neuron\nDynamics τ dz(t)dt = −z(t) + µ+ Θ (h(t)− θ) (h(t)− θ) + ξ(t)\nType Three-factor Hebbian synapse\nPlasticity ∆wij = ηδ(t)gxj(t− δt)Θ(zi(t− δt)− θpost)\nE Input\nType Description\nObservation Rate r ∈ [−1, 1] according to tuning of place cell (using discretize adapter)\nReward Rate r ∈ [−1, 1] according to reward provided by the environment\nF Output\nType Description\nAction Rates ri ∈ [0,∞) according to activities of the actor units\nTable 1: Description of the network model (according to [55]).\n15\nB Populations: place cells\nName Values\nτ 5.0 (1.0)\ng 1.0\nµ 0.0\nσξ 0.0\nθ -0.5\nB Populations: critic\nName Values\nτ 0.1\ng 1.0\nµ -1.0\nσξ 0.0\nθ -1.0\nB Populations: reward\nName Values\nτ 1.0\ng 1.0\nµ 0.0\nσξ 0.0\nθ 0.001 (-0.0999)\nB Populations: actor\nName Values\nτ 0.1\ng 1.0\nµ 0.0\nσξ 0.2 (0.05)\nθ 0.0\nC Connectivity\nName Values\nwo 0.5\nwr 0.1\nwa 1.0\nwpc 0.0\nwminpc −1.0\nwmaxpc 1.0\nθpcpost −1.0\nwpa 0.9(0.3)\nwminpa 0.1(0.05)\nwmaxpa 1.0\nθpapost 0.5(0.1)\nd 1.0\nτr 20000.0\nα 1.2\nβ −0.55\nσξ 0.1\nηcritic 0.01(0.125)\nηactor 0.2(0.250)\nδt 19.0(0.0)\nE Input: discretize adapter\nName Values\nσx 0.01 (0.2)\nσy – (0.2)\nTable 2: Table of the network parameters used for both tasks (according to [55]). Values in brackets are used\nfor the MountainCar environment.\n16\nB Network description\nThe tables 1 and 2 summarize the network architecture and parameters.\nC JSON Message types\nListing 1 show the standard message types used for communication between the OpenAI Gym and the RMT. All messages\nare serialized using JSON and communicated via ZeroMQ.\nListing 1: Message types used for communication.\nBasicMsg\n{\nf l o a t min\nf l o a t max\nf l o a t v a l u e\nf l o a t t imes t amp\n}\nObserva t ionMsg\n{\nBasicMsg [ ] o b s e r v a t i o n s # one b a s i c msg p e r d imens ion\n}\nRewardMsg\n{\nBasicMsg [ ] reward # reward i s a lways one d i m e n s i o n a l\n}\nActionMsg\n{\nBasicMsg [ ] a c t i o n s # one d i m e n s i o n a l f o r d i s c r e t e a c t i o n s\n# o r one d imens ion p e r p o s s i b l e a c t i o n\n}\nD Example wrapper configuration file\nListing 2 shows an example configuration file for running the mountain car environment.\nListing 2: Example configuration file for the wrapper to run the “MountainCar-v0” environment.\n\" A l l \" :\n{\n\" seed \" : 12345 ,\n\" t i m e _ s t a m p _ t o l e r a n c e \" : 0 . 0 1 ,\n\" p r e f i x \" : nul l ,\n\" w r i t e _ r e p o r t \" : true ,\n\" r e p o r t _ f i l e \" : \" . / r e p o r t . j s o n \" ,\n\" o v e r w r i t e _ f i l e s \" : f a l s e ,\n\" f l u s h _ r e p o r t _ i n t e r v a l \" : n u l l\n17\n} ,\n\" Env \" :\n{\n\" env \" : \" MountainCar−v0 \" ,\n\" i n i t i a l _ r e w a r d \" : nul l ,\n\" f i n a l _ r e w a r d \" : nul l ,\n\" min_reward \" : −1.0 ,\n\" max_reward \" : 1 . 0 ,\n\" r e n d e r \" : true ,\n\" m o n i t o r \" : f a l s e ,\n\" m o n i t o r _ d i r \" : \" . / expe r imen t −0/ \" ,\n\" m o n i t o r _ a r g s \" :\n{\n\" w r i t e _ u p o n _ r e s e t \" : true ,\n\" v i d e o _ c a l l a b l e \" : f a l s e\n}\n} ,\n\" EnvRunner \" :\n{\n\" u p d a t e _ i n t e r v a l \" : 0 . 0 1 ,\n\" i n t e r _ t r i a l _ d u r a t i o n \" : 0 . 4\n} ,\n\" CommandReceiver \" :\n{\n\" s o c k e t \" : 5555 ,\n\" t i m e _ s t a m p _ t o l e r a n c e \" : 0 . 0 1\n} ,\n\" O b s e r v a t i o n S e n d e r \" :\n{\n\" s o c k e t \" : 5556 ,\n\" u p d a t e _ i n t e r v a l \" : 0 . 0 1\n} ,\n\" RewardSender \" :\n{\n\" s o c k e t \" : 5557 ,\n\" u p d a t e _ i n t e r v a l \" : 0 . 0 1\n}\nE Example MUSIC configuration file\nListing 3 shows an example MUSIC configuration file to run the MountainCar environment. It shows the different\nprocesses with parameters which are spawned by MUSIC including RMT adapters and NEST.\nListing 3: Example MUSIC configuration file to run the MountainCar environment.\ns t o p t i m e =150 .\nr t f =1 .\n[ r eward ]\nb i n a r y = z m q _ i n _ a d a p t e r\na r g s =\n18\nnp=1\nm u s i c _ t i m e s t e p =0.001\nmessage_ type =GymObservat ion\nzmq_top ic =\nzmq_addr= t c p : / / l o c a l h o s t :5557\n[ s e n s o r ]\nb i n a r y = z m q _ i n _ a d a p t e r\na r g s =\nnp=1\nm u s i c _ t i m e s t e p =0.001\nmessage_ type =GymObservat ion\nzmq_top ic =\nzmq_addr= t c p : / / l o c a l h o s t :5556\n[ d i s c r e t i z e ]\nb i n a r y = d i s c r e t i z e _ a d a p t e r\na r g s =\nnp=1\nm u s i c _ t i m e s t e p =0.001\ng r i d _ p o s i t i o n s _ f i l e n a m e = g r i d _ p o s . j s o n\n[ n e s t ]\nb i n a r y = . . / a c t o r _ c r i t i c _ n e t w o r k / ne twork . py\na r g s=− t 1 5 0 . −n 25 −m 3 −p ne twork_params . j s o n\nnp=1\n[ argmax ]\nb i n a r y = a r g m a x _ a d a p t e r\na r g s =\nnp=1\nm u s i c _ t i m e s t e p =0.001\n[ command ]\nb i n a r y = z m q _ o u t _ a d a p t e r\na r g s =\nnp=1\nm u s i c _ t i m e s t e p =0 .01\nmessage_ type =GymCommand\nzmq_top ic =\nzmq_addr= t c p : / / ∗ :5555\ns e n s o r . out−> d i s c r e t i z e . i n [ 2 ]\nd i s c r e t i z e . out−>n e s t . i n [ 2 5 ]\nreward . out−>n e s t . r e w a r d _ i n [ 1 ]\nn e s t . out−>argmax . i n [ 3 ]\nargmax . out−>command . i n [ 1 ]\nF Environments\nTable F shows parameters for the OpenAI Gym environments and the ZeroMQ wrapper.\n19\nOpenAI Gym\nName Values\nVersion 0.8.1\nMountainCar\nName Values\nVersion 0\nMax episode steps None\nInitial reward* -1.0\nFinal reward* -0.4\nInter-trial duration* 0.4\nUpdate interval (env runner)* 0.02\nFrozenLake\nName Values\nVersion 0\nMax episode steps None\nSlippery False\nFinal reward null* -0.1\nInter-trial duration* 0.1\nUpdate interval (env runner)* 0.1\nTable 3: Table of the environment parameters. Values marked with * indicate values for the ZeroMQ wrapper.\n20\n",
    "Link": "http://arxiv.org/abs/1709.05650"
}