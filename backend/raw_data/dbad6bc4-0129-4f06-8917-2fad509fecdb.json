{
    "Title": "Neural Natural Language Inference Models Enhanced with External\n  Knowledge",
    "Authors": "Chen, Qian, Inkpen, Diana, Ling, Zhen-Hua, Wei, Si, Zhu, Xiaodan",
    "Year": "No year available",
    "Abstract": "Modeling natural language inference is a very challenging task. With the\navailability of large annotated data, it has recently become feasible to train\ncomplex models such as neural-network-based inference models, which have shown\nto achieve the state-of-the-art performance. Although there exist relatively\nlarge annotated data, can machines learn all knowledge needed to perform\nnatural language inference (NLI) from these data? If not, how can\nneural-network-based NLI models benefit from external knowledge and how to\nbuild NLI models to leverage it? In this paper, we enrich the state-of-the-art\nneural natural language inference models with external knowledge. We\ndemonstrate that the proposed models improve neural NLI models to achieve the\nstate-of-the-art performance on the SNLI and MultiNLI datasets.Comment: Accepted by ACL 201",
    "Keywords": "No keywords available",
    "Publisher": "'Association for Computational Linguistics (ACL)'",
    "Publication Date": "No publication date available",
    "Journal": "No journal available",
    "Citation Count": 0,
    "Full Text": "Neural Natural Language Inference Models Enhanced with\nExternal Knowledge\nQian Chen\nUniversity of Science and\nTechnology of China\ncq1231@mail.ustc.edu.cn\nXiaodan Zhu\nECE, Queen’s University\nxiaodan.zhu@queensu.ca\nZhen-Hua Ling\nUniversity of Science and\nTechnology of China\nzhling@ustc.edu.cn\nDiana Inkpen\nUniversity of Ottawa\ndiana@site.uottawa.ca\nSi Wei\niFLYTEK Research\nsiwei@iflytek.com\nAbstract\nModeling natural language inference is a\nvery challenging task. With the avail-\nability of large annotated data, it has re-\ncently become feasible to train complex\nmodels such as neural-network-based in-\nference models, which have shown to\nachieve the state-of-the-art performance.\nAlthough there exist relatively large anno-\ntated data, can machines learn all knowl-\nedge needed to perform natural language\ninference (NLI) from these data? If not,\nhow can neural-network-based NLI mod-\nels benefit from external knowledge and\nhow to build NLI models to leverage it?\nIn this paper, we enrich the state-of-the-art\nneural natural language inference models\nwith external knowledge. We demonstrate\nthat the proposed models improve neural\nNLI models to achieve the state-of-the-art\nperformance on the SNLI and MultiNLI\ndatasets.\n1 Introduction\nReasoning and inference are central to both hu-\nman and artificial intelligence. Natural language\ninference (NLI), also known as recognizing tex-\ntual entailment (RTE), is an important NLP prob-\nlem concerned with determining inferential rela-\ntionship (e.g., entailment, contradiction, or neu-\ntral) between a premise p and a hypothesis h. In\ngeneral, modeling informal inference in language\nis a very challenging and basic problem towards\nachieving true natural language understanding.\nIn the last several years, larger anno-\ntated datasets were made available, e.g., the\nSNLI (Bowman et al., 2015) and MultiNLI\ndatasets (Williams et al., 2017), which made\nit feasible to train rather complicated neural-\nnetwork-based models that fit a large set of\nparameters to better model NLI. Such models\nhave shown to achieve the state-of-the-art per-\nformance (Bowman et al., 2015, 2016; Yu and\nMunkhdalai, 2017b; Parikh et al., 2016; Sha et al.,\n2016; Chen et al., 2017a,b; Tay et al., 2018).\nWhile neural networks have been shown to be\nvery effective in modeling NLI with large train-\ning data, they have often focused on end-to-end\ntraining by assuming that all inference knowledge\nis learnable from the provided training data. In\nthis paper, we relax this assumption and explore\nwhether external knowledge can further help NLI.\nConsider an example:\n• p: A lady standing in a wheat field.\n• h: A person standing in a corn field.\nIn this simplified example, when computers are\nasked to predict the relation between these two\nsentences and if training data do not provide the\nknowledge of relationship between “wheat” and\n“corn” (e.g., if one of the two words does not ap-\npear in the training data or they are not paired in\nany premise-hypothesis pairs), it will be hard for\ncomputers to correctly recognize that the premise\ncontradicts the hypothesis.\nIn general, although in many tasks learning tab-\nula rasa achieved state-of-the-art performance, we\nbelieve complicated NLP problems such as NLI\nar\nX\niv\n:1\n71\n1.\n04\n28\n9v\n3 \n [c\ns.C\nL]\n  2\n3 J\nun\n 20\n18\ncould benefit from leveraging knowledge accumu-\nlated by humans, particularly in a foreseeable fu-\nture when machines are unable to learn it by them-\nselves.\nIn this paper we enrich neural-network-based\nNLI models with external knowledge in co-\nattention, local inference collection, and inference\ncomposition components. We show the proposed\nmodel improves the state-of-the-art NLI models\nto achieve better performances on the SNLI and\nMultiNLI datasets. The advantage of using exter-\nnal knowledge is more significant when the size of\ntraining data is restricted, suggesting that if more\nknowledge can be obtained, it may bring more\nbenefit. In addition to attaining the state-of-the-\nart performance, we are also interested in under-\nstanding how external knowledge contributes to\nthe major components of typical neural-network-\nbased NLI models.\n2 Related Work\nEarly research on natural language inference and\nrecognizing textual entailment has been performed\non relatively small datasets (refer to MacCartney\n(2009) for a good literature survey), which in-\ncludes a large bulk of contributions made under\nthe name of RTE, such as (Dagan et al., 2005;\nIftene and Balahur-Dobrescu, 2007), among many\nothers.\nMore recently the availability of much larger\nannotated data, e.g., SNLI (Bowman et al.,\n2015) and MultiNLI (Williams et al., 2017), has\nmade it possible to train more complex mod-\nels. These models mainly fall into two types\nof approaches: sentence-encoding-based models\nand models using also inter-sentence attention.\nSentence-encoding-based models use Siamese ar-\nchitecture (Bromley et al., 1993). The parameter-\ntied neural networks are applied to encode both\nthe premise and the hypothesis. Then a neural\nnetwork classifier is applied to decide relationship\nbetween the two sentences. Different neural net-\nworks have been utilized for sentence encoding,\nsuch as LSTM (Bowman et al., 2015), GRU (Ven-\ndrov et al., 2015), CNN (Mou et al., 2016), BiL-\nSTM and its variants (Liu et al., 2016c; Lin et al.,\n2017; Chen et al., 2017b; Nie and Bansal, 2017),\nself-attention network (Shen et al., 2017, 2018),\nand more complicated neural networks (Bowman\net al., 2016; Yu and Munkhdalai, 2017a,b; Choi\net al., 2017). Sentence-encoding-based models\ntransform sentences into fixed-length vector rep-\nresentations, which may help a wide range of\ntasks (Conneau et al., 2017).\nThe second set of models use inter-sentence at-\ntention (Rockta¨schel et al., 2015; Wang and Jiang,\n2016; Cheng et al., 2016; Parikh et al., 2016;\nChen et al., 2017a). Among them, Rockta¨schel\net al. (2015) were among the first to propose neu-\nral attention-based models for NLI. Chen et al.\n(2017a) proposed an enhanced sequential infer-\nence model (ESIM), which is one of the best mod-\nels so far and is used as one of our baselines in this\npaper.\nIn this paper we enrich neural-network-based\nNLI models with external knowledge. Unlike\nearly work on NLI (Jijkoun and de Rijke, 2005;\nMacCartney et al., 2008; MacCartney, 2009) that\nexplores external knowledge in conventional NLI\nmodels on relatively small NLI datasets, we aim to\nmerge the advantage of powerful modeling ability\nof neural networks with extra external inference\nknowledge. We show that the proposed model\nimproves the state-of-the-art neural NLI models\nto achieve better performances on the SNLI and\nMultiNLI datasets. The advantage of using exter-\nnal knowledge is more significant when the size of\ntraining data is restricted, suggesting that if more\nknowledge can be obtained, it may have more ben-\nefit. In addition to attaining the state-of-the-art\nperformance, we are also interested in understand-\ning how external knowledge affect major compo-\nnents of neural-network-based NLI models.\nIn general, external knowledge has shown to be\neffective in neural networks for other NLP tasks,\nincluding word embedding (Chen et al., 2015;\nFaruqui et al., 2015; Liu et al., 2015; Wieting\net al., 2015; Mrksic et al., 2017), machine trans-\nlation (Shi et al., 2016; Zhang et al., 2017b), lan-\nguage modeling (Ahn et al., 2016), and dialogue\nsystems (Chen et al., 2016b).\n3 Neural-Network-Based NLI Models\nwith External Knowledge\nIn this section we propose neural-network-based\nNLI models to incorporate external inference\nknowledge, which, as we will show later in Sec-\ntion 5, achieve the state-of-the-art performance.\nIn addition to attaining the leading performance\nwe are also interested in investigating the effects\nof external knowledge on major components of\nneural-network-based NLI modeling.\nFigure 1 shows a high-level general view of the\nproposed framework. While specific NLI systems\nvary in their implementation, typical state-of-the-\nart NLI models contain the main components (or\nequivalents) of representing premise and hypoth-\nesis sentences, collecting local (e.g., lexical) in-\nference information, and aggregating and compos-\ning local information to make the global decision\nat the sentence level. We incorporate and investi-\ngate external knowledge accordingly in these ma-\njor NLI components: computing co-attention, col-\nlecting local inference information, and compos-\ning inference to make final decision.\n3.1 External Knowledge\nAs discussed above, although there exist relatively\nlarge annotated data for NLI, can machines learn\nall inference knowledge needed to perform NLI\nfrom the data? If not, how can neural network-\nbased NLI models benefit from external knowl-\nedge and how to build NLI models to leverage it?\nWe study the incorporation of external,\ninference-related knowledge in major compo-\nnents of neural networks for natural language\ninference. For example, intuitively knowledge\nabout synonymy, antonymy, hypernymy and\nhyponymy between given words may help model\nsoft-alignment between premises and hypotheses;\nknowledge about hypernymy and hyponymy\nmay help capture entailment; knowledge about\nantonymy and co-hyponyms (words sharing the\nsame hypernym) may benefit the modeling of\ncontradiction.\nIn this section, we discuss the incorporation of\nbasic, lexical-level semantic knowledge into neu-\nral NLI components. Specifically, we consider ex-\nternal lexical-level inference knowledge between\nword wi and wj , which is represented as a vec-\ntor rij and is incorporated into three specific com-\nponents shown in Figure 1. We will discuss the\ndetails of how rij is constructed later in the exper-\niment setup section (Section 4) but instead focus\non the proposed model in this section. Note that\nwhile we study lexical-level inference knowledge\nin the paper, if inference knowledge about larger\npieces of text pairs (e.g., inference relations be-\ntween phrases) are available, the proposed model\ncan be easily extended to handle that. In this paper,\nwe instead let the NLI models to compose lexical-\nlevel knowledge to obtain inference relations be-\ntween larger pieces of texts.\n3.2 Encoding Premise and Hypothesis\nSame as much previous work (Chen et al.,\n2017a,b), we encode the premise and the hypoth-\nesis with bidirectional LSTMs (BiLSTMs). The\npremise is represented as a = (a1, . . . , am) and\nthe hypothesis is b = (b1, . . . , bn), where m\nand n are the lengths of the sentences. Then a\nand b are embedded into de-dimensional vectors\n[E(a1), . . . ,E(am)] and [E(b1), . . . ,E(bn)] using\nthe embedding matrix E ∈ Rde×|V |, where |V | is\nthe vocabulary size and E can be initialized with\nthe pre-trained word embedding. To represent\nwords in its context, the premise and the hypothe-\nsis are fed into BiLSTM encoders (Hochreiter and\nSchmidhuber, 1997) to obtain context-dependent\nhidden states as and bs:\nasi = Encoder(E(a), i) , (1)\nbsj = Encoder(E(b), j) . (2)\nwhere i and j indicate the i-th word in the premise\nand the j-th word in the hypothesis, respectively.\n3.3 Knowledge-Enriched Co-Attention\nAs discussed above, soft-alignment of word pairs\nbetween the premise and the hypothesis may ben-\nefit from knowledge-enriched co-attention mech-\nanism. Given the relation features rij ∈ Rdr be-\ntween the premise’s i-th word and the hypothesis’s\nj-th word derived from the external knowledge,\nthe co-attention is calculated as:\neij = (a\ns\ni )\nTbsj + F (rij) . (3)\nThe function F can be any non-linear or linear\nfunctions. In this paper, we use F (rij) = λ1(rij),\nwhere λ is a hyper-parameter tuned on the devel-\nopment set and 1 is the indication function as fol-\nlows:\n1(rij) =\n{\n1 if rij is not a zero vector ;\n0 if rij is a zero vector .\n(4)\nIntuitively, word pairs with semantic relationship,\ne.g., synonymy, antonymy, hypernymy, hyponymy\nand co-hyponyms, are probably aligned together.\nWe will discuss how we construct external knowl-\nedge later in Section 4. We have also tried a two-\nlayer MLP as a universal function approximator\nin function F to learn the underlying combination\nfunction but did not observe further improvement\nover the best performance we obtained on the de-\nvelopment datasets.\nFigure 1: A high-level view of neural-network-based NLI models enriched with external knowledge in\nco-attention, local inference collection, and inference composition.\nSoft-alignment is determined by the co-\nattention matrix e ∈ Rm×n computed in Equa-\ntion (3), which is used to obtain the local relevance\nbetween the premise and the hypothesis. For the\nhidden state of the i-th word in the premise, i.e.,\nasi (already encoding the word itself and its con-\ntext), the relevant semantics in the hypothesis is\nidentified into a context vector aci using eij , more\nspecifically with Equation (5).\nαij =\nexp(eij)∑n\nk=1 exp(eik)\n, aci =\nn∑\nj=1\nαijb\ns\nj , (5)\nβij =\nexp(eij)∑m\nk=1 exp(ekj)\n, bcj =\nm∑\ni=1\nβija\ns\ni , (6)\nwhere α ∈ Rm×n and β ∈ Rm×n are the nor-\nmalized attention weight matrices with respect to\nthe 2-axis and 1-axis. The same calculation is per-\nformed for each word in the hypothesis, i.e., bsj ,\nwith Equation (6) to obtain the context vector bcj .\n3.4 Local Inference Collection with External\nKnowledge\nBy way of comparing the inference-related seman-\ntic relation between asi (individual word repre-\nsentation in premise) and aci (context representa-\ntion from hypothesis which is align to word asi ),\nwe can model local inference (i.e., word-level in-\nference) between aligned word pairs. Intuitively,\nfor example, knowledge about hypernymy or hy-\nponymy may help model entailment and knowl-\nedge about antonymy and co-hyponyms may help\nmodel contradiction. Through comparing asi and\naci , in addition to their relation from external\nknowledge, we can obtain word-level inference\ninformation for each word. The same calcula-\ntion is performed for bsj and b\nc\nj . Thus, we collect\nknowledge-enriched local inference information:\nami = G([a\ns\ni ;a\nc\ni ;a\ns\ni − aci ;asi ◦ aci ;\nn∑\nj=1\nαijrij ]) , (7)\nbmj = G([b\ns\nj , b\nc\nj ; b\ns\nj − bcj ; bsj ◦ bcj ;\nm∑\ni=1\nβijrji]) , (8)\nwhere a heuristic matching trick with difference\nand element-wise product is used (Mou et al.,\n2016; Chen et al., 2017a). The last terms in Equa-\ntion (7)(8) are used to obtain word-level infer-\nence information from external knowledge. Take\nEquation (7) as example, rij is the relation fea-\nture between the i-th word in the premise and\nthe j-th word in the hypothesis, but we care\nmore about semantic relation between aligned\nword pairs between the premise and the hypoth-\nesis. Thus, we use a soft-aligned version through\nthe soft-alignment weight αij . For the i-th word\nin the premise, the last term in Equation (7) is\na word-level inference information based on ex-\nternal knowledge between the i-th word and the\naligned word. The same calculation for hypoth-\nesis is performed in Equation (8). G is a non-\nlinear mapping function to reduce dimensionality.\nSpecifically, we use a 1-layer feed-forward neural\nnetwork with the ReLU activation function with\na shortcut connection, i.e., concatenate the hidden\nstates after ReLU with the input\n∑n\nj=1 αijrij (or∑m\ni=1 βijrji) as the output a\nm\ni (or b\nm\nj ).\n3.5 Knowledge-Enhanced Inference\nComposition\nIn this component, we introduce knowledge-\nenriched inference composition. To determine the\noverall inference relationship between the premise\nand the hypothesis, we need to explore a compo-\nsition layer to compose the local inference vectors\n(am and bm) collected above:\navi = Composition(a\nm, i) , (9)\nbvj = Composition(b\nm, j) . (10)\nHere, we also use BiLSTMs as building blocks\nfor the composition layer, but the responsibility\nof BiLSTMs in the inference composition layer\nis completely different from that in the input en-\ncoding layer. The BiLSTMs here read local in-\nference vectors (am and bm) and learn to judge\nthe types of local inference relationship and dis-\ntinguish crucial local inference vectors for overall\nsentence-level inference relationship. Intuitively,\nthe final prediction is likely to depend on word\npairs appearing in external knowledge that have\nsome semantic relation. Our inference model con-\nverts the output hidden vectors of BiLSTMs to\nthe fixed-length vector with pooling operations\nand puts it into the final classifier to determine\nthe overall inference class. Particularly, in addi-\ntion to using mean pooling and max pooling sim-\nilarly to ESIM (Chen et al., 2017a), we propose\nto use weighted pooling based on external knowl-\nedge to obtain a fixed-length vector as in Equation\n(11)(12).\naw =\nm∑\ni=1\nexp(H(\n∑n\nj=1 αijrij))∑m\ni=1 exp(H(\n∑n\nj=1 αijrij))\navi , (11)\nbw =\nn∑\nj=1\nexp(H(\n∑m\ni=1 βijrji))∑n\nj=1 exp(H(\n∑m\ni=1 βijrji))\nbvj . (12)\nIn our experiments, we regard the function H as\na 1-layer feed-forward neural network with ReLU\nactivation function. We concatenate all pooling\nvectors, i.e., mean, max, and weighted pooling,\ninto the fixed-length vector and then put the vector\ninto the final multilayer perceptron (MLP) clas-\nsifier. The MLP has one hidden layer with tanh\nactivation and softmax output layer in our exper-\niments. The entire model is trained end-to-end,\nthrough minimizing the cross-entropy loss.\n4 Experiment Set-Up\n4.1 Representation of External Knowledge\nLexical Semantic Relations As described in\nSection 3.1, to incorporate external knowledge\n(as a knowledge vector rij) to the state-of-the-\nart neural network-based NLI models, we first\nexplore semantic relations in WordNet (Miller,\n1995), motivated by MacCartney (2009). Specif-\nically, the relations of lexical pairs are derived as\ndescribed in (1)-(4) below. Instead of using Jiang-\nConrath WordNet distance metric (Jiang and Con-\nrath, 1997), which does not improve the perfor-\nmance of our models on the development sets, we\nadd a new feature, i.e., co-hyponyms, which con-\nsistently benefit our models.\n(1) Synonymy: It takes the value 1 if the words in\nthe pair are synonyms in WordNet (i.e., be-\nlong to the same synset), and 0 otherwise. For\nexample, [felicitous, good] = 1, [dog, wolf] =\n0.\n(2) Antonymy: It takes the value 1 if the words\nin the pair are antonyms in WordNet, and 0\notherwise. For example, [wet, dry] = 1.\n(3) Hypernymy: It takes the value 1− n/8 if one\nword is a (direct or indirect) hypernym of the\nother word in WordNet, where n is the num-\nber of edges between the two words in hier-\narchies, and 0 otherwise. Note that we ignore\npairs in the hierarchy which have more than 8\nedges in between. For example, [dog, canid]\n= 0.875, [wolf, canid] = 0.875, [dog, carni-\nvore] = 0.75, [canid, dog] = 0\n(4) Hyponymy: It is simply the inverse of the hy-\npernymy feature. For example, [canid, dog]\n= 0.875, [dog, canid] = 0.\n(5) Co-hyponyms: It takes the value 1 if the two\nwords have the same hypernym but they do\nnot belong to the same synset, and 0 other-\nwise. For example, [dog, wolf] = 1.\nAs discussed above, we expect features like syn-\nonymy, antonymy, hypernymy, hyponymy and co-\nhyponyms would help model co-attention align-\nment between the premise and the hypothesis.\nKnowledge of hypernymy and hyponymy may help\ncapture entailment; knowledge of antonymy and\nco-hyponyms may help model contradiction. Their\nfinal contributions will be learned in end-to-end\nmodel training. We regard the vector r ∈ Rdr as\nthe relation feature derived from external knowl-\nedge, where dr is 5 here. In addition, Table 1 re-\nports some key statistics of these features.\nFeature #Words #Pairs\nSynonymy 84,487 237,937\nAntonymy 6,161 6,617\nHypernymy 57,475 753,086\nHyponymy 57,475 753,086\nCo-hyponyms 53,281 3,674,700\nTable 1: Statistics of lexical relation features.\nIn addition to the above relations, we also use\nmore relation features in WordNet, including in-\nstance, instance of, same instance, entailment,\nmember meronym, member holonym, substance\nmeronym, substance holonym, part meronym, part\nholonym, summing up to 15 features, but these ad-\nditional features do not bring further improvement\non the development dataset, as also discussed in\nSection 5.\nRelation Embeddings In the most recent years\ngraph embedding has been widely employed to\nlearn representation for vertexes and their relations\nin a graph. In our work here, we also capture\nthe relation between any two words in WordNet\nthrough relation embedding. Specifically, we em-\nployed TransE (Bordes et al., 2013), a widely used\ngraph embedding methods, to capture relation em-\nbedding between any two words. We used two\ntypical approaches to obtaining the relation em-\nbedding. The first directly uses 18 relation em-\nbeddings pretrained on the WN18 dataset (Bordes\net al., 2013). Specifically, if a word pair has a cer-\ntain type relation, we take the corresponding re-\nlation embedding. Sometimes, if a word pair has\nmultiple relations among the 18 types; we take an\naverage of the relation embedding. The second ap-\nproach uses TransE’s word embedding (trained on\nWordNet) to obtain relation embedding, through\nthe objective function used in TransE, i.e., l ≈\nt− h, where l indicates relation embedding, t in-\ndicates tail entity embedding, and h indicates head\nentity embedding.\nNote that in addition to relation embedding\ntrained on WordNet, other relational embedding\nresources exist; e.g., that trained on Freebase\n(WikiData) (Bollacker et al., 2007), but such\nknowledge resources are mainly about facts (e.g.,\nrelationship between Bill Gates and Microsoft)\nand are less for commonsense knowledge used in\ngeneral natural language inference (e.g., the color\nyellow potentially contradicts red).\n4.2 NLI Datasets\nIn our experiments, we use Stanford Natural Lan-\nguage Inference (SNLI) dataset (Bowman et al.,\n2015) and Multi-Genre Natural Language Infer-\nence (MultiNLI) (Williams et al., 2017) dataset,\nwhich focus on three basic relations between a\npremise and a potential hypothesis: the premise\nentails the hypothesis (entailment), they contradict\neach other (contradiction), or they are not related\n(neutral). We use the same data split as in previ-\nous work (Bowman et al., 2015; Williams et al.,\n2017) and classification accuracy as the evaluation\nmetric. In addition, we test our models (trained on\nthe SNLI training set) on a new test set (Glockner\net al., 2018), which assesses the lexical inference\nabilities of NLI systems and consists of 8,193 sam-\nples. WordNet 3.0 (Miller, 1995) is used to extract\nsemantic relation features between words. The\nwords are lemmatized using Stanford CoreNLP\n3.7.0 (Manning et al., 2014). The premise and the\nhypothesis sentences fed into the input encoding\nlayer are tokenized.\n4.3 Training Details\nFor duplicability, we release our code1. All our\nmodels were strictly selected on the development\nset of the SNLI data and the in-domain devel-\nopment set of MultiNLI and were then tested on\nthe corresponding test set. The main training de-\ntails are as follows: the dimension of the hid-\nden states of LSTMs and word embeddings are\n300. The word embeddings are initialized by\n300D GloVe 840B (Pennington et al., 2014), and\nout-of-vocabulary words among them are initial-\nized randomly. All word embeddings are updated\nduring training. Adam (Kingma and Ba, 2014)\nis used for optimization with an initial learning\nrate of 0.0004. The mini-batch size is set to 32.\nNote that the above hyperparameter settings are\nsame as those used in the baseline ESIM (Chen\net al., 2017a) model. ESIM is a strong NLI\nbaseline framework with the source code made\navailable at https://github.com/lukecq1231/nli (the\nESIM core code has also been adapted to sum-\nmarization (Chen et al., 2016a) and question-\nanswering tasks (Zhang et al., 2017a)).\nThe trade-off λ for calculating co-\n1https://github.com/lukecq1231/kim\nattention in Equation (3) is selected in\n[0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50] based on the\ndevelopment set. When training TransE for\nWordNet, relations are represented with vectors\nof 20 dimension.\n5 Experimental Results\n5.1 Overall Performance\nTable 2 shows the results of state-of-the-art models\non the SNLI dataset. Among them, ESIM (Chen\net al., 2017a) is one of the previous state-of-the-art\nsystems with an 88.0% test-set accuracy. The pro-\nposed model, namely Knowledge-based Inference\nModel (KIM), which enriches ESIM with external\nknowledge, obtains an accuracy of 88.6%, the best\nsingle-model performance reported on the SNLI\ndataset. The difference between ESIM and KIM is\nstatistically significant under the one-tailed paired\nt-test at the 99% significance level. Note that the\nKIM model reported here uses five semantic rela-\ntions described in Section 4. In addition to that, we\nalso use 15 semantic relation features, which does\nnot bring additional gains in performance. These\nresults highlight the effectiveness of the five se-\nmantic relations described in Section 4. To further\ninvestigate external knowledge, we add TransE re-\nlation embedding, and again no further improve-\nment is observed on both the development and test\nsets when TransE relation embedding is used (con-\ncatenated) with the semantic relation vectors. We\nconsider this is due to the fact that TransE embed-\nding is not specifically sensitive to inference in-\nformation; e.g., it does not model co-hyponyms\nfeatures, and its potential benefit has already been\ncovered by the semantic relation features used.\nTable 3 shows the performance of models on the\nMultiNLI dataset. The baseline ESIM achieves\n76.8% and 75.8% on in-domain and cross-domain\ntest set, respectively. If we extend the ESIM with\nexternal knowledge, we achieve significant gains\nto 77.2% and 76.4% respectively. Again, the gains\nare consistent on SNLI and MultiNLI, and we ex-\npect they would be orthogonal to other factors\nwhen external knowledge is added into other state-\nof-the-art models.\n5.2 Ablation Results\nFigure 2 displays the ablation analysis of differ-\nent components when using the external knowl-\nedge. To compare the effects of external knowl-\nedge under different training data scales, we ran-\nModel Test\nLSTM Att. (Rockta¨schel et al., 2015) 83.5\nDF-LSTMs (Liu et al., 2016a) 84.6\nTC-LSTMs (Liu et al., 2016b) 85.1\nMatch-LSTM (Wang and Jiang, 2016) 86.1\nLSTMN (Cheng et al., 2016) 86.3\nDecomposable Att. (Parikh et al., 2016) 86.8\nNTI (Yu and Munkhdalai, 2017b) 87.3\nRe-read LSTM (Sha et al., 2016) 87.5\nBiMPM (Wang et al., 2017) 87.5\nDIIN (Gong et al., 2017) 88.0\nBCN + CoVe (McCann et al., 2017) 88.1\nCAFE (Tay et al., 2018) 88.5\nESIM (Chen et al., 2017a) 88.0\nKIM (This paper) 88.6\nTable 2: Accuracies of models on SNLI.\nModel In Cross\nCBOW (Williams et al., 2017) 64.8 64.5\nBiLSTM (Williams et al., 2017) 66.9 66.9\nDiSAN (Shen et al., 2017) 71.0 71.4\nGated BiLSTM (Chen et al., 2017b) 73.5 73.6\nSS BiLSTM (Nie and Bansal, 2017) 74.6 73.6\nDIIN * (Gong et al., 2017) 77.8 78.8\nCAFE (Tay et al., 2018) 78.7 77.9\nESIM (Chen et al., 2017a) 76.8 75.8\nKIM (This paper) 77.2 76.4\nTable 3: Accuracies of models on MultiNLI. * in-\ndicates models using extra SNLI training set.\ndomly sample different ratios of the entire training\nset, i.e., 0.8%, 4%, 20% and 100%. “A” indicates\nadding external knowledge in calculating the co-\nattention matrix as in Equation (3), “I” indicates\nadding external knowledge in collecting local in-\nference information as in Equation (7)(8), and “C”\nindicates adding external knowledge in compos-\ning inference as in Equation (11)(12). When we\nonly have restricted training data, i.e., 0.8% train-\ning set (about 4,000 samples), the baseline ESIM\nhas a poor accuracy of 62.4%. When we only\nadd external knowledge in calculating co-attention\n(“A”), the accuracy increases to 66.6% (+ absolute\n4.2%). When we only utilize external knowledge\nin collecting local inference information (“I”), the\naccuracy has a significant gain, to 70.3% (+ ab-\nsolute 7.9%). When we only add external knowl-\nedge in inference composition (“C”), the accuracy\ngets a smaller gain to 63.4% (+ absolute 1.0%).\nThe comparison indicates that “I” plays the most\nimportant role among the three components in us-\ning external knowledge. Moreover, when we com-\npose the three components (“A,I,C”), we obtain\nthe best result of 72.6% (+ absolute 10.2%). When\nwe use more training data, i.e., 4%, 20%, 100%\nof the training set, only “I” achieves a significant\ngain, but “A” or “C” does not bring any signifi-\ncant improvement. The results indicate that ex-\nternal semantic knowledge only helps co-attention\nand composition when limited training data is lim-\nited, but always helps in collecting local inference\ninformation. Meanwhile, for less training data, λ\nis usually set to a larger value. For example, the\noptimal λ on the development set is 20 for 0.8%\ntraining set, 2 for the 4% training set, 1 for the\n20% training set and 0.2 for the 100% training set.\nFigure 3 displays the results of using different\nratios of external knowledge (randomly keep dif-\nferent percentages of whole lexical semantic rela-\ntions) under different sizes of training data. Note\nthat here we only use external knowledge in col-\nlecting local inference information as it always\nworks well for different scale of the training set.\nBetter accuracies are achieved when using more\nexternal knowledge. Especially under the condi-\ntion of restricted training data (0.8%), the model\nobtains a large gain when using more than half of\nexternal knowledge.\nFigure 2: Accuracies of models of incorporat-\ning external knowledge into different NLI compo-\nnents, under different sizes of training data (0.8%,\n4%, 20%, and the entire training data).\n5.3 Analysis on the (Glockner et al., 2018)\nTest Set\nIn addition, Table 4 shows the results on a newly\npublished test set (Glockner et al., 2018). Com-\npared with the performance on the SNLI test\nFigure 3: Accuracies of models under differ-\nent sizes of external knowledge. More external\nknowledge corresponds to higher accuracies.\nModel SNLI Glockner’s(∆)\n(Parikh et al., 2016)* 84.7 51.9 (-32.8)\n(Nie and Bansal, 2017)* 86.0 62.2 (-23.8)\nESIM * 87.9 65.6 (-22.3)\nKIM (This paper) 88.6 83.5 ( -5.1)\nTable 4: Accuracies of models on the SNLI and\n(Glockner et al., 2018) test set. * indicates the re-\nsults taken from (Glockner et al., 2018).\nset, the performance of the three baseline mod-\nels dropped substantially on the (Glockner et al.,\n2018) test set, with the differences ranging from\n22.3% to 32.8% in accuracy. Instead, the proposed\nKIM achieves 83.5% on this test set (with only a\n5.1% drop in performance), which demonstrates\nits better ability of utilizing lexical level inference\nand hence better generalizability.\nFigure 5 displays the accuracy of ESIM\nand KIM in each replacement-word category of\nthe (Glockner et al., 2018) test set. KIM outper-\nforms ESIM in 13 out of 14 categories, and only\nperforms worse on synonyms.\n5.4 Analysis by Inference Categories\nWe perform more analysis (Table 6) using the sup-\nplementary annotations provided by the MultiNLI\ndataset (Williams et al., 2017), which have 495\nsamples (about 1/20 of the entire development set)\nfor both in-domain and out-domain set. We com-\npare against the model outputs of the ESIM model\nacross 13 categories of inference. Table 6 reports\nthe results. We can see that KIM outperforms\nESIM on overall accuracies on both in-domain and\nCategory Instance ESIM KIM\nAntonyms 1,147 70.4 86.5\nCardinals 759 75.5 93.4\nNationalities 755 35.9 73.5\nDrinks 731 63.7 96.6\nAntonyms WordNet 706 74.6 78.8\nColors 699 96.1 98.3\nOrdinals 663 21.0 56.6\nCountries 613 25.4 70.8\nRooms 595 69.4 77.6\nMaterials 397 89.7 98.7\nVegetables 109 31.2 79.8\nInstruments 65 90.8 96.9\nPlanets 60 3.3 5.0\nSynonyms 894 99.7 92.1\nOverall 8,193 65.6 83.5\nTable 5: The number of instances and accu-\nracy per category achieved by ESIM and KIM on\nthe (Glockner et al., 2018) test set.\nCategory In-domain Cross-domain\nESIM KIM ESIM KIM\nActive/Passive 93.3 93.3 100.0 100.0\nAntonym 76.5 76.5 70.0 75.0\nBelief 72.7 75.8 75.9 79.3\nConditional 65.2 65.2 61.5 69.2\nCoreference 80.0 76.7 75.9 75.9\nLong sentence 82.8 78.8 69.7 73.4\nModal 80.6 79.9 77.0 80.2\nNegation 76.7 79.8 73.1 71.2\nParaphrase 84.0 72.0 86.5 89.2\nQuantity/Time 66.7 66.7 56.4 59.0\nQuantifier 79.2 78.4 73.6 77.1\nTense 74.5 78.4 72.2 66.7\nWord overlap 89.3 85.7 83.8 81.1\nOverall 77.1 77.9 76.7 77.4\nTable 6: Detailed Analysis on MultiNLI.\ncross-domain subset of development set. KIM out-\nperforms or equals ESIM in 10 out of 13 cate-\ngories on the cross-domain setting, while only 7\nout of 13 categories on in-domain setting. It indi-\ncates that external knowledge helps more in cross-\ndomain setting. Especially, for antonym category\nin cross-domain set, KIM outperform ESIM sig-\nnificantly (+ absolute 5.0%) as expected, because\nantonym feature captured by external knowledge\nwould help unseen cross-domain samples.\n5.5 Case Study\nTable 7 includes some examples from the SNLI\ntest set, where KIM successfully predicts the in-\nference relation and ESIM fails. In the first exam-\nP/G Sentences\ne/c p: An African person standing in a wheat\nfield.\nh: A person standing in a corn field.\ne/c p: Little girl is flipping an omelet in the\nkitchen.\nh: A young girl cooks pancakes.\nc/e p: A middle eastern marketplace.\nh: A middle easten store.\nc/e p: Two boys are swimming with boogie\nboards.\nh: Two boys are swimming with their floats.\nTable 7: Examples. Word in bold are key words\nin making final prediction. P indicates a predicted\nlabel and G indicates gold-standard label. e and c\ndenote entailment and contradiction, respectively.\nple, the premise is “An African person standing in\na wheat field” and the hypothesis “A person stand-\ning in a corn field”. As the KIM model knows that\n“wheat” and “corn” are both a kind of cereal, i.e,\nthe co-hyponyms relationship in our relation fea-\ntures, KIM therefore predicts the premise contra-\ndicts the hypothesis. However, the baseline ESIM\ncannot learn the relationship between “wheat” and\n“corn” effectively due to lack of enough samples\nin the training sets. With the help of external\nknowledge, i.e., “wheat” and “corn” having the\nsame hypernym “cereal”, KIM predicts contradic-\ntion correctly.\n6 Conclusions\nOur neural-network-based model for natural lan-\nguage inference with external knowledge, namely\nKIM, achieves the state-of-the-art accuracies. The\nmodel is equipped with external knowledge in its\nmain components, specifically, in calculating co-\nattention, collecting local inference, and compos-\ning inference. We provide detailed analyses on our\nmodel and results. The proposed model of infus-\ning neural networks with external knowledge may\nalso help shed some light on tasks other than NLI.\nAcknowledgments\nWe thank Yibo Sun and Bing Qin for early helpful\ndiscussion.\nReferences\nSungjin Ahn, Heeyoul Choi, Tanel Pa¨rnamaa, and\nYoshua Bengio. 2016. A neural knowledge lan-\nguage model. CoRR, abs/1608.00318.\nKurt D. Bollacker, Robert P. Cook, and Patrick Tufts.\n2007. Freebase: A shared database of structured\ngeneral human knowledge. In Proceedings of the\nTwenty-Second AAAI Conference on Artificial In-\ntelligence, July 22-26, 2007, Vancouver, British\nColumbia, Canada, pages 1962–1963.\nAntoine Bordes, Nicolas Usunier, Alberto Garcı´a-\nDura´n, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference on\nNeural Information Processing Systems 2013. Pro-\nceedings of a meeting held December 5-8, 2013,\nLake Tahoe, Nevada, United States., pages 2787–\n2795.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large an-\nnotated corpus for learning natural language infer-\nence. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Process-\ning, EMNLP 2015, Lisbon, Portugal, September 17-\n21, 2015, pages 632–642.\nSamuel R. Bowman, Jon Gauthier, Abhinav Ras-\ntogi, Raghav Gupta, Christopher D. Manning, and\nChristopher Potts. 2016. A fast unified model for\nparsing and sentence understanding. In Proceedings\nof the 54th Annual Meeting of the Association for\nComputational Linguistics, ACL 2016, August 7-12,\n2016, Berlin, Germany, Volume 1: Long Papers.\nJane Bromley, Isabelle Guyon, Yann LeCun, Eduard\nSa¨ckinger, and Roopak Shah. 1993. Signature veri-\nfication using a siamese time delay neural network.\nIn Advances in Neural Information Processing Sys-\ntems 6, [7th NIPS Conference, Denver, Colorado,\nUSA, 1993], pages 737–744.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,\nand Hui Jiang. 2016a. Distraction-based neural net-\nworks for modeling document. In Proceedings of\nthe Twenty-Fifth International Joint Conference on\nArtificial Intelligence, IJCAI 2016, New York, NY,\nUSA, 9-15 July 2016, pages 2754–2760.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei,\nHui Jiang, and Diana Inkpen. 2017a. Enhanced\nLSTM for natural language inference. In Proceed-\nings of the 55th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2017, Vancou-\nver, Canada, July 30 - August 4, Volume 1: Long\nPapers, pages 1657–1668.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui\nJiang, and Diana Inkpen. 2017b. Recurrent neural\nnetwork-based sentence encoder with gated atten-\ntion for natural language inference. In Proceedings\nof the 2nd Workshop on Evaluating Vector Space\nRepresentations for NLP, RepEval@EMNLP 2017,\nCopenhagen, Denmark, September 8, 2017, pages\n36–40.\nYun-Nung Chen, Dilek Z. Hakkani-Tu¨r, Go¨khan Tu¨r,\nAsli C¸elikyilmaz, Jianfeng Gao, and Li Deng.\n2016b. Knowledge as a teacher: Knowledge-\nguided structural attention networks. CoRR,\nabs/1609.03286.\nZhigang Chen, Wei Lin, Qian Chen, Xiaoping Chen,\nSi Wei, Hui Jiang, and Xiaodan Zhu. 2015. Re-\nvisiting word embedding for contrasting meaning.\nIn Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the\n7th International Joint Conference on Natural Lan-\nguage Processing of the Asian Federation of Natural\nLanguage Processing, ACL 2015, July 26-31, 2015,\nBeijing, China, Volume 1: Long Papers, pages 106–\n115.\nJianpeng Cheng, Li Dong, and Mirella Lapata. 2016.\nLong short-term memory-networks for machine\nreading. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Process-\ning, EMNLP 2016, Austin, Texas, USA, November\n1-4, 2016, pages 551–561.\nJihun Choi, Kang Min Yoo, and Sang-goo Lee. 2017.\nUnsupervised learning of task-specific tree struc-\ntures with tree-lstms. CoRR, abs/1707.02786.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loı¨c\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2017, Copen-\nhagen, Denmark, September 9-11, 2017, pages 670–\n680.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL recognising textual entailment\nchallenge. In Machine Learning Challenges, Eval-\nuating Predictive Uncertainty, Visual Object Clas-\nsification and Recognizing Textual Entailment, First\nPASCAL Machine Learning Challenges Workshop,\nMLCW 2005, Southampton, UK, April 11-13, 2005,\nRevised Selected Papers, pages 177–190.\nManaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar,\nChris Dyer, Eduard H. Hovy, and Noah A. Smith.\n2015. Retrofitting word vectors to semantic lexi-\ncons. In NAACL HLT 2015, The 2015 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Denver, Colorado, USA, May 31 - June 5,\n2015, pages 1606–1615.\nMax Glockner, Vered Shwartz, and Yoav Goldberg.\n2018. Breaking nli systems with sentences that re-\nquire simple lexical inferences. In The 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL), Melbourne, Australia.\nYichen Gong, Heng Luo, and Jian Zhang. 2017.\nNatural language inference over interaction space.\nCoRR, abs/1709.04348.\nSepp Hochreiter and Ju¨rgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation,\n9(8):1735–1780.\nAdrian Iftene and Alexandra Balahur-Dobrescu. 2007.\nProceedings of the ACL-PASCAL Workshop on Tex-\ntual Entailment and Paraphrasing, chapter Hypoth-\nesis Transformation and Semantic Variability Rules\nUsed in Recognizing Textual Entailment. Associa-\ntion for Computational Linguistics.\nJay J. Jiang and David W. Conrath. 1997. Seman-\ntic similarity based on corpus statistics and lexical\ntaxonomy. In Proceedings of the 10th Research\non Computational Linguistics International Confer-\nence, ROCLING 1997, Taipei, Taiwan, August 1997,\npages 19–33.\nValentin Jijkoun and Maarten de Rijke. 2005. Recog-\nnizing textual entailment using lexical similarity. In\nProceedings of the PASCAL Challenges Workshop\non Recognising Textual Entailment.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\nZhouhan Lin, Minwei Feng, Cı´cero Nogueira dos San-\ntos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua\nBengio. 2017. A structured self-attentive sentence\nembedding. CoRR, abs/1703.03130.\nPengfei Liu, Xipeng Qiu, Jifan Chen, and Xuanjing\nHuang. 2016a. Deep fusion lstms for text seman-\ntic matching. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-\nmany, Volume 1: Long Papers.\nPengfei Liu, Xipeng Qiu, Yaqian Zhou, Jifan Chen, and\nXuanjing Huang. 2016b. Modelling interaction of\nsentence pair with coupled-lstms. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2016, Austin,\nTexas, USA, November 1-4, 2016, pages 1703–1712.\nQuan Liu, Hui Jiang, Si Wei, Zhen-Hua Ling, and\nYu Hu. 2015. Learning semantic word embeddings\nbased on ordinal knowledge constraints. In Pro-\nceedings of the 53rd Annual Meeting of the Associ-\nation for Computational Linguistics and the 7th In-\nternational Joint Conference on Natural Language\nProcessing of the Asian Federation of Natural Lan-\nguage Processing, ACL 2015, July 26-31, 2015, Bei-\njing, China, Volume 1: Long Papers, pages 1501–\n1511.\nYang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang.\n2016c. Learning natural language inference us-\ning bidirectional LSTM model and inner-attention.\nCoRR, abs/1605.09090.\nBill MacCartney. 2009. Natural Language Inference.\nPh.D. thesis, Stanford University.\nBill MacCartney, Michel Galley, and Christopher D.\nManning. 2008. A phrase-based alignment model\nfor natural language inference. In 2008 Conference\non Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2008, Proceedings of the Confer-\nence, 25-27 October 2008, Honolulu, Hawaii, USA,\nA meeting of SIGDAT, a Special Interest Group of\nthe ACL, pages 802–811.\nChristopher D. Manning, Mihai Surdeanu, John Bauer,\nJenny Rose Finkel, Steven Bethard, and David Mc-\nClosky. 2014. The stanford corenlp natural language\nprocessing toolkit. In Proceedings of the 52nd An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2014, June 22-27, 2014, Baltimore,\nMD, USA, System Demonstrations, pages 55–60.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural\nInformation Processing Systems 30: Annual Con-\nference on Neural Information Processing Systems\n2017, 4-9 December 2017, Long Beach, CA, USA,\npages 6297–6308.\nGeorge A. Miller. 1995. Wordnet: A lexical database\nfor english. Commun. ACM, 38(11):39–41.\nLili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui\nYan, and Zhi Jin. 2016. Natural language infer-\nence by tree-based convolution and heuristic match-\ning. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2016, August 7-12, 2016, Berlin, Germany, Volume\n2: Short Papers.\nNikola Mrksic, Ivan Vulic, Diarmuid O´ Se´aghdha, Ira\nLeviant, Roi Reichart, Milica Gasic, Anna Korho-\nnen, and Steve J. Young. 2017. Semantic special-\nisation of distributional word vector spaces using\nmonolingual and cross-lingual constraints. CoRR,\nabs/1706.00374.\nYixin Nie and Mohit Bansal. 2017. Shortcut-\nstacked sentence encoders for multi-domain infer-\nence. In Proceedings of the 2nd Workshop on\nEvaluating Vector Space Representations for NLP,\nRepEval@EMNLP 2017, Copenhagen, Denmark,\nSeptember 8, 2017, pages 41–45.\nAnkur P. Parikh, Oscar Ta¨ckstro¨m, Dipanjan Das, and\nJakob Uszkoreit. 2016. A decomposable attention\nmodel for natural language inference. In Proceed-\nings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2016,\nAustin, Texas, USA, November 1-4, 2016, pages\n2249–2255.\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In Proceedings of the 2014\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2014, October 25-29,\n2014, Doha, Qatar, A meeting of SIGDAT, a Special\nInterest Group of the ACL, pages 1532–1543.\nTim Rockta¨schel, Edward Grefenstette, Karl Moritz\nHermann, Toma´s Kocisky´, and Phil Blunsom. 2015.\nReasoning about entailment with neural attention.\nCoRR, abs/1509.06664.\nLei Sha, Baobao Chang, Zhifang Sui, and Sujian Li.\n2016. Reading and thinking: Re-read LSTM unit\nfor textual entailment recognition. In COLING\n2016, 26th International Conference on Computa-\ntional Linguistics, Proceedings of the Conference:\nTechnical Papers, December 11-16, 2016, Osaka,\nJapan, pages 2870–2879.\nTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,\nShirui Pan, and Chengqi Zhang. 2017. Disan: Di-\nrectional self-attention network for rnn/cnn-free lan-\nguage understanding. CoRR, abs/1709.04696.\nTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Sen\nWang, and Chengqi Zhang. 2018. Reinforced self-\nattention network: a hybrid of hard and soft attention\nfor sequence modeling. CoRR, abs/1801.10296.\nChen Shi, Shujie Liu, Shuo Ren, Shi Feng, Mu Li,\nMing Zhou, Xu Sun, and Houfeng Wang. 2016.\nKnowledge-based semantic embedding for machine\ntranslation. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguis-\ntics, ACL 2016, August 7-12, 2016, Berlin, Ger-\nmany, Volume 1: Long Papers.\nYi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2018. A\ncompare-propagate architecture with alignment fac-\ntorization for natural language inference. CoRR,\nabs/1801.00102.\nIvan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel\nUrtasun. 2015. Order-embeddings of images and\nlanguage. CoRR, abs/1511.06361.\nShuohang Wang and Jing Jiang. 2016. Learning natu-\nral language inference with LSTM. In NAACL HLT\n2016, The 2016 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, San Diego\nCalifornia, USA, June 12-17, 2016, pages 1442–\n1451.\nZhiguo Wang, Wael Hamza, and Radu Florian. 2017.\nBilateral multi-perspective matching for natural lan-\nguage sentences. In Proceedings of the Twenty-Sixth\nInternational Joint Conference on Artificial Intelli-\ngence, IJCAI 2017, Melbourne, Australia, August\n19-25, 2017, pages 4144–4150.\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen\nLivescu. 2015. From paraphrase database to compo-\nsitional paraphrase model and back. TACL, 3:345–\n358.\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. CoRR,\nabs/1704.05426.\nHong Yu and Tsendsuren Munkhdalai. 2017a. Neural\nsemantic encoders. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics, EACL 2017, Valen-\ncia, Spain, April 3-7, 2017, Volume 1: Long Papers,\npages 397–407.\nHong Yu and Tsendsuren Munkhdalai. 2017b. Neu-\nral tree indexers for text understanding. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics,\nEACL 2017, Valencia, Spain, April 3-7, 2017, Vol-\nume 1: Long Papers, pages 11–21.\nJunbei Zhang, Xiaodan Zhu, Qian Chen, Lirong\nDai, Si Wei, and Hui Jiang. 2017a. Ex-\nploring question understanding and adaptation in\nneural-network-based question answering. CoRR,\nabs/arXiv:1703.04617v2.\nShiyue Zhang, Gulnigar Mahmut, Dong Wang, and\nAskar Hamdulla. 2017b. Memory-augmented\nchinese-uyghur neural machine translation. In 2017\nAsia-Pacific Signal and Information Processing As-\nsociation Annual Summit and Conference, APSIPA\nASC 2017, Kuala Lumpur, Malaysia, December 12-\n15, 2017, pages 1092–1096.\n",
    "Link": "http://arxiv.org/abs/1711.04289"
}