{
    "Title": "When Are We Done with Games?",
    "Authors": "Debus, Michael S., Justesen, Niels Orsleff, Risi, Sebastian",
    "Year": "No year available",
    "Abstract": null,
    "Keywords": "No keywords available",
    "Publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
    "Publication Date": "No publication date available",
    "Journal": "No journal available",
    "Citation Count": 0,
    "Full Text": "When Are We Done with Games?Niels JustesenIT University of CopenhagenCopenhagen, Denmarknoju@itu.dkMichael S. DebusIT University of CopenhagenCopenhagen, Denmarkmsde@itu.dkSebastian RisiIT University of CopenhagenCopenhagen, Denmarksebr@itu.dkAbstract—From an early point, games have been promotedas important challenges within the research field of ArtificialIntelligence (AI). Recent developments in machine learning haveallowed a few AI systems to win against top professionals ineven the most challenging video games, including Dota 2 andStarCraft. It thus may seem that AI has now achieved all ofthe long-standing goals that were set forth by the researchcommunity. In this paper, we introduce a black box approachthat provides a pragmatic way of evaluating the fairness of AI vs.human competitions, by only considering motoric and perceptualfairness on the competitors’ side. Additionally, we introduce thenotion of extrinsic and intrinsic factors of a game competition andapply these to discuss and compare the competitions in relationto human vs. human competitions. We conclude that Dota 2 andStarCraft II are not yet mastered by AI as they so far only havebeen able to win against top professionals in limited competitionstructures in restricted variants of the games.I. INTRODUCTIONGames provide a suite of diverse and challenging tasks,helping to benchmark progress in the field of Artificial In-telligence (AI). Performing well in renowned games such asChess, Go, Dota 2 (Valve Corporation, 2013), and StarCraftII (Blizzard Entertainment, 2010) are important milestones inAI; due to their immense computational complexities, playingthem well seemed to require some form of human-levelintelligence. These games are, after all, designed to challengethe human mind on a highly competitive level. However,due to the increase in computational resources and recentdevelopments in machine learning, deep neural networks nowseem to achieve human-level control in these games, from self-play alone or with the help of human demonstrations. WithDeepMind’s ’AlphaGo’ beating the world’s best Go playerLee Sedol, ’OpenAI Five’ beating a team of professionalsin a restricted game of Dota 2, and DeepMind’s ’AlphaStar’beating the professional StarCraft II player Grzegorz ’MaNa’Komincz on one selected map, it may seem that AI has nowachieved all of these long-standing goals that were set forthby the research community. So where does this leave us? Arewe, as AI researchers, done with games?This paper provides a discussion of designing and evaluatingfairness in human vs. AI game competitions. Ultimately,we argue that a claim of superiority of AIs over humansis unfounded, until AIs compete with and beat humans incompetitions that are structurally the same as common humanvs. human competitions. These competitions are, after all,designed to erase particular elements of unfairness within thegame, the players, or their environments.We take a black-box approach that ignores some dimensionsof fairness such as learning speed and prior knowledge,focusing only on perceptual and motoric fairness. Additionally,we introduce the notions of game extrinsic factors, such as thecompetition format and rules, and game intrinsic factors, suchas different mechanical systems and configurations within onegame. We apply these terms to critically review the aforemen-tioned AI achievements and observe that game extrinsic factorsare rarely discussed in this context, and that game intrinsicfactors are significantly limited in AI vs. human competitionsin digital games.Claimed AI achievements in games were also reviewedby Canaan et al. [14], focusing on how researchers and themedia have portrayed the achieved results. Additionally, theyproposed six dimensions of fairness in human vs. AI gamecompetitions: 1) Perceptual: do the two competitors use thesame input space? 2) Motoric: do the two competitors use thesame output space? 3) Historic: did the two systems spend thesame amount of time on training? 4) Knowledge: do the twosystems have the same access to declarative knowledge aboutthe game? 5) Compute: do the two systems have the samecomputational power? 6) Common-sense: do the agents havethe same knowledge about other things? Based on their sixdimensions of fairness, they concluded that “a completely faircompetition can only be achieved against an artificial systemthat is essentially equivalent to a flesh and blood human”. Wewill return to their evaluation in a later paragraph.Our main critique of current evaluation procedures istwofold. AI superiority in games cannot be claimed withoutcarefully treating the game extrinsic and intrinsic factors,such as the competition’s structure and rules, and the game’sconfigurations. First of all, it is necessary that AIs compete ina game’s extrinsic tournament structures, as already employedin human vs. human competitions. Thus, to formulate a propercompetition between humans and AI systems, we argue thatwe must first study the extrinsic game factors that are inplay when humans are competing and then formulate anexperimental setup that imitates them, without limiting thegame’s intrinsic variables, such as maps, races, heroes, etc.,as is currently be done.Through the discussion of two areas (the competitors per-ceptual and motoric abilities; and the game’s extrinsic andintrinsic factors) we hope to show that, so far, no fair compe-tition between AIs and human has occurred in Dota 2 andStarCraft II. We argue that, if these factors are accountedfor in the future, and we ignore the competitors individualcharacteristics of knowledge acquisition (considering them asblack boxes), we can construct a competition that is capableof producing a fair evaluation of the competitors’ output. Thisoutput can then form the basis for future discussions on AIvs. human intelligence. For instance, if the AI wins in ourhypothetically fair competition, does this mean it is moreintelligent? If the human wins, what are the areas in whichthe AI has to improve? Are there still other factors that wedid not observe and account for? In this manner, our currentapproach does not claim ultimate correctness, but constitutesa step forward in the area of human vs. AI competition, bycritically evaluating the current state and proposing areas ofconsideration for future competitions. Thus, our approach isdifferent from Canaan et al.’s [14] in that they claim theimpossibility of a fair competition due to differences in thecompetitors, which we simply treat as black boxes and focuson the competition instead.While this paper focuses on claims of super-human perfor-mance in games, there are other claims made in this contextthat are worth discussing; for example, whether a system haslearned from tabula rasa [29]. Due to our black-box approach,these questions will not be discussed in this paper.In particular, we aim to address the game AI commu-nity with this paper and its discussion of how to createa competition that enables us to claim AI superiority. Theimplications for players, communities, game designers andgame studies researchers outside of AI are outside the scope ofthis paper. Furthermore, the scope of this paper is limited toonly considering AI systems in the role of playing a gamecompetitively. It is, however, important to not neglect thatAI/CI has many other roles for games such as proceduralcontent generation, player modeling, and data mining [50].When we ask the question are we done with games?, we arethus only concerned with the traditional branch of AI in gameswhere the goal is to create a system that plays competitively.II. GAMES FOR AISince the birth of the research field of AI in 1956, gameshave been a vital test bed to compare algorithms and mea-sure the progress in the field. Several games have throughthe years been promoted by researchers as key challengesfor AI with the ultimate goal of defeating the best humanplayers. Initially, traditional board games stood the test, suchas Checkers, Backgammon, and Chess. Shannon wrote in hisseminal paper on computational Chess, that “chess is generallyconsidered to require ‘thinking’ for skillful play; a solution ofthis problem will force us either to admit the possibility ofa mechanized thinking or to further restrict our concept of‘thinking’” [40] and similarly Herbert Simon wrote “if onecould devise a successful chess machine one would seem tohave penetrated the core of human intellectual endeavor” [42].Today, it is undisputed that computers play these games at asuper-human level. After Chess, Go was promoted as the nextgrand AI challenge due to its significantly higher complexity[9, 11, 22, 28, 30]. Demis Hassabis, the CEO of the companyDeepMind that eventually developed the first computer systemto beat a human professional in Go, has described Go in aninterview as the “Mount Everest” for AI scientists1.Video games, on the other hand, present a new array ofchallenges for AI, which has led researchers to promotethe Atari 2600 arcade video games [10], Dota 2 [20], andStarCraft [12, 32, 45, 47] as even harder challenges. Re-searchers have considered StarCraft to be the hardest gamesfor computers to play [16, 51], which ultimately suggests thatthis game is the final grand challenge for AI in games beforetackling real-world problems.III. THE BLACKBOX APPROACHIn this paper, we propose a pragmatic way of evaluating AIagainst human intelligence in game competitions. To be ableto do this, some obvious differences have to be pointed outand disregarded. First of all, for the purpose of comparabil-ity, we consider AI systems as black boxes. Their training,knowledge, common sense and idiosyncratic function will notbe considered. Treating an AI as a black box thus disregardsfour of the six dimensions of fairness introduced in [14]. Thisleaves us with perceptual and motoric fairness as they dealwith the system’s interaction with the game. In fact, humanvs. human competitions also take this approach: we ignorehow contestants have prepared for the competition and theirIQ scores are not considered important. Only in few cases (e.g.weight in boxing or gender in physical sports) is it deemednecessary to impose further restrictions and limitations. In thecase of electronic sports (eSports), gender segregation is atopic of an ongoing debate, which to cover would exceedthe scope of this paper. However, to explicate the ’obviousdifferences’ that will be disregarded, a discussion of types ofpotential superintelligences by Nick Bostrom is useful. Whilehis position on the emergence of superintelligences and itsconsequences are arguable, we deem his threefold distinctionas useful here, to describe and understand ways in whichmachines are simply and obviously different from humans.In the book Superintelligence [8], Bostrom distinguishedthree different forms of possible superintelligences [8, p.63]that we will use to explicate the aforementioned differences.Speed intelligence describes a superintelligence that “[...] cando all that a human intellect can do, but much faster” [8, p.64]. AI systems can usually speed up matches and play themmuch faster than a human player can. It is thus an obviousdifference that AIs can train faster than humans if the numberof games over time (not the learning outcome) is the measureof speed. Collective intelligence describes “a system composedof a large number of smaller intellects such that the system’soverall performance across many very general domains vastlyoutstrips that of any current cognitive system” [8, p. 65]. Inthe context of learning, many machine learning algorithmsfit this description, as neural networks usually are trained1http://www.taipeitimes.com/News/front/archives/2016/03/14/2003641528in several instances in parallel to ultimately combine thegathered information into one system – an option that a humanplayer does not have. In fact, AlphaGo, OpenAI Five, andAlphaStar all relied heavily on these advantages. AlphaGoplayed 1,280,000 games against itself using 50 GPUs [41],OpenAI Five played around 180 years of real-time Dota perday on 256 GPUs and 128,000 CPU cores [36], and forAlphaStar, a league of agents each agent played up to 200years of real-time StarCraft, each using 16 TPUs [48].Finally, Bostrom describes quality superintelligence as onesystem that is “[...] at least as fast as a human mind and vastlyqualitatively smarter” [8, p. 68]. Simply put, this resembles thedifference between two humans with different IQs; or moretheoretically, the encounter with an alien race that “thinks ona different level”, incomprehensible to us. This quality superintelligence is one potential interpretation of the outcomes ofa fair competition between AIs and humans.Especially when it comes to learning and training, the AIshave advantages over humans in what Bostrom called speedand collective intelligence. During training, developers canspeed up games to a degree which exceed human capabilities.In addition to that, one version of a system can play hundredsof games simultaneously and gather the gained informationafterward. This is a strength of artificial intelligence that weshould embrace and not handicap. This factor is thus ignoredby our black box approach.As discussed here, AI systems and humans are differentin many regards, and we agree with Canaan et al.’s [14]conclusion that a final, holistic comparison of the two isnonsensical. However, in the current paper, we want to dis-cuss what happens if we exclude the obvious differences (aspartially done in human vs. human competitions as well).Our interest lies in an evaluation of whether, or to whatextent, the competitions between AI systems and humans wereactually carried out on equal grounds. In other words, weexclude the characteristics of the participants, to evaluate thecharacteristics of the competition.To be able to discuss an AI vs. human competition withingames, we need a prototypical example of a human vs.human competition and a rudimentary distinction of how suchcompetitions regulate game extrinsic and intrinsic factors.The purpose of this is to establish certain terms that enablean in-depth discussion and comparison of AI vs. humancompetitions and their idiosyncratic structures.IV. A PROTOTYPICAL HUMAN COMPETITIONFor the following discussion, some definitions of terms arenecessary. First of all, the questions what a game is and whatshould be considered parts of games have been considered inmany publications [5, 6, 13, 25, 43, 44] as well as some heateddebates [1, 21, 26, 31]. The authors accept that ultimatelydefining games might be an impossible task that bears nor-mative and discriminatory potential. However, for the currentpurpose of structuring our argument and observations, we willdevelop a makeshift model of game intrinsic and extrinsic    Fig. 1: A prototypical human vs. human competition consisting of one or moreseries between two teams or individuals. Each series consists of multiple gameinstances of the same orthogame.factors. This model will be based on previous research onthe ontology of games [3], and metagames [15, 18].Aarseth and Calleja’s cybermedia model [3] constitutes adescriptive model that covers games but intentionally alsoother phenomena. They describe games as a player’s perspec-tive on a cybermedia object, which consists of a materiality, asign system, and a mechanical system. Especially in the caseof the mechanical system, it is possible that one cybermediaobject contains several systems. Their example is World ofWarcraft (Blizzard Entertainment, 2004), which contains mul-tiple mechanical systems, such as questing, raiding, PvP arena,and PvP battlegrounds.To avoid the confusion that the term game brings with it andto avoid the processual perspective on games that Aarseth andCalleja take2, we will use Carter et al.’s term “orthogame” [15]instead of Aarseth and Calleja’s “cybermedia objects” [3]. Theorthogame describes “[...] what players collectively considerto be the ’right and correct game’” [15]. We understandthe orthogame as the digital artifact installed on a computeror physical artifact as used for play (including its rules).This explicitly excludes the player from the object itself. Itis important to note that especially digital orthogames havevarious different ’starting configurations’, i.e. maps or chosenraces. These starting configurations determine individual gameinstances: subsets of the orthogame with one particular con-figuration. A series occurs between two teams or individualsacross multiple instances of the orthogame, i.e. a best-of-fiveseries. Finally, all of these concepts are encompassed by an“added metagame” [18, p. 5]. Added metagames are structuresregulating leagues, ladders, tournaments, competitions, etc.We will now put these terms into work in an exampleof a StarCraft II instance. It must be noted that it is aprototypical example and a more detailed model could bedrawn (as discussed at the end of this section). However, thedeveloped terms will still be applicable in those cases, eventhough the structure could be expanded. Figure 1 illustratesthis prototypical version of a human vs. human competition.In our example, two players are competing for the worldchampionship in StarCraft II. Over the course of the lastmonths, they both proceeded through an added metagame:a ladder, as well as a KO system in the finals, which are2One of the unsolved problems regarding games is the question whetherthey are objects of processes [2] The ontological commitment of a processperspective onto games is that the player constitutes an element of the gameitself; a perspective that the authors do not share in the current endeavor.held at a physical location. Now they face off in the grandfinal. The grand final is constituted by a best-of-five series.This means that the players face each other at least threetimes, playing the same orthogame (StarCraft II), but differentinstances of it. These instances (circle, triangle, and diamondshapes) are usually determined by one of two processes. Onthe one hand, there exists a “material metagame” [18, p.5], which encompasses drafting armies or heroes in somegames. In our StarCraft II example, there is a map selectionprocedure before a series, where players can veto maps thatwill be removed from the map pool. On the other hand,the particular configurations can be regulated by the addedmetagame beforehand, such as 1 vs. 1 competitions in Dota 2.These limit the orthogame to a particular player composition(two players) and a spatial layout (middle lane only). Thus, theexample actually constitutes a combination of both processes,through the limitation of a map pool in StarCraft II (addedmetagame restriction), and the subsequent selection of mapsfrom the pool by the players (material metagame process).We can expand the model and include the whole addedmetagame of the world championships by adding more seriesto Figure 1, as indicated by the additional ’series’ frame.These additional series, in turn, consist of (potentially) dif-ferently configured instances of the orthogame, played bydifferent players. Another possible constellation is an addedmetagame between the same players but within differentorthogames. However, we further argue that the function ofadded metagames is to regulate game extrinsic and intrinsicfactors within the added metagame, with the purpose ofbalancing and fairness. We will elaborate on these conceptsin the following sections.V. GAME EXTRINSIC AND INTRINSIC FACTORSTo reiterate the previous section, we can split a competitioninto three general areas: the added metagame, series within theadded metagame and instances of an orthogame played withinthe series. We argue that the function of the added metagame isto regulate both, the format of the series (extrinsic factors), aswell as the particular configurations of the orthogame (intrinsicfactors). The extrinsic and intrinsic factors of regulation willbe exemplified in the following sections.A. Fairness: Game Extrinsic FactorsAdded metagames generally regulate two game extrinsicareas in a competition: the structure or format of the com-petition and the competitors participating in it. An exampleof an added metagame are ladder system, as implementedin many contemporary online multiplayer games, such asLeague of Legends (Riot Games, 2009), Heroes of the Storm(Blizzard Entertainment, 2015) or StarCraft II. In the worldchampionship of soccer, a mini-league system, in which teamsplay in discrete groups, and a KO round, in which teamseliminate each other in best-of-one series, are combined. Othersports require the finalists of a competition to face each otherin several instances, for example, a best-of-five-series. The aimof this is to minimize arbitrary factors that could predeterminea victory as much as possible. In Chess, for example, the whiteplayer starts the game with a slight advantage. To balancewhite’s advantage, an extrinsic system (the series) is employed,which guarantees that both players will start with white.Another potential reason for the implementation of series isto negate factors such as day to day performance, weather, orhome turf advantage. In other words, these regulations aim tomake sure the best player wins and not a player that was luckyto gain a game extrinsic advantage.Yet in other sports, as well as eSports, it is common todivide participants into groups, depending on their physicalattributes. In boxing, for example, the added metagame dividesparticipants into groups by weight. In nearly every sport it isalso common to distinguish between youth- and adult-leagues.The intention here is not only to make the individual instancesmore interesting for the spectators but also to prevent injuries.A similar, but a more controversial division is related to theparticipants’ gender (e.g. [46]). This division is also employedin eSports, where its usefulness is a topic of ongoing debate.We will discuss the necessity of limiting ’physical’ capabilitiesof participants in AI vs. human competitions in the section”Critique of AI Achievements in Games.”B. Fairness: Game Intrinsic FactorsAs discussed earlier, contemporary digital game artifactscontain multiple mechanical systems. Aarseth and Calleja[3] mention raiding, questing and player vs. player (PvP)as examples in World of Warcraft. Different models identifydifferent (types of) elements of games for different purposes(e.g. [4, 7, 49]). Elverdam and Aarseth [19], for example,identify dimensions of games for the purpose of classifica-tion. These include the players’ relation, virtual and physicalspace, struggle (including the game’s goals), and more. Theindividual categories are less important than the observationthat contemporary video game can rarely be described in onlyone manner. While it is easy to argue that Chess particularlyrequires two opposed players, it is impossible to make thesame general statement about the digital artifact StarCraft II,which contains the potential player compositions of singleplayer, two player, multiplayer, and multiteam (see[19]).Another game intrinsic factor is different maps (e.g. inStarCraft II), which would be comparable to different boardsin Chess or Go. These maps influence the individual strategies,but also the available player numbers. While StarCraft IIalso allows playing three different races, it is common fora player to stick to one race throughout their ’career’. InDota 2, however, players need to be able to play a rangeof different heroes, as each series includes the drafting andbanning of heroes from a pool of (as of writing) around115 heroes. Selecting heroes can be considered a “materialmetagame” [18] to determine the configuration of the gameinstance. Added metagames can also regulate clear orthogameintrinsic content, such as available items inside of Dota 2. Insome cases, items that heroes use to increase their strengthof abilities were banned by leagues, as they were consideredoverpowered or were simply bugged.When comparing AI and human intelligence in games, thehere discussed multifacetedness of games must be consideredand both participants’ capabilities of engaging with the artifactas a whole needs to be examined. This insight leads us to thenext section, in which we examine particular games and com-petitions within them, using the here developed distinction ofgame extrinsic and intrinsic factors, as well as the prototypicalhuman vs. human added metagame.VI. CRITIQUE OF AI ACHIEVEMENTS IN GAMESIn this section we will review a few selected milestoneachievements of AI in games, focusing on particular systemsand their comparisons to human professionals. We will discussthe fairness of these competitions and whether it is validto claim that the AI systems are super-human using theblack box approach and the coherence of game extrinsic andintrinsic factors. The goal is to identify to what extent thepresented milestones adhere to or deviate from the prototypicalhuman vs. human added metagame. Any deviations, we argue,indicates an unfairness in the competition (for either side) anda claim of superiority must be treated carefully in these cases.A. AlphaGoAlphaGo, developed by DeepMind, is the first Go-playingsystem to win against professional Go players without handi-cap on a full-sized board. The first of these games were againstthe 2-dan European Go champion Fan Hui in 2015, whereAlphaGo won 5-0 [41]. The 9-dan 18-time world championLee Sedol was the next target for AlphaGo, and a five-gamematch was scheduled in 2016 with a one million dollar prizeand following the official rules: Chinese ruleset, a 7.5-pointkomi, and two-hour time limit for each player. Prior to thematch, the Fan Hui games were published allowing Lee Sedolto prepare against AlphaGo.3 AlphaGo won 4-1 with a lossin game four, showing a weakness in the system that mightbe exploitable. A new version of AlphaGo called ’Master’was anonymously registered to the ’Tygem’ and ’FoxGo’ Goservers, playing a total of 50 game instances, with a shortertime limit than usual game instances, against professionaland top players, winning all of them4. AlphaGo’s last gameinstances were at the Future of Go Summit, where AlphaGowon aginst several top players including the highest rankedplayer Ke Jie and a team of five human players, without losinga single game. After this event, AlphaGo was retired5.We argue that AlphaGo ultimately competed fairly in non-restricted matches against numerous top professional playersboth online and in settings similar to human competitions bothin terms of game intrinsic and extrinsic factors.3https://web.archive.org/web/20160214135238/https://gogameguru.com/an-younggils-pro-go-videos-alphago-vs-fan-hui-game-2/4https://www.nature.com/news/google-reveals-secret-test-of-ai-bot-to-beat-top-go-players-1.212535https://deepmind.com/research/alphago/alphago-vs-alphago-self-play-games/B. OpenAI FiveIn 2016, the company OpenAI decided to pursue the chal-lenge of beating human professionals in the multiplayer onlinebattle arena (MOBA) game Dota 2 [35]. Dota 2 is a fast-paced real-time game, has partially observable states, high-dimensional observation and action spaces, and has long timehorizons [36]. Normally in Dota 2, two teams of five playersplay against each other while there also is a one vs. one (1v1)variant. In 2017, OpenAI developed a bot capable of playingthe 1v1 version that beat the former professional player ’Blitz’3-0, the professional players ’Pajkatt’ 2-1, ’CC&C’ 3-0, thetop 1v1 player ’Sumail’ 6-0, and ’Dendi’ 2-0. The standard(or most popular) variant of Dota 2 is played in teams offive players. However, because there exist serious 1v1 Dotacompetitions, the added metagame does adhere to at least anexisting version of the added human vs. human metagame,mimicking a termination tournament with five players. Thebot was updated by the developers between each series [34];possibly bugs were fixed and control parameters were tuned.’Sumail’ also played against the previous version of the botand won this time 2-1.It can be argued that altering the bot in-between gameinstances is a violation of the black-box approach, as iteffectively becomes a new system. However, human playersusually have the ability to discuss strategies with a coach in-between game instances, so perhaps an AI should also beallowed to be influenced by a ’coach’. In any case, it dependson whether the developers that are modifying it, are consideredpart of the entity that is competing, which we would argue,they are not. Human modification of the AI system should thusnot take place within a series of individual game instances, oreven whole added metagames. Because both positions (for andagainst human intervention) are arguable, it appears necessaryto develop an explicit regulation in this area for future humanvs. AI competitions.These series were played under standard 1v1 tournamentrules. The bot had direct access to the features of the gameartifact from the API, instead of being presented to the visualrepresentation of the game artifact. The bot could only accessthe same information that would have been available to ahuman player but it was structured differently. For instance,humans have to infer the position of heroes and thus estimatethe distances between units, which are important for rangedattacks, while the bot can access the exact positions and thuscalculate the exact distances, instantly. This arguably goesagainst perceptual fairness because the input space should bethe same. Here, the input space includes the same informationfor both the AI system and the human, but by perceivingthe game state in a different way than humans, the AIsystem might have an advantage or disadvantage. The bot hadaccess to the same actions as humans players and they wereperformed at similar frequencies but with a quicker reactiontime of 80ms [33]. The reaction time was, however, reducedin later competitions.After the 1v1 win, OpenAI let the bot play thousands ofgames against various players, where several exploits werefound to overcome the bot [34]. This setup mimics a humanladder where we would expect experienced human playersnot to have trivial exploits. The AI, however, would quicklydescend the ladder due to these discovered exploits.In 2018, a newer version of the bot named OpenAI Fivewas able to beat a team consisting of 99.95th percentile players’Blitz’, ’Cap’, ’Fogged’, ’Merlini’, and ’MoonMeander’ (someare former professionals) in a restricted version of the 5v5game [33]. This series was named the OpenAI Five Bench-mark. Some of the restrictions include a fixed hero pool of 18heroes (instead of 117) resulting in 11 million possible gameinstances, no summons/illusions, and no Scan. The reactiontime was increased from 80ms to 200ms in an attempt to matchthat of humans. The bot won the first two game instanceswhere it did the hero drafting itself and lost the third gamewhere the audience did the draft. The restricted hero pool is asignificant limitation of the game intrinsic factors, effectivelyreducing the possible game instances to a much smaller subsetthan are usual in human vs. human competitions.’OpenAI Five’ later played two show series against the twoteams of top professionals ’paiN Gaming’ and ’Big God’ andlost.6 In 2019, ’OpenAI Five’ won a best-of-three series 2-0(the OpenAI Five Finals) against the Dota team OG, whichconsists of top-professional players. In this series, the heropool was further restricted to just 17 heroes [37]. Playingagainst just one team mimics the final series of a tournamentbut not an entire tournament, ignoring the game extrinsicfactors of complete added metagame structures.After the win against OG, the OpenAI Five Arena allowedanyone to play against OpenAI Five. These games had thesame restrictions as earlier. OpenAI Five won 7,215 gamesand lost 42 (99.4% win rate) against a total of 15,019 play-ers7. One team, mainly consisting of the players ’ainodehna’,’backtoashes’, ’CANYGODXXX’, ’.tv/juniorclanwar’, and’gazeezy’, was able to reach a ten game winning streak.The OpenAI Arena is basically an extensive ladder setupcohering to game extrinsic factors. A ladder challenges thebot to be robust to many different strategies and playing styles.The fact that it won almost every game but one team was ableto beat it repeatedly is interesting. If this result was due toa trivial exploit, then most teams, knowing about the exploit,would be able to beat it; for a human opponent this would notbe the case. However, the bot won 99.4% of the games in anextrinsically fair setup, which we would not expect even fromhuman world champions. The criteria for being the best Dota2 team on a ladder is not to have a 100% win rate, and wethus should not impose that expectation on the bot.C. AlphaStarStarCraft was, along with other real-time strategy games,proposed as a new challenge for AI in 2003 [12] with arenewed interest by research teams at Facebook in 2016 [45]6https://liquipedia.net/dota2/The International/2018/OpenAI Showmatches7https://arena.openai.com/#/resultsand DeepMind in 2017 [47]. In 2019, DeepMind played theirbot AlphaStar against the two top professional players Dario’TLO’ Wu¨nsch and Grzegorz ’MaNa’ Komincz and won bothseries 5-0 [48]. All games in these two series were Protoss vs.Protoss on the standard medium-sized map CatalystLE.It was claimed that these series adhered to professionalmatch conditions [48], while this is in fact not the case.Tournaments never use just a single map for a whole series butinstead a predefined map pool, thus the competition did notadhere to the game extrinsic factors. Additionally, in profes-sional tournaments, players face multiple players controllingany of the three races, and not just Protoss.After the match ’MaNa’ also mentioned that he made a fewmistakes because they played an earlier version of StarCraftII than the one he was used to; he also did not warm up,which he would usually do [27, 19:50]. The actions per minute(APM) count of AlphaStar was around 280, which is lowerthan professional players, and with a reaction time of 350mson average. AlphaStar had only access to visual informationfrom the game, similarly, but not exactly identical, to thescreen pixels presented to human players [47]. This is arguablya violation of the game intrinsic factors, similarly to OpenAIFive, since AlphaStar has a different input space than humanplayers have. It is, however, a weak violation since AlphaStar’srepresentation of the game state has the same information,while it is just structured differently.Importantly, AlphaStar was not restricted to the limited viewof the camera, which a human player has to control manually.As DeepMind puts it: “it could observe the attributes of its ownand its opponents visible units on the map directly, withouthaving to move the camera - effectively playing with a zoomedout view of the game.” [48]. ’MaNa’ expressed this as being“very unfair” [27, 1:17:15]. This is, however, a clear advantageof AlphaStar on both the levels of perceptional capabilitiesand motoric necessities. Furthermore, it could even be arguedthat this alters the “perspective dimension” [19] of StarCraft IIfrom vagrant to omnipresent, which is arguably an alterationof the orthogame itself.Later, the professional player ’MaNa’ played against aprototype of AlphaStar that controlled the camera as well,in a single-game series and won. He found a weakness inAlphaStar during a Warp Prism harassment with Immortals,continuously warping in units, picking them up, and escaping.Whether this weakness was due to the camera control or ifit was a critical exploit of AlphaStar is not known. It may,however, seem that he won the last game because it was playedat a later date than the others and he had time to prepareagainst its style. Specifically, he said that “We (‘TLO’ and‘MaNa’) noticed that the agent sticks to the basic units a lot.It’s very confident in its micro, and it should be, it’s greatmicro, but it doesn’t really transition out of it.” [27, 1:19:15].’MaNa’ said his new plan was to “... defeat AlphaStar withsimply better unit composition rather than unit control” [27,1:20:20]. We notice here, that in the two 5-0 wins against’TLO’ and ’MaNa’, they did not have a chance to scrutinizeany recorded games played by AlphaStar, which professional Fig. 2: A typical AI vs. human competition consisting of one series betweentwo teams or individuals. The series often consist of identical game instances,or a limited set of game instances, of the same orthogame.player typically can do before important human vs. humanseries. In contrast, the developers of AlphaStar picked andknew the opponent in advance. ’MaNa’ said, commenting onhis first series against AlphaStar: “I was completely in the dark... I don’t know what to expect. If you are a StarCraft playeryou are familiar with people you are playing on the ladder ...you know what their styles are.” [27, 18:40]. Compared to aprototypical human vs. human competition, this is an unusualsetup of the competition, as professional players know eachothers’ play styles before playing. To observe the problemfrom another angle: human vs. human added metagames neverkeep their participants secret from their participants, as was thecase in ’MaNa’ vs. AlphaStar.VII. CONCLUSIONSWe introduced a black box approach that can be used whendesigning and evaluating human vs. AI game competitionsas well as the notions of game extrinsic and intrinsic fac-tors. We applied these to discuss the fairness of recent AIachievements of AlphaGo, OpenAI Five, and AlphaStar. Itappears that the added metagame’s role in an AI vs. humancompetition has a different focus than in the human vs. humancompetitions. The added metagame in a human vs. humancompetition regulates mostly the bigger structure of extrinsicfactors, such as a sequence of series, number of instances ina series and groupings due to a physical difference betweenthe competitors. The added metagame’s role in the AI vs.human competitions, however, is focused on the regulation ofgame intrinsic factors. This means, in competitions betweenAIs and humans in digital games, the orthogame is so faralways limited to either one particular configuration or a verysmall amount of possible configurations, compared to humanvs. human competitions. A visualization of this setup is shownin Figure 2. OpenAI Five, for example, is capable of playingonly 17 out of (approximately) 115 heroes of Dota. Thus, theorthogame (Dota 2) in the competition between OpenAI Fiveand humans had to be limited to these 17 heroes.OpenAI Five had direct access to game state variableswhile humans must infer positions, attack ranges, and healthfrom a raw visual representation, which ultimately leads to’educated guesses’ more than factual knowledge. AlphaStar,in fact, used a visual representation but a different one thanwhat is presented to humans. Furthermore, OpenAI Five andAlphaStar are incapable of ’misclicking’, which is the actof giving a command unintended by the player. These twofactors constitute imbalances in the perceptual and motoriccapabilities of the competitors, which must be accountedfor in the future. To have a fair competition, the AIs mustbe handicapped through their interaction with the game toimitate how humans are interacting with it, i.e. if humanshave imprecise and slow means of interacting with the gameit should be the same for the AI system. It can be argued thatthis is something that naturally occurs in human vs. humancompetition, as every human participant is implicitly restrictedto human capabilities. Thus, the addition of a handicap tothe AI simply constitutes an explicit correction through gameextrinsic factors.We thus conclude that we are not done with games. Thegames proposed as ultimate AI challenges, Dota 2 and Star-Craft II, are not yet mastered by AI. As we identified, sofar AI vs. human competitions are different in that (1) theAIs do not compete in a tournament structure, but are simplymatched with the best (available) human player and (2) theylimit the orthogame in particular ways, such as range ofmaps, heroes or races. To be able to claim that ’We aredone with games’ the AI has to engage in a fair competitionwith humans that is constituted by the same external factors,such as several matches against the same opponent, as wellas different opponents. Additionally, it should not limit gameinternal factors, such as only allowing certain playable heroesor maps. Only then, the claim that AI is superior to humansin games is justified, given that StarCraft II and DotA 2 areand remain the most complex games to beat.We are, however, not neglecting the significant progressmade towards achieving the goals, as no system before OpenAIFive and AlphaStar could win against professionals in anycompetition in these games. The fact that game extrinsic fac-tors have been largely ignored in human vs. AI competitions,might indicate where to focus our future efforts. Adapting to avariety of possibly unseen game instances as well as differentplayers are important open challenges; they deal with problemsof generality [17, 23, 52], transfer learning [39], and life-longlearning [38], which are all active areas of focus in machinelearning research [24].An ultimate goal that would demonstrate that an AI systemcan fully master a game, beyond the extrinsic factors of humanvs. human competitions, would be to allow anyone to playagainst it over a long period of time. This setup would besimilar to OpenAI Arena, without restricting any intrinsicgame factors. This goal was to some degree achieved byAlphaGo when it played on the Tygem and FoxGo Go serverswithout losing, and without restricting the intrinsic factors.VIII. ACKNOWLEDGEMENTSThis research has received funding from the EuropeanResearch Council (ERC) under the European Unions Horizon2020 research and innovation programme (Grant AgreementNo [695528] MSG: Making Sense of Games).REFERENCES[1] E. Aarseth. Ludology. In M. J. P. Wolf, editor, The RoutledgeCompanion to Video Game Studies. Routledge, 2014.[2] E. Aarseth. Ontology. In M. J. P. Wolf, editor, The Routledgecompanion to video game studies, pages 510–518. Routledge,2014.[3] E. Aarseth and G. Calleja. The word game: The ontologyof an undefinable object. In Foundations of Digital GamesConference, 2015.[4] E. Aarseth, S. M. Smedstad, and L. Sunnana˚. A multidimen-sional typology of games. In DiGRA Conference, 2003.[5] J. Arjoranta. Game definitions: A wittgensteinian approach.Game Studies: the international journal of computer gameresearch, 14, 2014.[6] E. M. Avedon and B. Sutton-Smith. The study of games. JohnWiley & Sons, 1971.[7] S. Bjork and J. Holopainen. Patterns in game design. CharlesRiver Media, 2004.[8] N. Bostrom. Superintelligence: paths, dangers, strategies.Oxford University Press, 2014.[9] B. Bouzy and T. Cazenave. Computer go: an ai oriented survey.Artificial Intelligence, 132(1):39–103, 2001.[10] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schul-man, J. Tang, and W. Zaremba. Openai gym. arXiv preprintarXiv:1606.01540, 2016.[11] J. Burmeister and J. Wiles. The challenge of go as a domain forai research: a comparison between go and chess. In Proceedingsof Third Australian and New Zealand Conference on IntelligentInformation Systems. ANZIIS-95, pages 181–186. IEEE, 1995.[12] M. Buro. Real-time strategy games: A new ai research chal-lenge. In IJCAI, volume 2003, pages 1534–1535, 2003.[13] R. Caillois. Man, play, and games. University of Illinois Press,2001.[14] R. Canaan, C. Salge, J. Togelius, and A. Nealen. Levelingthe playing field-fairness in ai versus human game benchmarks.arXiv preprint arXiv:1903.07008, 2019.[15] M. Carter, M. Gibbs, and M. Harrop. Metagames, paragamesand orthogames: A new vocabulary. In Proceedings of theinternational conference on the foundations of digital games,pages 11–17. ACM, 2012.[16] M. Cˇerticky` and D. Churchill. The current state of starcraft aicompetitions and bots. In Thirteenth Artificial Intelligence andInteractive Digital Entertainment Conference, 2017.[17] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman.Quantifying generalization in reinforcement learning. arXivpreprint arXiv:1812.02341, 2018.[18] M. S. Debus. Metagames: on the ontology of games outside ofgames. In Proceedings of the 12th International Conference onthe Foundations of Digital Games, page 18. ACM, 2017.[19] C. Elverdam and E. Aarseth. Game classification and gamedesign: Construction through critical analysis. Games andCulture, 2(1):3–22, 2007.[20] J. M. F. Fernandez and T. Mahlmann. The dota 2 bot compe-tition. IEEE Transactions on Games, 2018.[21] G. Frasca. Ludologists love stories, too: notes from a debatethat never took place. In DiGRA conference, 2003.[22] S. Gelly, L. Kocsis, M. Schoenauer, M. Sebag, D. Silver,C. Szepesva´ri, and O. Teytaud. The grand challenge of computergo: Monte carlo tree search and extensions. Communicationsof the ACM, 55(3):106–113, 2012.[23] N. Justesen, R. R. Torrado, P. Bontrager, A. Khalifa, J. Togelius,and S. Risi. Illuminating generalization in deep reinforcementlearning through procedural level generation. arXiv preprintarXiv:1806.10729, 2018.[24] N. Justesen, P. Bontrager, J. Togelius, and S. Risi. Deep learningfor video game playing. IEEE Transactions on Games, 2019.[25] J. Juul. Half-real: Video games between real rules and fictionalworlds. MIT press, 2011.[26] B. Keogh. Across worlds and bodies: Criticism in the age ofvideo games. Journal of Games Criticism, 1(1):1–26, 2014.[27] G. M. Komincz. Deepmind starcraft 2 demonstration - mana’spersonal experience. URL https://www.youtube.com/watch?v=zgIFoepzhIo.[28] K. L. Kroeker. A new benchmark for artificial intelligence.Commun. ACM, 54(8):13–15, 2011.[29] G. Marcus. Innateness, alphazero, and artificial intelligence.arXiv preprint arXiv:1801.05667, 2018.[30] M. Mu¨ller. Computer go. Artificial Intelligence, 134(1-2):145–179, 2002.[31] J. H. Murray. The last word on ludology v narratology in gamestudies. In International DiGRA Conference, 2005.[32] S. Ontano´n, G. Synnaeve, A. Uriarte, F. Richoux, D. Churchill,and M. Preuss. A survey of real-time strategy game ai researchand competition in starcraft. IEEE Transactions on Computa-tional Intelligence and AI in games, 5(4):293–311, 2013.[33] OpenAI. Openai five benchmark. https://openai.com/blog/openai-five-benchmark/, .[34] OpenAI. More on dota 2. https://openai.com/blog/more-on-dota-2/, .[35] OpenAI. Openai five. https://openai.com/five/, .[36] OpenAI. Openai five. https://blog.openai.com/openai-five/, .[37] OpenAI. How to train your openai five. https://openai.com/blog/how-to-train-your-openai-five/, .[38] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter.Continual lifelong learning with neural networks: A review.Neural Networks, 2019.[39] E. Parisotto, J. L. Ba, and R. Salakhutdinov. Actor-mimic: Deepmultitask and transfer reinforcement learning. arXiv preprintarXiv:1511.06342, 2015.[40] C. E. Shannon. Programming a computer for playing chess. InComputer chess compendium, pages 2–13. Springer, 1988.[41] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. VanDen Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershel-vam, M. Lanctot, et al. Mastering the game of go with deepneural networks and tree search. nature, 529(7587):484, 2016.[42] H. Simon and W. Chase. Skill in chess. In Computer chesscompendium, pages 175–188. Springer, 1988.[43] J. Stenros. The game definition game: A review. Games andCulture, 12(6):499–520, 2017.[44] B. Suits. The Grasshopper-: Games, Life and Utopia. Broad-view Press, 2014.[45] G. Synnaeve, N. Nardelli, A. Auvolat, S. Chintala, T. Lacroix,Z. Lin, F. Richoux, and N. Usunier. Torchcraft: a library formachine learning research on real-time strategy games. arXivpreprint arXiv:1611.00625, 2016.[46] S. Teetzel. On transgendered athletes, fairness and doping: Aninternational challenge. Sport in Society, 9(2):227–251, 2006.[47] O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhn-evets, M. Yeo, A. Makhzani, H. Ku¨ttler, J. Agapiou, J. Schrit-twieser, et al. Starcraft ii: A new challenge for reinforcementlearning. arXiv preprint arXiv:1708.04782, 2017.[48] O. e. a. Vinyals. Alphastar: Mastering the real-time strategygame starcraft ii. URL https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/.[49] M. J. Wolf. Inventing space: Toward a taxonomy of on-andoff-screen space in video games. Film Quarterly (ARCHIVE),51(1):11, 1997.[50] G. N. Yannakakis. Game ai revisited. In Proceedings of the9th conference on Computing Frontiers, pages 285–292. ACM,2012.[51] G. N. Yannakakis and J. Togelius. Artificial intelligence andgames, volume 2. Springer, 2018.[52] C. Zhang, O. Vinyals, R. Munos, and S. Bengio. A studyon overfitting in deep reinforcement learning. arXiv preprintarXiv:1804.06893, 2018.",
    "Link": "https://core.ac.uk/download/286340898.pdf"
}