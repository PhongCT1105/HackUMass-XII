{
    "Title": "Talking About Large Language Models",
    "Authors": "Shanahan, Murray",
    "Year": "No year available",
    "Abstract": "Thanks to rapid progress in artificial intelligence, we have entered an era\nwhen technology and philosophy intersect in interesting ways. Sitting squarely\nat the centre of this intersection are large language models (LLMs). The more\nadept LLMs become at mimicking human language, the more vulnerable we become to\nanthropomorphism, to seeing the systems in which they are embedded as more\nhuman-like than they really are. This trend is amplified by the natural\ntendency to use philosophically loaded terms, such as \"knows\", \"believes\", and\n\"thinks\", when describing these systems. To mitigate this trend, this paper\nadvocates the practice of repeatedly stepping back to remind ourselves of how\nLLMs, and the systems of which they form a part, actually work. The hope is\nthat increased scientific precision will encourage more philosophical nuance in\nthe discourse around artificial intelligence, both within the field and in the\npublic sphere",
    "Keywords": "No keywords available",
    "Publisher": "",
    "Publication Date": "No publication date available",
    "Journal": "No journal available",
    "Citation Count": 0,
    "Full Text": "arXiv:2212.03551v2  [cs.CL]  11 Dec 2022Talking About Large Language ModelsMurray ShanahanImperial College Londonm.shanahan@imperial.ac.ukDecember 2022AbstractThanks to rapid progress in artificial intelligence,we have entered an era when technology andphilosophy intersect in interesting ways. Sit-ting squarely at the centre of this intersectionare large language models (LLMs). The moreadept LLMs become at mimicking human lan-guage, the more vulnerable we become to an-thropomorphism, to seeing the systems in whichthey are embedded as more human-like than theyreally are. This trend is amplified by the natu-ral tendency to use philosophically loaded terms,such as “knows”, “believes”, and “thinks”, whendescribing these systems. To mitigate this trend,this paper advocates the practice of repeatedlystepping back to remind ourselves of how LLMs,and the systems of which they form a part, ac-tually work. The hope is that increased scien-tific precision will encourage more philosophicalnuance in the discourse around artificial intelli-gence, both within the field and in the publicsphere.1 IntroductionThe advent of large language models (LLMs)such as Bert (Devlin et al., 2018) and GPT-2 (Radford et al., 2019) was a game-changerfor artificial intelligence. Based on transformerarchitectures (Vaswani et al., 2017), compris-ing hundreds of billions of parameters, andtrained on hundreds of terabytes of textual data,their contemporary successors such as GPT-3(Brown et al., 2020), Gopher (Rae et al., 2021),and PaLM (Chowdhery et al., 2022) have givennew meaning to the phrase “unreasonable effec-tiveness of data” (Halevy et al., 2009).The effectiveness of these models is “unreason-able” (or, with the benefit of hindsight, some-what surprising) in three inter-related ways.First, the performance of LLMs on benchmarksscales with the size of the training set (and, toa lesser degree with model size). Second, thereare qualitative leaps in capability as the modelsscale. Third, a great many tasks that demand in-telligence in humans can be reduced to next tokenprediction with a sufficiently performant model.It is the last of these three surprises that is thefocus of the present paper.As we build systems whose capabilities moreand more resemble those of humans, despite thefact that those systems work in ways that arefundamentally different from the way humanswork, it becomes increasingly tempting to an-thropomorphise them. Humans have evolved toco-exist over many millions of years, and humanculture has evolved over thousands of years tofacilitate this co-existence, which ensures a de-gree of mutual understanding. But it is a seriousmistake to unreflectingly apply to AI systems thesame intuitions that we deploy in our dealingswith each other, especially when those systemsare so profoundly different from humans in theirunderlying operation.The AI systems we are building today haveconsiderable utility and enormous commercialpotential, which imposes on us a great respon-sibility. To ensure that we can make informeddecisions about the trustworthiness and safety ofthe AI systems we deploy, it is advisable to keepto the fore the way those systems actually work,and thereby to avoid imputing to them capaci-ties they lack, while making the best use of theremarkable capabilities they genuinely possess.2 What LLMs Really DoAs Wittgenstein reminds us, human language useis an aspect of human collective behaviour, andit only makes sense in the wider context of thehuman social activity of which it forms a part(Wittgenstein, 1953). A human infant is borninto a community of language users with which1it shares a world, and it acquires language byinteracting with this community and with theworld it shares with them. As adults (or indeedas children past a certain age), when we have acasual conversation, we are engaging in an activ-ity that is built upon this foundation. The sameis true when we make a speech or send an emailor deliver a lecture or write a paper. All of thislanguage-involving activity makes sense becausewe inhabit a world we share with other languageusers.A large language model is a very differ-ent sort of animal (Bender and Koller, 2020;Bender et al., 2021; Marcus and Davis, 2020).(Indeed, it is not an animal at all, which is verymuch to the point.) LLMs are generative math-ematical models of the statistical distributionof tokens in the vast public corpus of human-generated text, where the tokens in question in-clude words, parts of words, or individual char-acters including punctuation marks. They aregenerative because we can sample from them,which means we can ask them questions. Butthe questions are of the following very specifickind. “Here’s a fragment of text. Tell me howthis fragment might go on. According to yourmodel of the statistics of human language, whatwords are likely to come next?”1It is very important to bear in mind that thisis what large language models really do. Sup-pose we give an LLM the prompt “The first per-son to walk on the Moon was ”, and suppose itresponds with “Neil Armstrong”. What are wereally asking here? In an important sense, weare not really asking who was the first person towalk on the Moon. What we are really askingthe model is the following question: Given thestatistical distribution of words in the vast pub-lic corpus of (English) text, what words are mostlikely to follow the sequence “The first person towalk on the Moon was ”? A good reply to thisquestion is “Neil Armstrong”.Similarly, we might give an LLM the prompt“Twinkle twinkle ”, to which it will most likelyrespond “little star”. On one level, for sure, weare asking the model to remind us of the lyricsof a well-known nursery rhyme. But in an im-portant sense what we are really doing is ask-ing it the following question: Given the statis-1Even if an LLM is fine-tuned, for example using rein-forcement learning with human feedback (e.g. to filter outpotentially toxic language) (Glaese et al., 2022), the re-sult is still a model of the distribution of tokens in humanlanguage, albeit one that has been slightly perturbed.tical distribution of words in the public corpus,what words are most likely to follow the sequence“Twinkle twinkle ”? To which an accurate an-swer is “little star”.Here’s a third example. Suppose you are thedeveloper of an LLM and you prompt it withthe words “After the ring was destroyed, FrodoBaggins returned to ”, to which it responds “theShire”. What are you doing here? On onelevel, it seems fair to say, you might be testingthe model’s knowledge of the fictional world ofTolkien’s novels. But, in an important sense,the question you are really asking (as you pre-sumably know, because you are the developer) isthis: Given the statistical distribution of wordsin the public corpus, what words are most likelyto follow the sequence “After the ring was de-stroyed, Frodo Baggins returned to ”? To whichan appropriate response is “the Shire”.To the human user, each of these examplespresents a different sort of relationship to truth.In the case of Neil Armstrong, the ultimategrounds for the truth or otherwise of the LLMsanswer is the real world. The Moon is a real ob-ject and Neil Armstrong was a real person, andhis walking on the Moon is a fact about the phys-ical world. Frodo Baggins, on the other hand, isa fictional character, and the Shire is a fictionalplace. Frodo’s return to the Shire is a fact aboutan imaginary world, not a real one. As for the lit-tle star in the nursery rhyme, well that is barelyeven a fictional object, and the only fact at issueis the occurrence of the words “little star” in afamiliar English rhyme.These distinctions are invisible at the level ofwhat the LLM actually does, which is simply togenerate statistically likely sequences of words.However, when we evaluate the utility of themodel, these distinctions matter a great deal.There is no point in seeking Frodo’s (fictional)descendants in the (real) English county of Sur-rey. This is one reason why it’s a good idea forusers to repeatedly remind themselves of whatLLMs really do. It’s also a good idea for develop-ers to remind themselves of this, to avoid the mis-leading use of philosophically fraught words todescribe the capabilities of LLMs, words such as“belief”, “knowledge”, “understanding”, “self”,or even “consciousness”.23 LLMs and the Intentional StanceIt is perfectly natural to use anthropomorphiclanguage in everyday conversations about arte-facts, especially in the context of informationtechnology. We do it all the time. My watchdoesn’t realise we’re on daylight saving time. Myphone thinks we’re in the car park. The mailserver won’t talk to the network. And so on.These examples of what Dennett calls the inten-tional stance are harmless and useful forms ofshorthand for complex processes whose detailswe don’t know or care about.2 They are harm-less because no-one takes them seriously enoughto ask their watch to get it right next time, say, orto tell the mail server to try harder. Even with-out having read Dennett, everyone understandsthey are taking the intentional stance, that theseare just useful turns of phrase.The same consideration applies to LLMs, bothfor users and for developers. Insofar as everyoneimplicitly understands that these turns of phraseare just convenient shorthands, that they aretaking the intentional stance, it does no harm touse them. However, in the case of LLMs, such istheir power, things can get a little blurry. Whenan LLM can be made to improve its performanceon reasoning tasks simply by being told to “thinkstep by step” (Kojima et al., 2022) (to pick justone remarkable discovery), the temptation to seeit as having human-like characteristics is almostoverwhelming.To be clear, it is not the argument of this paperthat a system based on a large language modelcould never, in principle, warrant description interms of beliefs, intentions, reason, etc. Nor doesthe paper advocate any particular account of be-lief, of intention, or of any other philosophicallycontentious concept.3 Rather, the point is thatsuch systems are simultaneously so very differ-ent from humans in their construction, yet (oftenbut not always) so human-like in their behaviour,that we need to pay careful attention to how theywork before we speak of them in language sug-gestive of human capabilities and patterns of be-haviour.To sharpen the issue, let’s compare two very2“The intentional stance is the strategy of interpretingthe behavior of an entity ... by treating it as if it were arational agent ” (Dennett, 2009).3In particular, when I use the term “really”, as in thequestion ‘Does X “really” have Y?’, I am not assum-ing there is some metaphysical fact of the matter here.Rather, the question is whether, when more is revealedabout the nature of X, we still want to use the word Y.short conversations, one between Alice and Bob(both human), and a second between Aliceand BOT, a fictional question-answering systembased on a large language model. Suppose Al-ice asks Bob “What country is to the south ofRwanda?” and Bob replies “I think it’s Bu-rundi”. Shortly afterwards, because Bob is oftenwrong in such matters, Alice presents the samequestion to BOT, which (to her mild disappoint-ment) offers the same answer: “Burundi is to thesouth of Rwanda”. Alice might now reasonablyremark that both Bob and BOT knew that Bu-rundi was south of Rwanda. But what is reallygoing on here? Is the word “know” being usedin the same sense in the two cases?4 Humans and LLMs ComparedWhat is Bob, a representative human, doingwhen he correctly answers a straightforward fac-tual question in an everyday conversation? Tobegin with, Bob understands that the questioncomes from another person (Alice), that his an-swer will be heard by that person, and that it willhave an effect on what she believes. In fact, af-ter many years together, Bob knows a good dealelse about Alice that is relevant to such situa-tions: her background knowledge, her interests,her opinion of him, and so on. All of this framesthe communicative intent behind his reply, whichis to impart a certain fact to her, given his un-derstanding of what she wants to know.Moreover, when Bob announces that Burundiis to the south of Rwanda, he is doing so againstthe backdrop of various human capacities thatwe all take for granted when we engage in every-day commerce with each other. There is a wholebattery of techniques we can call upon to ascer-tain whether a sentence expresses a true propo-sition, depending on what sort of sentence it is.We can investigate the world directly, with ourown eyes and ears. We can consult Google orWikipedia, or even a book. We can ask some-one who is knowledgeable on the relevant sub-ject matter. We can try to think things through,rationally, by ourselves, but we can also arguethings out with our peers. All of this relies onthere being agreed criteria external to ourselvesagainst which what we say can be assessed.How about BOT? What is going on when alarge language model is used to answer such ques-tions? First, it’s worth noting that a bare-bones3LLM is, by itself, not a conversational agent.4For a start, the LLM will have to be embeddedin a larger system to manage the turn-taking inthe dialogue. But it will also need to be coaxedinto producing conversation-like behaviour.5 Re-call that an LLM simply generates sequences ofwords that are statistically likely follow-ons froma given prompt. But the sequence “What coun-try is to the south of Rwanda? Burundi is to thesouth of Rwanda”, with both sentences squashedtogether exactly like that, may not, in fact, bevery likely. A more likely pattern, given thatnumerous plays and film scripts feature in thepublic corpus, would be something like the fol-lowing.Fred: What country is south of Rwanda?Jane: Burundi is south of Rwanda.Of course, those exact words may not appear,but their likelihood, in the statistical sense, willbe high. In short, BOT will be much betterat generating appropriate responses if they con-form to this pattern rather than to the patternof actual human conversation. Fortunately, theuser (Alice) doesn’t have to know anything aboutthis. In the background, the LLM is invisiblyprompted with a prefix along the following lines.This is a conversation betweenUser, a human, and BOT, a clever andknowledgeable AI agent.User: What is 2+2?BOT: The answer is 4.User: Where was Albert Einstein born?BOT: He was born in Germany.Alice’s query, in the following form, is ap-pended to this prefix.User: What country is south of Rwanda?BOT:This yields the full prompt to be submittedto the LLM, which will hopefully predict a con-tinuation along the lines we are looking for, i.e.“Burundi is south of Rwanda”.Dialogue is just one application of LLMs thatcan be facilitated by the judicious use of promptprefixes. In a similar way, LLMs can be adaptedto perform numerous tasks without further train-ing (Brown et al., 2020). This has led to a wholenew category of AI research, namely prompt en-gineering, which will remain relevant until wehave better models of the relationship between4Strictly speaking, the large language model itself com-prises just the model architecture and the trained param-eters.5See Thoppilan et al. (2022) for an example of such asystem, as well as a useful survey of related dialogue work.what we say and what we want.5 Do LLMs Really Know Anything?Turning an LLM into a question-answering sys-tem by a) embedding it in a larger system, andb) using prompt engineering to elicit the requiredbehaviour exemplifies a pattern found in muchcontemporary work. In a similar fashion, LLMscan be used not only for question-answering,but also to summarise news articles, to generatescreenplays, to solve logic puzzles, and to trans-late between languages, among other things.There are two important takeaways here. First,the basic function of a large language model,namely to generate statistically likely continua-tions of word sequences, is extraordinarily versa-tile. Second, notwithstanding this versatility, atthe heart of every such application is a model do-ing just that one thing: generating statisticallylikely continuations of word sequences.With this insight to the fore, let’s revisit thequestion of how LLMs compare to humans, andreconsider the propriety of the language we useto talk about them. In contrast to humans likeBob and Alice, a simple LLM-based question-answering system, such as BOT, has no commu-nicative intent (Bender and Koller, 2020). In nomeaningful sense, even under the licence of theintentional stance, does it know that the ques-tions it is asked come from a person, or that aperson is on the receiving end of its answers. Byimplication, it knows nothing about that person.It has no understanding of what they want toknow nor of the effect its response will have ontheir beliefs.Moreover, in contrast to its human interlocu-tors, a simple LLM-based question-answeringsystem like BOT does not properly speaking havebeliefs.6 BOT does not “really” know that Bu-rundi is south of Rwanda, although the inten-tional stance does, in this case, license Alice’scasual remark to the contrary. To see this, weneed to think separately about the underlyingLLM and the system in which it is embedded.First, let’s consider the underlying LLM, thatis to say the bare-bones model, comprising the6This paper focuses on belief, knowledge, and rea-son. Others have argued about meaning in LLMs(Bender and Koller, 2020; Piantadosi and Hill, 2022).Here we take no particular stand on meaning, instead pre-ferring questions about how words are used, whether theyare words generated by the LLMs themselves or wordsgenerated by humans that are about LLMs.4model architecture and the trained parameters.A bare-bones LLM doesn’t “really” know any-thing because all it does, at a fundamental level,is sequence prediction. Sometimes a predictedsequence takes the form of a proposition. But thespecial relationship propositional sequences haveto truth is apparent only to the humans who areasking questions, or to those who provided thedata the model was trained on. Sequences ofwords with a propositional form are not specialto the model itself in the way they are to us. Themodel itself has no notion of truth or falsehood,properly speaking, because it lacks the means toexercise these concepts in anything like the waywe do.It could perhaps be argued that an LLM“knows” what words typically follow what otherwords, in a sense that does not rely on the inten-tional stance. But even if we allow this, knowingthat the word “Burundi” is likely to succeed thewords “The country to the south of Rwanda is”is not the same as knowing that Burundi is to thesouth of Rwanda. To confuse those two thingsis to make a profound category mistake. If youdoubt this, consider whether knowing that theword “little” is likely to follow the words “Twin-kle, twinkle” is the same as knowing that twinkletwinkle little. The idea doesn’t even make sense.So much for the bare-bones language model.What about the whole dialogue system of whichthe LLM is the core component? Does that havebeliefs, properly speaking? At least the very ideaof the whole system having beliefs makes sense.There is no category error here. However, fora simple dialogue agent like BOT, the answer issurely still “no”. A simple LLM-based question-answering system like BOT lacks the means touse the words “true” and “false” in all the ways,and in all the contexts, that we do. It cannotparticipate fully in the human language game oftruth, because it does not inhabit the world wehuman language-users share.76 What About Emergence?Contemporary large language models are so pow-erful, so versatile, and so useful that the argu-ment above might be difficult to accept. Ex-changes with state-of-the-art (in late 2022) LLM-based conversational agents are so convincing, itis hard to not to anthropomorphise them. Could7For a discussion of the “language game of truth”, seeShanahan (2010), pp.36–39.it be that something more complex and subtle isgoing on here?One tempting line of argument goes like this.Although large language models, at root, onlyperform sequence prediction, it’s possible that,in learning to do this, they have discoveredemergent mechanisms that warrant a descriptionin higher-level terms. These higher-level termsmight include “knowledge” and “belief”. Indeed,we know that artificial neural networks can ap-proximate any computable function to an arbi-trary degree of accuracy. So whatever mecha-nisms are needed to enable the formation of be-liefs, they probably reside in the parameter spacesomewhere. Given a big enough model, enoughdata of the right sort, and enough computingpower to train the model, perhaps stochastic gra-dient descent can discover such mechanisms ifthey are the best way to optimise the objectiveof making accurate sequence predictions.This argument has considerable appeal. Af-ter all, the overriding lesson of recent progressin LLMs is that extraordinary and unexpectedcapabilities emerge when big enough models aretrained on very large quantities of textual data.However, as long as our considerations are con-fined to a simple LLM-based question-answeringsystem, this has little bearing on the issue ofcommunicative intent. It doesn’t matter whatinternal mechanisms it uses, a sequence predic-tor is not, in itself, the kind of thing that could,even in principle, have communicative intent,and simply embedding it in a dialogue manage-ment system will not help.But what about knowledge and belief? Coulda sophisticated emergent mechanism of the rightsort license us to speak of an LLM as if it “re-ally” knew or believed something? Again, it’simportant to distinguish between the bare-bonesmodel and the whole system. Only in the contextof a capacity to distinguish truth from falsehoodcan we legitimately speak of “belief” in its fullestsense. But an LLM is not in the business of mak-ing judgements. It just models what words arelikely to follow from what other words. The in-ternal mechanisms it uses to do this, whateverthey are, cannot in themselves be sensitive tothe truth or otherwise of the word sequences itpredicts.Of course, it is perfectly acceptable to saythat an LLM “encodes”, “stores”, or “contains”knowledge, in the same sense that an encyclo-pedia can be said to encode, store, or contain5knowledge. Indeed, it can reasonably be claimedthat one emergent property of an LLM is thatit encodes kinds of knowledge of the everydayworld and the way it works that no encyclope-dia captures (Li et al., 2021). But if Alice wereto remark that “Wikipedia knew that Burundiwas south of Rwanda”, it would be a figure ofspeech, not a literal statement. An encyclopediadoesn’t literally “know” or “believe” anything,in the way that a human does, and neither doesa bare-bones LLM.The real issue here is that, whatever emergentproperties it has, the LLM itself has no accessto any external reality against which its wordsmight be measured, nor the means to apply anyother external criteria of truth, such as agree-ment with other language-users.8 It only makessense to speak of such criteria in the context ofthe system as a whole, and for a system as awhole to meet them, it needs to be more thana simple conversational agent. In the words ofB.C.Smith, it must “authentically engage withthe world’s being the way in which [its] repre-sentations represent it as being” (Smith, 2019).7 External Information SourcesThe point here does not concern any specific be-lief. It concerns the prerequisites for ascribingany beliefs at all to a system. Nothing can countas a belief about the world we share — in thelargest sense of the term — unless it is againstthe backdrop of the ability to update beliefs ap-propriately in the light of evidence from thatworld, an essential aspect of the capacity to dis-tinguish truth from falsehood.Could Wikipedia, or some other trustworthyfactual website, provide external criteria againstwhich the truth or falsehood of a belief might bemeasured? Suppose an LLM were embedded ina system that regularly consulted such sources(as LaMDA does (Thoppilan et al., 2022)), andused a contemporary model editing techniqueto maintain the factual accuracy of its predic-tions (such as the one described by Meng et al.(2022)9). Would this not count as exercising the8Davidson uses a similar argument to call into questionwhether belief is possible without language (Davidson,1982). The point here is different. We are concernedwith conditions that have to be met for the generation ofa natural language sentence to reflect the possession of apropositional attitude.9Commendably, Meng et al. (2022) use the term “fac-tual associations” to denote the information that under-required sort of capacity to update belief in thelight of evidence?Crucially, this line of thinking depends on theshift from the language model itself to the largersystem of which the language model is a part.The language model itself is still just a sequencepredictor, and has no more access to the exter-nal world than it ever did. It is only with respectto the whole system that the intentional stancebecomes more compelling in such a case. Butbefore yielding to it, we should remind ourselvesof how very different such systems are from hu-man beings. When Alice took to Wikipedia andconfirmed that Burundi was south of Rwanda,what took place was more than just an updateto a model in her head of the distribution of wordsequences in the English language.The change that took place in Alice was a re-flection of her nature as a language-using ani-mal inhabiting a shared world with a communityof other language-users. Humans are the natu-ral home of talk of beliefs and the like, and thebehavioural expectations that go hand-in-handwith such talk are grounded in our mutual under-standing, which is itself the product of a commonevolutionary heritage. When we interact with anAI system based on a large language model, thesegrounds are absent, an important considerationwhen deciding whether or not to speak of such asystem as if it “really” had beliefs.8 Vision-Language ModelsA sequence predictor may not by itself be thekind of thing that could have communicativeintent or form beliefs about an external real-ity. But, as repeatedly emphasised, LLMs in thewild must be embedded in larger architecturesto be useful. To build a question-answering sys-tem, the LLM simply has to be supplementedwith a dialogue management system that queriesthe model as appropriate. There is nothing thislarger architecture does that might count as com-municative intent or the capacity to form beliefs.So the point stands.However, LLMs can be combined with othersorts of models and / or embedded in morecomplex architectures. Vision-language mod-els (VLMs) such as VilBERT (Lu et al., 2019)and Flamingo (Alayrac et al., 2022), for exam-ple, combine a language model with an imagelies an LLM’s ability to generate word sequences with apropositional form.6encoder, and are trained on a multi-modal cor-pus of text-image pairs. This enables them topredict how a given sequence of words will con-tinue in the context of a given image. VLMs canbe used for visual question-answering or to en-gage in a dialogue about a user-provided image.Could a user-provided image stand in for anexternal reality against which the truth or false-hood of a proposition can be assessed? Could itbe legitimate to speak of a VLM’s beliefs, in thefull sense of the term? We can indeed imagine aVLM that uses an LLM to generate hypothesesabout an image, then verifies their truth with re-spect to that image (perhaps by consulting a hu-man), and then fine-tunes the LLM not to makestatements that turn out to be false. Talk ofbelief here would perhaps be less problematic.However, most contemporary VLM-based sys-tems don’t work this way. Rather, they dependon frozen models of the joint distribution of textand images. In this respect, the relationship be-tween a user-provided image and the words gen-erated by the VLM is fundamentally differentfrom the relationship between the world sharedby humans and the words we use when we talkabout that world. Importantly, the former rela-tionship is mere correlation, while the latter iscausal.10The consequences of the lack of causality aretroubling. If the user presents the VLM with apicture of a dog, and the VLM says “This is apicture of a dog”, there is no guarantee that itswords are connected with the dog in particular,rather than some other feature of the image thatis spuriously correlated with dogs (such as thepresence of a kennel). Conversely, if the VLMsays there is a dog in an image, there is no guar-antee that there actually is a dog, rather thanjust a kennel.Whether or not these concerns apply to anyspecific VLM-based system depends on exactlyhow that system works; what sort of model ituses, and how that model is embedded in thesystem’s overall architecture. But to the extentthat the relationship between words and thingsfor a given VLM-based system is different thanit is for human language-users, it might be pru-dent not to take literally talk of what that system“knows” or “believes”.10Of course, there is causal structure to the computa-tions carried out by the model during inference. But thisis not the same as there being causal relations betweenwords and the things those words are taken to be about.9 What About Embodiment?Humans are members of a community oflanguage-users inhabiting a shared world, andthis primal fact makes them essentially differentto large language models. Human language userscan consult the world to settle their disagree-ments and update their beliefs. They can, so tospeak, “triangulate” on objective reality. In iso-lation, an LLM is not the sort of thing that cando this, but in application, LLMs are embeddedin larger systems. What if an LLM is embeddedin a system capable of interacting with a worldexternal to itself? What if the system in ques-tion is embodied, either physically in a robot orvirtually in an avatar?When such a system inhabits a world like ourown — a world populated with 3D objects, someof which are other agents, some of whom arelanguage-users — it is, in this important respect,a lot more human-like than a disembodied lan-guage model. But whether or not it is appro-priate to speak of communicative intent in thecontext of such a system, or of knowledge andbelief, in their fullest sense, depends on exactlyhow the LLM is embodied.As an example, let’s consider the SayCan sys-tem of Ahn et al. (2022). In this work, an LLMis embedded in a system that controls a physi-cal robot. The robot carries out everyday tasks(such as clearing a spillage) in accordance witha user’s high-level natural language instruction.The job of LLM is to map the user’s instructionto low-level actions (such as finding a sponge)that will help the robot to achieve the requiredgoal. This is done via an engineered promptprefix that makes the model output natural lan-guage descriptions of suitable low-level actions,scoring them for usefulness.The language model component of the SayCansystem suggests actions without taking into ac-count what the environment actually affords therobot at the time. Perhaps there is a spongeto hand. Perhaps not. Accordingly, a separateperceptual module assesses the scene using therobot’s sensors and determines the current feasi-bility of performing each low-level action. Com-bining the LLM’s estimate of each action’s use-fulness with the perceptual module’s estimate ofeach action’s feasibility yields the best action toattempt next.SayCan exemplifies the many innovative waysthat a large language model can be put to use.Moreover, it could be argued that the natu-7ral language descriptions of recommended low-level actions generated by the LLM are groundedthanks to their role as intermediaries betweenperception and action.11 Nevertheless, despitebeing physically embodied and interacting withthe real world, the way language is learned andused in a system like SayCan is very differentfrom the way it is learned and used by a human.The language models incorporated in systemslike SayCan are pre-trained to perform sequenceprediction in a disembodied setting from a text-only dataset. They have not learned language bytalking to other language-users while immersedin a shared world and engaged in joint activity.SayCan is suggestive of the kind of embodiedlanguage-using system we might see in the fu-ture. But in such systems today, the role of lan-guage is very limited. The user issues instruc-tions to the system in natural language, and thesystem generates interpretable natural languagedescriptions of its actions. But this tiny reper-toire of language use hardly bears comparisonto the cornucopia of collective activity that lan-guage supports in humans.The upshot of this is that we should be justas cautious in our choice of words when talkingabout embodied systems incorporating LLMs aswe are when talking about disembodied systemsthat incorporate LLMs. Under the licence of theintentional stance, a user might say that a robotknew there was a cup to hand if it stated “I canget you a cup” and proceeded to do so. Butif pressed, the wise engineer might demur whenasked whether the robot really understood thesituation, especially if its repertoire is confinedto a handful of simple actions in a carefully con-trolled environment.10 Can Language Models Reason?While the answer to the question “Do LLM-based systems really have beliefs?” is usually“no”, the question “Can LLM-based systems re-ally reason?” is harder to settle. This is be-cause reasoning, insofar as it is founded in formallogic, is content neutral. The modus ponens ruleof inference, for example, is valid whatever thepremises are about. If all squirgles are splonkyand Gilfred is a squirgle then it follows that Gil-11None of the symbols manipulated by an LLM aregrounded in the sense of Harnad (1990), that is to saythrough perception, except indirectly and parasiticallythrough the humans who generated the original trainingdata.fred is splonky. The conclusion follows from thepremises here irrespective of the meaning (if any)of “squirgle” and “splonky”, and whoever the un-fortunate Gilfred might be.The content neutrality of logic means that wecannot criticise talk of reasoning in LLMs on thegrounds that they have no access to an exter-nal reality against which truth or falsehood canbe measured. However, as always, it’s crucialto keep in mind what LLMs really do. If weprompt an LLM with “All humans are mortaland Socrates is human therefore”, we are notinstructing it to carry out deductive inference.Rather, we are asking it the following question.Given the statistical distribution of words in thepublic corpus, what words are likely to follow thesequence ‘All humans are mortal and Socrates ishuman therefore”.If reasoning problems could be solved withnothing more than a single step of deductive in-ference, then an LLM’s ability to answer ques-tions such as this might be sufficient. But non-trivial reasoning problems require multiple infer-ence steps. LLMs can be effectively applied tomulti-step reasoning, without further training,thanks to clever prompt engineering. In chain-of-thought prompting, for example, a prompt pre-fix is submitted to the model, before the user’squery, containing a few examples of multi-stepreasoning, with all the intermediate steps ex-plicitly spelled out (Nye et al., 2021; Wei et al.,2022).Including a prompt prefix in the chain-of-thought style encourages the model to generatefollow-on sequences in the same style, which isto say comprising a series of explicit reasoningsteps that lead to the final answer. As usual,the question really being posed to the model isof the form “Given the statistical distribution ofwords in the public corpus, what words are likelyto follow the sequence S”, where in this case thesequence S is the chain-of-thought prompt prefixplus the user’s query. The sequences of tokensthat are most likely to follow S will have a simi-lar form to sequences found in the prompt prefix,which is to say they will include multiple steps ofreasoning, so these are what the model generates.Remarkably, not only do the model’s responsestake the form of an argument with multiple steps,the argument in question is often (but not al-ways) valid, and the final answer is often (butnot always) correct. To the extent that a suit-ably prompted LLM appears to reason correctly,8it does so by mimicking well-formed argumentsin its training set and / or in the prompt. Butcould this mimicry ever amount to genuine rea-soning? Even if today’s models make occasionalmistakes, could further scaling iron these out tothe point that a model’s performance was indis-tinguishable from that of a hard-coded reasoningalgorithm, such as we find in a theorem prover,for example? Maybe. But how would we know?How could we come to trust such a model?Well, the sequences of sentences generated bya theorem prover are faithful to logic, in thesense that they are the result of an underly-ing computational process whose causal struc-ture mirrors the inferential structure of theproblem (Creswell and Shanahan, 2022). Oneway to build a trustworthy reasoning systemusing an LLM is to embed it in an algo-rithm that enforces the same causal structure(Creswell and Shanahan, 2022; Creswell et al.,2022). But if we stuck with a pure LLM, the onlyway to fully trust the arguments it generatedwould be by reverse engineering it and discov-ering emergent mechanisms that conformed tothe faithful reasoning prescription. In the meantime, we should proceed with caution, and usediscretion when characterising what these mod-els do.11 Conclusion: Why This MattersDoes the foregoing discussion amount to any-thing more than philosophical nitpicking? Surelywhen researchers talk of “beliefs”, “knowledge”,“reason”, and the like, the meaning of thoseterms is perfectly clear. In papers, researchersuse such terms as a convenient shorthand for pre-cisely defined computational mechanisms, as al-lowed by the intentional stance. Well, this is fineas long as there is no possibility of anyone as-signing more weight to such terms than they canlegitimately bear, if there is no danger of theiruse misleading anyone about the character andcapabilities of the systems being described.However, today’s large language models, andthe applications that use them, are so powerful,so convincingly intelligent, that such licence canno longer safely be applied (Ruane et al., 2019;Weidinger et al., 2021). As AI practitioners, theway we talk about LLMs matters. It matters notonly when we write scientific papers, but alsowhen we interact with policy makers or speakto the media. The careless use of philosophicallyloaded words like “believes” and “thinks” is espe-cially problematic, because such terms obfuscatemechanism and actively encourage anthropomor-phism.Interacting with a contemporary LLM-basedconversational agent can create a compelling il-lusion of being in the presence of a thinkingcreature like ourselves. Yet in their very na-ture, such systems are fundamentally not likeourselves. The shared “form of life” that un-derlies mutual understanding and trust amonghumans is absent, and these systems can be in-scrutable as a result, presenting a patchwork ofless-than-human with superhuman capacities, ofuncannily human-like with peculiarly inhumanbehaviours.The sudden presence among us of exotic,mind-like entities might precipitate a shift in theway we use familiar psychological terms like “be-lieves” and “thinks”, or perhaps the introductionof new words and turns of phrase. But it takestime for new language to settle, and for new waysof talking to find their place in human affairs. Itmay require an extensive period of interactingwith, of living with, these new kinds of artefactbefore we learn how best to talk about them.12Meanwhile, we should try to resist the siren callof anthropomorphism.AcknowledgmentsThanks to Toni Creswell, Richard Evans, Chris-tos Kaplanis, Andrew Lampinen, and KyriacosNikiforou for invaluable (and robust) discussionson the topic of this paper.ReferencesM. Ahn, A. Brohan, N. Brown, Y. Chebotar,O. Cortes, et al. Do as I can, not as I say:Grounding language in robotic affordances.arXiv preprint arXiv:2204.01691, 2022.J.-B. Alayrac, J. Donahue, P. Luc, A. Miech,I. Barr, et al. Flamingo: a visual languagemodel for few-shot learning. arXiv preprintarXiv:2204.14198, 2022.E. Bender and A. Koller. Climbing towards NLU:On meaning, form, and understanding in the12Ideally, we would also like a theoretical understand-ing of their inner workings. But at present, despite somecommendable work in the right direction (Elhage et al.,2021; Li et al., 2021; Olsson et al., 2022), we still awaitsuch an understanding.9age of data. In Proceedings of the 58th AnnualMeeting of the Association for ComputationalLinguistics, pages 5185–5198, 2020.E. Bender, T. Gebru, A. McMillan-Major, andS. Shmitchell. On the dangers of stochasticparrots: Can language models be too big? InProceedings of the 2021 ACM Conference onFairness, Accountability, and Transparency,pages 610–623, 2021.T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D.Kaplan, et al. Language models are few-shotlearners. In Advances in Neural InformationProcessing Systems, volume 33, pages 1877–1901, 2020.A. Chowdhery, S. Narang, J. Devlin, M. Bosma,G. Mishra, et al. PaLM: Scaling languagemodeling with pathways. arXiv preprintarxiv:2204.02311, 2022.A. Creswell and M. Shanahan. Faithful reasoningusing large language models. arXiv preprintarXiv:2208.14271, 2022.A. Creswell, M. Shanahan, and I. Higgins.Selection-inference: Exploiting large languagemodels for interpretable logical reasoning.arXiv preprint arXiv:2205.09712, 2022.D. Davidson. Rational animals. Dialectica, 36:317–327, 1982.D. Dennett. Intentional systems theory. In TheOxford Handbook of Philosophy of Mind, pages339–350. Oxford University Press, 2009.J. Devlin, M.-W. Chang, K. Lee, andK. Toutanova. BERT: Pre-training of deepbidirectional transformers for language under-standing. arXiv preprint arXiv:1810.04805,2018.N. Elhage, N. Nanda, C. Olsson, T. Henighan,N. Joseph, et al. A mathematical frame-work for transformer circuits. TransformerCircuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html.A. Glaese, N. McAleese, M. Trȩbacz,J. Aslanides, V. Firoiu, et al. Improving align-ment of dialogue agents via targeted humanjudgements. arXiv preprint arXiv:2209.14375,2022.A. Y. Halevy, P. Norvig, and F. Pereira. Theunreasonable effectiveness of data. IEEE In-telligent Systems, 24(2):8–12, 2009.S. Harnad. The symbol grounding problem.Physica D: Nonlinear Phenomena, 42(1-3):335–346, 1990.T. Kojima, S. S. Gu, M. Reid, Y. Mat-suo, and Y. Iwasawa. Large language mod-els are zero-shot reasoners. arXiv preprintarXiv:2205.11916, 2022.B. Z. Li, M. Nye, and J. Andreas. Implicit repre-sentations of meaning in neural language mod-els. In Proceedings of the 59th Annual Meetingof the Association for Computational Linguis-tics and the 11th International Joint Confer-ence on Natural Language Processing (Volume1: Long Papers), 2021.J. Lu, D. Batra, D. Parikh, and S. Lee.ViLBERT: Pretraining task-agnostic visiolin-guistic representations for vision-and-languagetasks. arXiv preprint arXiv:1908.02265, 2019.G. Marcus and E. Davis. GPT-3, bloviator: Ope-nAI’s language generator has no idea what it’stalking about. MIT Technology Review, Au-gust 2020, 2020.K. Meng, D. Bau, A. J. Andonian, and Y. Be-linkov. Locating and editing factual associa-tions in GPT. In Advances in Neural Infor-mation Processing Systems, 2022.M. Nye, A. J. Andreassen, G. Gur-Ari,H. Michalewski, et al. Show your work:Scratchpads for intermediate computationwith language models. arXiv preprintarXiv:2112.00114, 2021.C. Olsson, N. Elhage, N. Nanda, N. Joseph,N. DasSarma, et al. In-context learn-ing and induction heads. TransformerCircuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.S. T. Piantadosi and F. Hill. Meaning with-out reference in large language models. arXivpreprint arXiv:2208.02957, 2022.A. Radford, J. Wu, R. Child, D. Luan,D. Amodei, and I. Sutskever. Language mod-els are unsupervised multitask learners. 2019.10J. W. Rae, S. Borgeaud, T. Cai, K. Millican,J. Hoffmann, et al. Scaling language models:Methods, analysis & insights from training Go-pher. arXiv preprint arXiv:2112.11446, 2021.E. Ruane, A. Birhane, and A. Ventresque. Con-versational AI: Social and ethical considera-tions. In Proceedings 27th AIAI Irish Con-ference on Artificial Intelligence and CognitiveScience, pages 104–115, 2019.M. Shanahan. Embodiment and the Inner Life:Cognition and Consciousness in the Space ofPossible Minds. Oxford University Press,2010.B. C. Smith. The Promise of Artificial Intelli-gence: Reckoning and Judgment. MIT Press,2019.R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer,A. Kulshreshtha, et al. LaMDA: Languagemodels for dialog applications. arXiv preprintarXiv:2201.08239, 2022.A. Vaswani, N. Shazeer, N. Parmar, J. Uszko-reit, L. Jones, A. N. Gomez,  L. Kaiser, andI. Polosukhin. Attention is all you need. In Ad-vances in Neural Information Processing Sys-tems, pages 5998–6008, 2017.J. Wei, X. Wang, D. Schuurmans, M. Bosma,B. Ichter, et al. Chain-of-thought prompt-ing elicits reasoning in large language models.In Advances in Neural Information ProcessingSystems, 2022.L. Weidinger, J. Mellor, M. Rauh, C. Griffin,J. Uesato, et al. Ethical and social risks ofharm from language models. arXiv preprintarXiv:2112.04359, 2021.L. Wittgenstein. Philosophical Investigations.(Translated by Anscombe, G.E.M.). BasilBlackwell, 1953.11",
    "Link": "http://arxiv.org/abs/2212.03551"
}