{
    "Title": "Large Margin Neural Language Model",
    "Authors": "Huang, Jiaji, Huang, Liang, Li, Yi, Ping, Wei",
    "Year": "No year available",
    "Abstract": "We propose a large margin criterion for training neural language models.\nConventionally, neural language models are trained by minimizing perplexity\n(PPL) on grammatical sentences. However, we demonstrate that PPL may not be the\nbest metric to optimize in some tasks, and further propose a large margin\nformulation. The proposed method aims to enlarge the margin between the \"good\"\nand \"bad\" sentences in a task-specific sense. It is trained end-to-end and can\nbe widely applied to tasks that involve re-scoring of generated text. Compared\nwith minimum-PPL training, our method gains up to 1.1 WER reduction for speech\nrecognition and 1.0 BLEU increase for machine translation.Comment: 9 pages. Accepted as a long paper in EMNLP201",
    "Keywords": "No keywords available",
    "Publisher": "",
    "Publication Date": "No publication date available",
    "Journal": "No journal available",
    "Citation Count": 0,
    "Full Text": "Large Margin Neural Language Model\nJiaji Huang1 Yi Li1 Wei Ping1 Liang Huang1,2∗\n1 Baidu Research, Sunnyvale, CA, USA\n2 School of EECS, Oregon State University, Corvallis, OR, USA\n{huangjiaji, liyi17, pingwei01, lianghuang}@baidu.com\nAbstract\nWe propose a large margin criterion for train-\ning neural language models. Conventionally,\nneural language models are trained by mini-\nmizing perplexity (PPL) on grammatical sen-\ntences. However, we demonstrate that PPL\nmay not be the best metric to optimize in\nsome tasks, and further propose a large mar-\ngin formulation. The proposed method aims\nto enlarge the margin between the “good” and\n“bad” sentences in a task-specific sense. It is\ntrained end-to-end and can be widely applied\nto tasks that involve re-scoring of generated\ntext. Compared with minimum-PPL training,\nour method gains up to 1.1 WER reduction for\nspeech recognition and 1.0 BLEU increase for\nmachine translation.\n1 Introduction\nLanguage models (LMs) estimate the likelihood\nof a symbol sequence {xt}Tt=0, based on the joint\nprobability,\np(x0, . . . , xT ) = p(x0)\nT∏\nt=1\np(xt|x0, . . . , xt−1).\n(1)\nTo measure the quality of an LM, a commonly\nadopted metric is perplexity (PPL), defined as\nPPL , exp\n{\n− 1\nT\nT∑\nt=0\nlog p(xt|x0, . . . , xt−1)\n}\n,\nA good language model has a small PPL, being\nable to assign higher likelihoods to sentences that\nare more likely to appear.\nLMs are widely applied in automatic speech\nrecognition (ASR) (Yu and Deng, 2014) and ma-\nchine translation (MT) (Koehn, 2009). Follow-\ning Koehn (2009), one may interpret the language\n∗Contributions were made while at Baidu Research.\nmodel as prior knowledge on the text to be in-\nferred, which provides information complemen-\ntary to the ASR or MT system itself. In prac-\ntice, there are several ways to incorporate the lan-\nguage model. The simplest way may be re-scoring\nan n-best list returned by the ASR or MT sys-\ntem (Mikolov et al., 2010; Sundermeyer et al.,\n2012). A slightly more sophisticated way is to\njointly consider the ASR/MT and language model\nin a beam search decoder (Amodei et al., 2016).\nSpecifically, at each time step, the decoder ap-\npends every symbol in the vocabulary to each se-\nquence in the current candidate set. For every\nhypothesis, a score is calculated as a linear com-\nbination of the log-likelihoods given by both the\nASR/MT and language models. Then, only the\ntop K hypotheses with the highest scores are re-\ntained, as an updated candidate set. More recently,\nGulcehre et al. (2015) and Sriram et al. (2017) pro-\npose to predict the next symbol based on a fusion\nof the hidden states in the ASR/MT and language\nmodels. A gating mechanism is jointly trained to\ndetermine how much the language model should\ncontribute.\nThe afore-discussed language models are gen-\nerative in the sense that they merely model the\njoint distribution of a symbol sequence (Eq. (1)).\nWhile the research community is mostly focused\non pushing the limit of PPL (e.g., Jozefowicz et al.,\n2016), very limited attention has been paid to the\ndiscrimination power of language models when\nthey are applied to real tasks, such as ASR and\nMT (Li and Khudanpur, 2008). By contrast, dis-\ncriminative language modeling aims at enhancing\nthe performance in downstream applications. For\nexample, existing works (Roark et al., 2004, 2007)\noften target at improving ASR accuracy. The\nkey motivation underlying them is that the model\nshould be able to discriminate between “good” and\n“bad” sentences in a task-specific sense, instead\nar\nX\niv\n:1\n80\n8.\n08\n98\n7v\n1 \n [c\ns.C\nL]\n  2\n7 A\nug\n 20\n18\nof just modeling grammatical ones. The com-\nmon methodology (Dikic et al., 2013) is to build\na binary classifier upon hand-crafted features ex-\ntracted from the sentences. However, it is not ob-\nvious how these methods can utilize large unanno-\ntated corpus, which is often easily available, and\nthe hand-crafted features are also ad hoc and may\nresult in suboptimal performance.\nIn this work, we study how to improve the dis-\ncrimination ability of a recurrent network-based\nneural language model (RNNLM). The goal is to\nenlarge the difference between the log-likelihoods\nof “good” and “bad” sentences. In an contrast to\nthe existing works (Roark et al., 2004, 2007), our\nmethod does not rely on hand-crafted features, and\nis trained in end-to-end manner and able to take\nadvantage of large external text corpus. In fact,\nit is a general training criterion that is transparent\nto the network architecture of the RNNLM, and\ncan be applied to various text generation tasks, in-\ncluding ASR and MT. Experiments on state-of-art\nASR and MT systems show its significant advan-\ntage over an LM trained by minimizing PPL.\n2 Background on RNNLM\nWe first give some background knowledge on\nRNNLMs. The prototypical RNNLM (Mikolov\net al., 2010) has one layer of recurrent cell and\nworks as follows. Denote a sentence as x =\n[x0, . . . , xt, . . . ], where the xt’s are words. Let\n~xt be the embedding vector for xt. The recurrent\ncell takes in the embedding and produces a hidden\nstate ~ht by\n~ht = σ(U~xt + V~ht−1),\nwhere σ(z) = 1\n1+e−z is sigmoid activation func-\ntion. ~ht−1 is the hidden state at the last timestep.\nU and V are learnable parameters. The ~ht is then\npassed into a multi-way classifier to produce a\nprobability distribution over the vocabulary (for\nthe next word),\n~p = softmax(W~ht +~b).\nThe W and ~b are also trainable parameters.\nThe training objective is to maximize the log-\nlikelihood of the next word, and the parameters are\nlearned by back-propagation algorithm.\nThe vanilla recurrent cell can also be re-\nplaced by one or multiple layers of LSTM cells,\nwhich produces better results (Zaremba et al.,\n2014). In a more general form, the RNNLM\ncan be represented as a conditional probability,\npθ(x\nt|x0, . . . , xt−1), parameterized by θ. In the\nprototypical case, θ = [U, V,W,~b]. We could de-\nfine the LM-score of a sentence x as\nLM-score(x) , log pθ(x)\n=\n∑\nt\nlog pθ(x\nt|x0, . . . , xt−1).\nThe RNNLM is trained by maximizing the aver-\nage LM-score over all the x’s in a corpus, or equiv-\nalently, minimizing the PPL on the corpus.\n3 Problem Formulation\nWe motivate and formulate a large margin training\ncriterion in this section. Suppose for every ref-\nerence sentence xi, we have a collection of hy-\npotheses xi,j , j = 1, . . . ,K, usually obtained as\nthe top-K candidates by a beam search decoder.\n3.1 A Motivating Example\nAn RNNLM trained by minimizing PPL cannot\nguarantee a higher score on the “gold” reference\nthan the inferior hypothesis, which is undesirable.\nOne example is given in Tab. 1. The reference is\ntaken from the text labels of dev93’ set of Wall\nStreet Journal (WSJ) dataset. The hypothesis is\ngenerated by a CTC-based (Graves et al., 2006)\nASR system trained on WSJ training set. Words\nin red are mistakes made by the hypothesis. We\nthen train an RNNLM on Common Crawl1 co-\npora by minimizing PPL. Training follows a typi-\ncal setup (Jozefowicz et al., 2016) with a vocabu-\nlary of 400K the most frequent words. Any out-of-\nvocabulary word is replaced by an 〈UNK〉 token.\nThe RNNLM is then employed to score the sen-\ntences. The LM-score of the erroneous hypothesis\nis higher than that of the reference. In fact, this is\nreasonable as “a decade as concerns” seems to be\na more common phrase. In the training corpus, we\nfind that “a decade as concerns” appears once, but\n“its defeat is confirmed” does not appear. More-\nover, “a decade as” appears 2,280 times, but “its\ndefeat is” appears only 24 times. However, this is\nundesirable because if there is another hypothesis\nthat happens to be the same as reference, which\nwill not be ranked as the best candidate.\nIt would be helpful if the LM can also learn\nfrom the imperfect hypotheses so that it can tell\n1http://web-language-models.\ns3-website-us-east-1.amazonaws.com/\nwmt16/deduped/en-new.xz\nSentence LM-score\nreference\nconiston declined to discuss its\nplans for its defeat is confirmed\nbut indicated that it doesn’t\nplan to simply walk away\n-116.52\nhypothesis\nconiston declined to discuss its\nplans for a decade as concerns\nbut indicated that it doesn’t\nplan to simply walk away\n-112.65\nTable 1: Reference and one hypothesis, scored by\nan RNNLM. Words in red are mistakes in the hy-\npothesis. The RNNLM is trained on Common\nCrawl copora by minimizing PPL. We want the\nreference to be higher scored than the hypothesis,\nbut it does not happen here.\n0 2000 4000 6000 8000 10000\niteration\n−2\n0\n2\n4\n6\n8\ntra\nin\n lo\nss\n1e7\n(a)\n0 2000 4000 6000 8000 10000\niteration\n0.0\n0.5\n1.0\n1.5\n2.0\ntra\nin\n lo\nss\n(b)\nFigure 1: Training losses of (a) straightforward for-\nmulation Eq. (2); and (b) large margin formulation\nEq. (3)\n(a) (b)\nFigure 2: Histogram of the margin log p(xi) −\nlog p(xi,j). The more positive, the more the dis-\ncrimination. (a) Straightforward formulation; (b)\nLMLM compared with RNNLM (a minimum-PPL\nLM trained on Common Crawl)\napart “good” and “bad” candidates. With this\nmotivation, we train to assign larger LM-scores\nfor the xi’s but smaller ones for the (imper-\nfect) xi,j’s. A quantity of particular interest is\nlog p(xi) − log p(xi,j), the margin/difference be-\ntween the LM-scores of the references and the\n(imperfect) hypotheses. The intuition is that the\nmore positive the margin, the better the LM is at\ndiscrimination.\n3.2 Straightforward but Failed Formulation\nWithout loss of generality, we assume that all\nthe xi,j’s are imperfect and different from xi. A\nstraightforward way is to adopt the following ob-\njective:\nmin\nθ\n1\nN\nN∑\ni=1\n− log pθ(xi) + 1\nK\nK∑\nj=1\nlog pθ(xi,j)\n .\n(2)\nSimilar formulation is also seen in (Tachioka and\nWatanabe, 2015), where they only utilize one\nbeam candidate, i.e., K = 1. Optimization can be\ncarried out by mini-batch stochastic gradient de-\nscent (SGD). Each iteration, SGD randomly sam-\nples a batch of i’s and j’s, computes stochastic\ngradient w.r.t. θ, and takes an update step. How-\never, a potential problem with this formulation is\nthat the second term (corresponding to the infe-\nrior hypotheses) may dominate the optimization.\nSpecifically, the training is almost always driven\nby the xi,j’s, but does not effectively enhance the\ndiscrimination. We illustrate this fact in the fol-\nlowing experiment.\nUsing the ASR system in section 3.1, we extract\n256 beam candidates for every training example in\nWall Street Journal (WSJ) dataset. Warm started\nfrom the pre-trained RNNLM in section 3.1, we\napply SGD to minimize the loss in Eq. (2), with\na mini-batch size of 128. The training loss is\nshown in Fig. 1a. We observe that the learning\ndynamic is very unstable, and deceases to be neg-\native. The unbound decreasing is due to the second\nterm in Eq. (2) being negative and dominating the\ntraining process. Next, we inspect log pθ(xi) −\nlog pθ(xi,j), the margin between the scores of a\nground-truth and a candidate. In Fig. 2a, we his-\ntogram the margins for all the i, j’s in a dev set.\nThe distribution appears to be symmetric around\nzero, which indicates poor discrimination ability.\nGiven these facts, we conclude that the straight-\nforward formulation in Eq. (2) is not effective.\n3.3 Large Margin Formulation\nTo effectively utilize all the imperfect beam candi-\ndates, we propose the following objective,\nmin\nθ\nN∑\ni=1\nB∑\nj=1\nmax\n{\n0, τ−(log pθ(xi)−log pθ(xi,j))\n}\n,\n(3)\nwhere log pθ(xi) − logθ(xi,j) is the margin be-\ntween the scores of a ground-truth xi and a can-\ndidate xi,j . The hinge loss on the margin encour-\nages the log-likelihood of the ground-truth to be at\nleast τ larger than that of the imperfect hypothesis.\nWe call an LM trained by the above formulation as\nLarge Margin Language Model (LMLM).\nWe repeat the same experiment in section 3.2,\nbut change the objective function to Eq. (3) and\nset τ = 1. Fig. 1b shows the training loss, which\nsteadily decreases and approaches zero rapidly.\nCompared with the learning curve of naive formu-\nlation (Fig. 1a), the large margin based training is\nmuch more stable. In Fig. 2b, we also examine\nthe histogram of log pθ(xi) − log pθ(xi,j), where\npθ(·) is now the LM learned by LMLM. Compared\nwith the histogram by the conventional RNNLM,\nLMLM significantly moves the distribution to the\npositive side, indicating more discrimination.\n3.4 Ranking Loss Type Formulation\nIn most cases, all beam candidates are imperfect.\nIt may be beneficial to exploit the information\nthat some candidates are relatively better than the\nothers. We consider ranking them according to\nsome metrics w.r.t. the ground-truth sentences. For\nASR, the metric is WER, and for MT, the metric\nis BLEU score. We define xi,0 , xi and assume\nthat the candidates {xi,j}Kj=1 are sorted such that\nWER(xi,xi,j−1) < WER(xi,xi,j)\nfor ASR, and\nBLEU(xi,xi,j−1) > BLEU(xi,xi,j)\nfor MT. In other words, xi,j−1 has better quality\nthan xi,j .\nWe then enforce the “better” sentences to have\na score at least τ larger than those “worse” ones.\nThis leads to the following formulation,\nmin\nθ\nN∑\ni=1\nB−1∑\nj=0\nB∑\nk=j+1\nmax\n{\n0,\nτ − (log pθ(xi,j)− logθ(xi,k)),\n}\n.\n(4)\nCompared with LMLM formulation Eq. (3), the\nabove introduces more comparisons among the\ncandidates, and hence more computational cost\nduring training. We call this formulation ranking-\nloss-based LMLM (rLMLM).\nTo summarize this section, we have proposed\nLMLM and rLMLM that aim at discriminating be-\ntween hypotheses in a task-specific (e.g., WER or\nBLEU) sense, instead of minimizing PPL.\n4 Experiments on ASR\nWe apply the LMs trained under different criteria\nto rescore the beams in various ASR systems. In\nparticular, we are interested in knowing which of\nthe two training mechanisms is better: minimizing\nPPL (e.g., the RNNLM in Section 3.1), or fitting\nto the WER metric by the proposed methods.\nAdapting an RNNLM to a specific domain has\nbeen of interest, especially to the speech commu-\nnity (Park et al., 2010; Chen et al., 2015; Ma et al.,\n2017). We adopt Ma et al. (2017) that fine-tune\nthe softmax layer of RNNLM by minimizing the\nPPL on the text labels of training set. According\nto Ma et al. (2017), the reason not to fine-tune all\nthe layers is due to the limited text labels in the tar-\nget domain. Indeed, we also observe overfitting if\nadapting all layers, but adapting only the softmax\nlayer effectively decreases the PPL on the text la-\nbels of dev sets. We refer to this fine-tuning as\nRNNLM-adapted in the following sections.\nTo make a fair comparison with the adapted\nmodel, we also use the RNNLM as an initializa-\ntion for our LMLM and rLMLM. In total, there\nare four language models for rescoring the beams.\nRNNLM and its adapted version that aim at reduc-\ning PPL; and the two proposed methods, LMLM\nand rLMLM that try to fit to WER.\n4.1 WSJ Dataset\nThe WSJ corpora consists of about 80 hours of\nread speech with texts drawn from a machine-\nreadable corpus of Wall Street Journal news.\nWe use the standard configuration of train si284\ndataset for training, dev93 for development and\neval92 for testing.\nOur ASR model has one convolution layer, fol-\nlowed by 5 bidirectional RNNs and one fully con-\nnected layer, with a CTC loss on top. The text\nlabels of the training set are used to train a 4-gram\nlanguage model, which is employed in the ASR\ndecoder. The beam search decoder has a beam\nwidth of 2000. Before beam rescoring, this ASR\nsystem achieves a WER of 12.16 on dev93 set and\n7.69 on eval92 set. To put this into perspective,\nwe list some previous state-of-the-art system in\nTab. 2. Compared with them, our baseline is al-\nready very competitive.\n4.1.1 WERs and PPLs\nThe out-of-vocabulary rate of WSJ text is only\n0.28%, making the RNNLM reasonable to use.\nWe apply the RNNLM, RNNLM-adapted (Ma\net al., 2017), LMLM and rLMLM to rescore the\nbeams on dev and test set. The final score assigned\nto a beam is a weighted sum of the ASR and lan-\nguage model scores. The weight is found by min-\nimizing the WER on the dev set.\nTab. 3 reports the WERs on dev93 and eval92\nsets. All methods reduce the WER over the\nbaseline without rescoring. However, LMLM\nand rLMLM are notably better than the other\ntwo methods. Moreover, although RNNLM and\nRNNLM-adapted achieve smaller PPLs on the text\nlabels, the advantage does not transfer to WER.\nASR Models\nWER\ndev93 eval92\nEESEN (Miao et al., 2015) N/A 7.34\nAttention (Bahdanau et al., 2016) N/A 9.30\nGram-CTC (Liu et al., 2017) N/A 6.75\n5-layer Bidi-RNNs (baseline) 12.16 7.69\nTable 2: Published WERs on WSJ dev93 and\neval92 set\nrescoring WER PPL\nlanguage model dev93 eval92 dev93 eval92\nbaseline\n12.16 7.69 N/A N/A\n(no rescore)\nRNNLM 10.71 6.59 207.43 205.00\nRNNLM-adapted 10.11 6.34 159.50 157.85\nLMLM 9.44 5.56 575.83 563.69\nrLMLM 9.63 5.48 345.60 348.32\nTable 3: Rescore 2000-best list of WSJ dev93 and\neval92 set. Digits in bold are the best and ital-\nics are the runner-ups. Lower PPL does not corre-\nspond to lower WER.\n4.1.2 Correlation between scores and WERs\nTo better understand the proposed methods, we\ncalculate the correlation coefficients between the\nhypotheses’ WERs and their scores (by different\nlanguage models). In specific, for every utterance\nin the test set, we have a set of beam candidates,\ntheir word level accuracies (100-WER) and scores\ngiven by an LM, from which a Pearson correla-\ntion coefficient can be calculated. We calculate\nthe coefficients for all the utterances in the test set,\nand boxplot these coefficients in Fig. 3. The cor-\nrelation coefficients by LMLM and rLMLM tend\nRNNLM adapted LMLM rLMLM\n0.0\n0.2\n0.4\n0.6\n0.8\nFigure 3: Correlation coefficients between word\nlevel accuracy (1 − WER/100) and LM-scores\nby the different LMs, higher is better. Red hori-\nzontal lines are medians. Green dots are means.\nWhiskers are 5% and 95% quantiles. Lower and\nupper box boundaries are 25% and 75% quantiles.\nto be higher than RNNLM and RNNLM-adapted.\nThis indicates that LMLM and rLMLM are more\naligned with the goal of reducing WER.\n4.1.3 Case Study\nTab. 4 posts some examples from the test set. The\nfirst column lists the ground-truth labels, and their\ncorresponding best candidates as re-ranked by the\nfour LMs (see notes in the second column). Words\nin red are mistakes made by the candidate sen-\ntences. Scores of these sentences are listed in the\nlast four columns. We have the following observa-\ntions:\n1. LMLM and rLMLM give worse scores\non the ground-truth labels than RNNLM\nand RNNLM-adapted, which explains their\nhigher PPL in Tab. 3.\n2. In the first example, RNNLM and RNNLM-\nadapted assign higher scores to a shorter sen-\ntence. This is reasonable (though not neces-\nsarily desirable) as LM-score is a summation\nof log-probabilities, each of which is nega-\ntive. In contrast, LMLM and rLMLM are\nable to assign higher scores to longer and bet-\nter candidates.\n3. In the other two examples, LMLM and\nrLMLM seem to favor more sensible sen-\ntences, though they are not more grammatical\nthan those picked by RNNLM and RNNLM-\nadapted. We conjecture that since LMLM\nand rLMLM utilize beam candidates in their\ntraining, they capture and compensate for\nreferences and hypotheses reference orranked 1st by\nLM-score\nRNNLM RNNLM-adapted LMLM rLMLM\nfor such group rate coverage employers can charge the\nformer workers and their families the average cost of providing\nthe health benefits plus a two percent administrative fee\nreference,\nLMLM -144.74 -142.90 -162.61 -165.93\nfor such group rate coverage employers can charge the\nformer workers and their families the average cost of providing\nthe health benefits plus two percent administrative\nRNNLM,\nRNNLM-adapted -144.53 -142.85 -172.66 -168.10\nfor such group rate coverage employers can charge the\nformer workers and their families the average cost of providing\ntheir health benefits plus a two percent administrative fee\nrLMLM -146.72 -145.67 -163.73 -162.92\nwe’d like to see something that leads to real democracy\nsays jaime bonilla vice secretary general reference -105.8 -106.28 -122.93 -108.42\nwe’d like to see something that leads the real democracy\nsays jm bonier vice secretary general\nRNNLM,\nRNNLM-adapted -103.13 -102.84 -141.94 -118.38\nwe’d like to see something that leads to real democracy\nsays jim bone vice secretary general LMLM, rLMLM -104.52 -105.37 -125.40 -104.73\nthe big shoe is going to drop when we see the trade number reference -64.28 -61.04 -80.63 -82.91\nthe big she was going to drop in to see the trade numbers RNNLM,RNNLM-adapted -64.32 -61.68 -94.26 -86.89\nthe big shoe is going to drop in see the trade number LMLM, rLMLM -67.53 -64.50 -84.09 -84.65\nTable 4: Some “gold” references and best hypotheses (after rescoring by different language models) for\neval92 set. In red are errors or missing word (denoted as ‘ ’).\nsome weakness in the ASR, which is not\nachieved by RNNLM and RNNLM-adapted.\n4.2 10K Speech Dataset\nWe further validate our methods on a larger noisy\ndataset collected by Liu et al. (2017). The dataset\nhas about 10K hours of spontaneous speech. The\nutterances are corrupted by background noise, and\na large portion of them are accented. Therefore it\nis much more challenging than WSJ. We adopt the\nsame training-dev-test split as in Liu et al. (2017).\nIn specific, there are 5.4M utterances for training,\n2,066 for development and 2,054 for testing.\nrescoring WER PPL\nlanguage model dev test dev test\nbaseline\n19.17 20.90 N/A N/A\n(no rescore)\nRNNLM 18.38 20.07 264.21 252.85\nRNNLM-adapted 18.29 20.03 236.74 226.22\nLMLM 18.17 19.62 2250.79 2095.39\nrLMLM 17.98 19.49 1225.04 1152.63\nTable 5: Rescore 2000-best list of our internal dev\nand test set. Digits in bold are the best and italics\nare the runner-ups.\nThe ASR we build has the same architecture as\nin Liu et al. (2017), except that its decoder inte-\ngrates an in-domain 5-gram language model. This\nsystem achieves a WER of 19.17 on dev set, bet-\nter than the reported 19.77 baseline in Liu et al.\n(2017). Based on the ASR, we repeat the same\nexperiments in section 4.1. Tab. 5 reports WERs\nand PPLs on dev and test sets. Both LMLM and\nrLMLM outperform the other methods in WER,\nalthough their PPLs are higher. This trend is simi-\nlar to that in Tab. 3.\n5 Experiments on NMT\nIn this section, we experiment the large-margin\ncriterion trained LM with a competitive Chinese-\nto-English NMT system. The NMT model is\ntrained from 2M parallel sentence pairs. Follow-\ning Shen et al. (2016), we use NIST 06 newswire\nportion (616 sentences) for development and NIST\n08 newswire portion (691 sentences) for testing.\nWe use OpenNMT-py2 package with the default\nconfiguration to train the model: batch size is 64;\nword embedding size is 500; dropout rate is 0.3;\ntarget vocabulary size is 50K; number of epochs is\n20, after which a minimum dev perplexity of 7.72\n2https://github.com/OpenNMT/OpenNMT-py\nis achieved.\n5.1 BLEUs and PPLs\nWe use a beam size of 10 for decoding, and re-\nport case-insensitive 4-reference BLEU-4 scores\n(by calling “multi bleu.perl”3). The NMT model\nachieves 35.18 BLEU score on dev set and 31.52\non test set (see table 6). To put this into per-\nspective, Shen et al. (2016) trains their models\non 2.56M pairs of sentences and reports a dev\nBLEU score of 32.7 (via MOSES) or 30.7 (via\nRNNsearch, beam size of 10). So our NMT model\nis already very competitive.\nTo construct the training data for LMLM and\nrLMLM, 10 beam candidates are extracted for ev-\nery sentence in the training set. We then fol-\nlow the same experimental steps outlined in sec-\ntion 4.1, except that the ASR score is now changed\nto NMT score. In addition, we also find that nor-\nmalizing the LM score by sentence length can\nimprove the re-scoring performance substantially.\nTab. 6 compares the BLEU score after re-ranking\nby the different LMs. LMLM and rLMLM both\nimprove upon the baseline significantly, and out-\nperform RNNLM and RNNLM-adapted by a no-\ntable margin. We also observe that the PPLs of\nLMLM and rLMLM are much larger than those\nof RNNLM and RNNLM-adapted, suggesting that\nthe PPL metric may be very poorly correlated with\nBLEU.\nInterestingly, RNNLM-adapted does not show\nany gain in BLEU score over RNNLM. To under-\nstand this, we recall that NMT is trained by min-\nimizing PPL on target text. Its decoder is implic-\nitly an RNNLM on target language. We conjecture\nthat adapting an LM to the target domain can only\nduplicate the functionality of the NMT decoder,\nwhich does not bring any additional benefit.\n5.2 Correlation between scores and BLEUs\nWe measure the correlation between the LM\nscores and BLUEs. The calculation is done on\ndev06 set in the same way as Section 4.1.2, but\nnow we change the WERs to BLEUs. The box-\nplot of the correlation coefficients are shown in\nFig. 4. Compared with the boxplot in Fig. 3,\nnow the correlation coefficients by all LMs are\nmore dispersed. Sometimes, they even take neg-\native values. The mean correlation by LMLM\n3https://github.com/OpenNMT/\nOpenNMT-py/blob/master/tools/multi-bleu.\nperl\nrescoring BLEU PPL\nlanguage model dev06 test08 dev06 test08\nbaseline\n35.18 31.52 N/A N/A\n(no rescore)\nRNNLM 36.17 32.17 129.91 137.25\nRNNLM-adapted 36.17 31.97 78.20 89.27\nLMLM 37.79 33.11 7.75e5 3.73e6\nrLMLM 37.82 33.13 2.68e5 1.12e6\nTable 6: Rescore 10-best list for dev (nist 06) and\ntest (nist 08) set. Digits in bold are the best and\nitalics are the runner-ups.\nRNNLM adapted LMLM rLMLM\n−1.0\n−0.5\n0.0\n0.5\n1.0\nFigure 4: Correlation coefficients between BLEUs\nand LM-scores by the different LMs, higher is\nbetter. Red horizontal lines are medians. Green\ndots are means. Whiskers are 5% and 95% quan-\ntiles. Lower and upper box boundaries are 25%\nand 75% quantiles.\nand rLMLM, however, is considerably higher than\nthose by RNNLM and RNNLM-adapted.\n6 Related Work\n“Language modeling is an art of determining the\nprobability of a sequence of words” (Goodman,\n2001). In the past decades, there has been a trend\nof increasing the context that an LM can condition\non. N-gram models (Chen and Goodman, 1996)\nassume that each symbol depends on the previ-\nous N − 1 symbols. Feed forward neural network\nbased LMs (Bengio et al., 2003) are not count\nbased but they inherit the restrictive assumption.\nTo model longer-term dependencies, RNNLMs\n(Mikolov et al., 2010) are proposed. RNNLMs of-\nten achieve smaller PPLs than the N-gram coun-\nterparts (Sundermeyer et al., 2012; Zaremba et al.,\n2014; Jozefowicz et al., 2016). This paper focuses\non RNNLM-type architectures.\nWhile these works all adopt PPL as the metric\nto optimize, sometimes one may optimize a task-\nspecific objective. For example, Kuo et al. (2002);\nRoark et al. (2007) and Dikic et al. (2013) pro-\npose discriminative LMs to improve speech recog-\nnition. The common methodology therein is to\nfit a probabilistic model, e.g., conditional random\nfield (Roark et al., 2004), to the space of text can-\ndidates, and maximize the probability at the de-\nsired candidate. The problem is often solved by\nperceptron algorithm. However, these methods all\nrely on ad-hoc choice of features, e.g., counts of\nn-grams where n varies in a small range (e.g.,1\nto 3). Moreover, it is also not clear how these\nmethods would take advantage of an existing lan-\nguage model (trained on large unsupervised cor-\npus). Nevertheless, the same methodology can be\nextended to RNNLMs, thus avoiding the afore-\nmentioned limitations. For example, Auli and Gao\n(2014) train an RNNLM by favoring sentences\nwith high BLEU scores and integrate it into a\nphrase-based MT decoder.\nIf we cast the problem of picking the best text\nsequence as a ranking problem, the aforemen-\ntioned works can be considered as “pointwise”\nlearning-to-rank approaches (Cossock and Zhang,\n2008). In contrast, the proposed method is a “pair-\nwise” approach (Liu, 2009), as it learns a neural\nlanguage model by comparison between pairs of\nsentences. Earlier works in this fashion may date\nback to (Collins and Koo, 2005), which improves\na semantic parser. Learning “by pairwise compari-\nson” is also seen in several MT literatures. For ex-\nample, Hopkins and May (2011) propose to train\na phrase-based MT system by minimizing a pair-\nwise ranking loss. Wiseman and Rush (2016) op-\ntimize the beam search process in a Neural Ma-\nchine Translation (NMT) system. They enforce\nthe score of a reference to be higher than that of\nits decoded k-th candidate by at least a unit mar-\ngin.\nRather than optimizing the MT system itself,\nthis work proposes a general method of training\nrecurrent neural language models, which can ben-\nefit various text generation tasks, including speech\nrecognition and machine translation.\n7 Conclusions\nWe have proposed a large margin criterion for\ntraining recurrent neural language models. Rather\nthan minimizing PPL, the proposed criterion\nis based on comparison between pairs of sen-\ntences. We have formulated two algorithms\nthat implement the training criterion. One com-\npares between references and imperfect hypothe-\nses (LMLM), the other compares between all pairs\nof hypotheses (rLMLM). We applied the language\nmodels trained by these two algorithms to speech\nrecognition and machine translation. Both of\nthem demonstrate superior performance over their\nminimum-PPL counterparts. However, the perfor-\nmance gain from LMLM to rLMLM is small, al-\nthough rLMLM is built on more pairwise compar-\nisons and requires more training efforts. The ef-\nficiency with respect to the number of pairs is a\nfuture research topic.\nReferences\nDario Amodei, Sundaram Ananthanarayanan, Rishita\nAnubhai, Jingliang Bai, Eric Battenberg, Carl Case,\nand Jared Casper et al. 2016. Deep speech 2: End-\nto-end speech recognition in english and mandarin.\nIn International Conference on Machine Learning.\nMichael Auli and Jianfeng Gao. 2014. Decoder inte-\ngration and expected bleu training for recurrent neu-\nral network language models. In ACL.\nDzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk,\nPhilemon Brakel, and Yoshua Bengio. 2016. End-\nto-end attention-based large vocabulary speech\nrecognition. In IEEE International Conference on\nAcoustics, Speech and Signal Processing.\nYoshua Bengio, Re´jean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of machine learning research,\n3(Feb):1137–1155.\nStanley F. Chen and Joshua Goodman. 1996. An\nempirical study of smoothing techniques for lan-\nguage modeling. Computer Speech & Language,\n13(4):359–394.\nXie Chen, Tian Tan, Xunying Liu, Pierre Lanchantin,\nMoquan Wan, Mark J.F. Gales, and Philip C. Wood-\nland. 2015. Recurrent neural network language\nmodel adaptation for multi-genre broadcast speech\nrecognition. In 16th Annual Conference of the In-\nternational Speech Communication Association.\nMichael Collins and Terry Koo. 2005. Discriminative\nreranking for natural language parsing. Computa-\ntional Linguistics, 31(1):25–70.\nDavid Cossock and Tong Zhang. 2008. Statistical anal-\nysis of bayes optimal subset ranking. IEEE Trans-\nactions on Information Theory, 54(11):5140–5154.\nErinc¸ Dikic, Murat Semerci, Murat Sarac¸lar, and\nEthem Alpaydin. 2013. Classification and ranking\napproaches to discriminative language modeling for\nasr. IEEE Transactions on Audio, Speech, and Lan-\nguage Processing, 21(2):291–300.\nJoshua T. Goodman. 2001. A bit of progress in lan-\nguage modeling. Computer Speech & Language,\n15(4):403–434.\nAlex Graves, Santiago Ferna´ndez, Faustino Gomez,\nand Ju¨rgen Schmidhuber. 2006. Connectionist\ntemporal classification: labelling unsegmented se-\nquence data with recurrent neural networks. In In\nProceedings of the 23rd international conference on\nMachine learning.\nCaglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2015. On us-\ning monolingual corpora in neural machine transla-\ntion. arXiv preprint arXiv:1503.03535.\nMark Hopkins and Jonathan May. 2011. Tuning as\nranking. In EMNLP.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring\nthe limits of language modeling. arXiv preprint\narXiv:1602.02410.\nPhilipp Koehn. 2009. Statistical machine translation.\nCambridge University Press.\nHong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang,\nand Chin-Hui Lee. 2002. Discriminative training of\nlanguage models for speech recognition in acoustics.\nIn IEEE International Conference on Speech, and\nSignal Processing (ICASSP).\nZhifei Li and Sanjeev Khudanpur. 2008. Large-scale\ndiscriminative n-gram language models for statisti-\ncal machine translation. In AMTA.\nHairong Liu, Zhenyao Zhu, Xiangang Li, and Sanjeev\nSatheesh. 2017. Gram-ctc: Automatic unit selection\nand target decomposition for sequence labelling. In\n34th International Conference on Machine Learn-\ning.\nTie-Yan Liu. 2009. Learning to rank for information\nretrieval, volume 3. Foundations and Trends R© in\nInformation Retrieval.\nMin Ma, Michael Nirschl, Fadi Biadsy, and Shankar\nKumar. 2017. Approaches for neural-network lan-\nguage model adaptation. In Proceedings of Inter-\nspeech.\nYajie Miao, Mohammad Gowayyed, and Florian\nMetze. 2015. Eesen: End-to-end speech recogni-\ntion using deep rnn models and wfst-based decod-\ning, pages 167–174. ieee, 2015. In Workshop on\nAutomatic Speech Recognition and Understanding\n(ASRU).\nTomas Mikolov, Martin Karafia´t, Lukas Burget, Jan\nCˇernocky´, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In 11th An-\nnual Conference of the International Speech Com-\nmunication Association.\nJunho Park, Xunying Liu, Mark J. F. Gales, and Phil C.\nWoodland. 2010. Improved neural network based\nlanguage modelling and adaptation. In 11th Annual\nConference of the International Speech Communi-\ncation Association.\nBrian Roark, Murat Saraclar, and Michael Collins.\n2007. Discriminative n-gram language modeling.\nComputer Speech & Language, 21(2):373–392.\nBrian Roark, Murat Saraclar, and Michael\nCollins Mark Johnson. 2004. Discriminative\nlanguage modeling with conditional random\nfields and the perceptron algorithm. In 42nd\nAnnual Meeting on Association for Computational\nLinguistics.\nShiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua\nWu, Maosong Sun, and Yang Liu. 2016. Minimum\nrisk training for neural machine translation. In ACL.\nAnuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and\nAdam Coates. 2017. Cold fusion: Training seq2seq\nmodels together with language models. arXiv\npreprint arXiv:1708.06426.\nMartin Sundermeyer, Ralf Schlu¨ter, and Hermann Ney.\n2012. Lstm neural networks for language model-\ning. In 13th Annual Conference of the International\nSpeech Communication Association.\nYuuki Tachioka and Shinji Watanabe. 2015. Discrim-\ninative method for recurrent neural network lan-\nguage models. In IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP).\nSam Wiseman and Alexander M. Rush. 2016.\nSequence-to-sequence learning as beam-search op-\ntimization. In EMNLP.\nDong Yu and Li Deng. 2014. Automatic Speech\nRecognition: A Deep Learning Approach. Springer\nPublish-ing Company.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329.\n",
    "Link": "http://arxiv.org/abs/1808.08987"
}