{
    "Title": "MaLLaM -- Malaysia Large Language Model",
    "Authors": "Adha, Kamarul, Nazhan, Ariff, Razak, Aisyah, Zolkepli, Husein",
    "Year": "No year available",
    "Abstract": "Addressing the gap in Large Language Model pretrained from scratch with\nMalaysian context, We trained models with 1.1 billion, 3 billion, and 5 billion\nparameters on a substantial 349GB dataset, equivalent to 90 billion tokens\nbased on our pretrained Byte Pair Encoding (BPE) tokenizer for a single epoch.\nMaLLaM contributes to enhanced natural language understanding and generation\ntasks in the Malay language. Although trained on a smaller dataset of 90\nbillion tokens, our instruction-tuned MaLLaM models perform competitively. When\ncompared to ChatGPT3.5 and Malaysian Mistral, MaLLaM's instruction-tuned models\ndemonstrate notable proficiency, underscoring the effectiveness of our approach\nin capturing and understanding the nuances of the Malaysian language. MaLLaM\nmodels mark a significant contribution to the field, providing comprehensive\nlanguage representations grounded in Malaysian context. This endeavor aims to\npave the way for enhanced natural language understanding and generation tasks\nspecific to the linguistic nuances present in Malaysia. We discuss the training\nmethodology, dataset composition, and the potential impact of MaLLaM in\nadvancing the capabilities of large language models within the context of the\nMalay language.\n  All models released at\nhttps://huggingface.co/collections/mesolitica/mallam-6577b59d1e0b436ae75f930",
    "Keywords": "No keywords available",
    "Publisher": "",
    "Publication Date": "No publication date available",
    "Journal": "No journal available",
    "Citation Count": 0,
    "Full Text": "MaLLaM - Malaysia Large Language ModelHusein Zolkepli∗ Aisyah Razak† Kamarul Adha‡ Ariff Nazhan§AbstractAddressing the gap in Large Language Model pretrained from scratch withMalaysian context, We trained models with 1.1 billion, 3 billion, and 5 billionparameters on a substantial 349GB dataset, equivalent to 90 billion tokens based onour pretrained Byte Pair Encoding (BPE) tokenizer for a single epoch. MaLLaMcontributes to enhanced natural language understanding and generation tasks in theMalay language.Although trained on a smaller dataset of 90 billion tokens, our instruction-tunedMaLLaM models perform competitively. When compared to ChatGPT3.5 andMalaysian Mistral, MaLLaM’s instruction-tuned models demonstrate notable profi-ciency, underscoring the effectiveness of our approach in capturing and understand-ing the nuances of the Malaysian language.MaLLaM models mark a significant contribution to the field, providing compre-hensive language representations grounded in Malaysian context. This endeavoraims to pave the way for enhanced natural language understanding and generationtasks specific to the linguistic nuances present in Malaysia. We discuss the trainingmethodology, dataset composition, and the potential impact of MaLLaM in ad-vancing the capabilities of large language models within the context of the Malaylanguage.All models released at HuggingFace Mesolitica MaLLaM Collection.1 IntroductionThe landscape of large language models (LLMs) has predominantly been shaped by models trainedon English, with subsequent adaptations and fine-tunings for languages like Tamil and Malaysian.Existing models such as Tamil-LLama [1] and Malaysian-Mistral [2] have emerged as valuable assets,leveraging the foundation of English-based LLMs for further optimization in non-English languages.However, despite their commendable contributions, these models still carry traces of English nuances,presenting a unique challenge in achieving a fully language-specific representation.This introduction underscores the significance of mitigating residual English influences in languagemodels tailored for specific linguistic contexts. While the adaptation of English-based LLMs hasfacilitated advancements in various languages, it falls short of capturing the intricacies and nuancesunique to languages like Tamil and Malaysian. To address this gap, we embark on a novel approachby pre-training a large language model entirely from scratch, with a specific focus on the Malaysianlanguage. This deliberate endeavor aims to establish a model that is inherently attuned to thelinguistic subtleties and idiosyncrasies of Malaysian, overcoming the limitations posed by modelswith English-centric origins.The presence of such inherent English biases becomes particularly pertinent when envisioning thecreation of a dedicated large language model for Malaysian linguistic contexts. The prevailing∗husein@mesolitica.com†aisyahrazak171@gmail.com‡kamarul.adha360@gmail.com§ariffnzhn@gmail.comTechnical Report.arXiv:2401.14680v2  [cs.CL]  29 Jan 2024influence of English-centric sources, prevalent in public English news and articles, poses a potentialchallenge to the development of a truly indigenous language model. The need for a linguistic modeldevoid of these biases is underscored by the desire to accurately capture the unique nuances andintricacies of the Malaysian language.In light of these considerations, our initiative introduces MaLLaM, a large language model specificallypre-trained from scratch using a robust dataset equivalent to 90 billion tokens sourced from Malaysiancontexts. The distinctiveness of MaLLaM lies in its genesis, free from the shadow of English-centricbiases pervasive in existing language models. By cultivating a language model that is inherentlyattuned to Malaysian linguistic idiosyncrasies, we aim to address the gap left by models that, despitebeing fine-tuned for Malaysian languages, retain traces of English nuances. MaLLaM stands as atestament to our commitment to fostering linguistic authenticity and overcoming the challenges posedby the dominance of English-centric biases in current language modeling paradigms.• Pre-training MaLLaM: We utilized a powerful infrastructure consisting of 10 nodes of theStandard_ND96amsr_A100_v4 Azure instance, with each node featuring 8 A100 80 GPUs.This configuration efficiently facilitated the pre-training of language models with 1.1 billion,3 billion, and 5 billion parameters on a substantial 349GB dataset of Malaysian texts.• Multi-turn Instruction-Tuned MaLLaM: To ensure a seamless and meaningful compar-ison, we opted to employ the exact chat instruction dataset from Malaysian Mistral [2].This approach enables us to replicate the same experimental setup, facilitating a direct andaccurate comparison of our work with the existing model.2 Pre-Training Data2.1 Public DataMalay Wikipedia, we incorporated the Malay Wikipedia dump, enriching the linguistic diversity bypartially converting it into Jawi script. Utilizing the Ejawi converter, we ensured that the model isnot only proficient in understanding the standard Malay script but also adept at comprehending theintricacies of Jawi.Malay Language study articles, We enriched our dataset with content from the Malay dictionaryand public articles from Dewan Bahasa Pustaka. This inclusion ensures our language model iswell-versed in word meanings, usages, and various linguistic styles prevalent in Malaysian literature.Malaysia Government public documents, We included public government documents from theofficial Malaysia government website and Google searches to enrich our dataset. This ensures that ourlanguage model is exposed to formal language and communication styles used in official governmentcontexts, enhancing its ability to understand and generate text relevant to administrative, legal, andpolicy domains in Malaysia.Malaysian public articles, We gathered a varied dataset by scraping articles from Malaysian sources.This process exposed our language model to different topics, writing styles, and language nuancesfound in Malaysian articles, contributing to a model well-versed in the Malaysian language acrossdiverse domains.Malaysian public social media, We gathered diverse data by scraping content from specific Facebookpages, filtering tweets based on location and keywords, and extracting information from platformslike c.cari.com.my, b.cari.com.my, carigold, lowyat, and transcripts of Malaysian YouTube videos.This approach broadens our dataset to include various language styles and colloquial expressionsfrom online Malaysian communities.Malaysia public journals, We added content from reputable Malaysian journals like https://mjpharm.org, https://myjgeosc.com, and https://www.akademisains.gov.my to ourdataset. This ensures our language model is familiar with formal and technical language usedin academic contexts, covering a variety of subjects.Malaysian related public research papers, We refined our dataset by filtering CrossRef usingkeywords like ’malaysia,’ ’malay,’ and ’melayu.’ This targeted approach ensures that our languagemodel is exposed to scholarly literature closely tied to Malaysia.Complete list of gathered data at Github Project - Prepare LLM dataset.22.2 Coding DataIncorporating a coding dataset is a crucial component of our diverse training data. For this purpose, weutilized the original dataset available at bigcode/the-stack-dedup. To ensure relevance and efficiency,we selectively picked specific programming languages, including Python, Julia, C, C++, HTML,CSS, JavaScript, Go, Rust, Java, SQL, Markdown, R, Dockerfile, Ruby, Typescript, and YAML. Tomanage the dataset size, each programming language was limited to a maximum of 10GB.2.3 Instruction-tuned DataTo augment the comprehensiveness of our training dataset, we devised a synthetic Malay instructiondataset. This encompassed diverse linguistic aspects, including the conversion between Rumi and Jawiscripts, dependency and constituency parsing, grammatical error generation (kesalahan tatabahasa),and coding instruction datasets related to various programming paradigms. Additionally, we includedinstructional content relevant to educational levels such as UPSR, PT3, and SPM, providing a broadcoverage of language proficiency levels. The incorporation of syntactic and semantic elements,coupled with coding instructions, contributes to a well-rounded language model capable of handlinga diverse array of linguistic tasks and understanding instructions across different domains.Complete list of Instruction-tuned Data at instruction-dataset.3 Deduplicating and Postprocessing DataWe removed duplicate entries from our public data from 2.1 using the MinHash implementationfrom https://github.com/ChenghaoMou/text-dedup.We configured the MinHash algorithm with the following parameters:Parameter Valuenum_perm 256threshold 0.95hash_func sha1hash_bits 64Complete deduplicating data implementation at here. All deduped dataset published at malaysia-ai/dedup-text-dataset.After removing duplicates, we employed the postprocessing technique mentioned in MalaysianMistral section 3.3 [2].4 Pre-Training TokenizerTo ensure an efficient and versatile tokenizer, we conducted pretraining on a BPE (Byte Pair Encoding)tokenizer using diverse datasets, including Malay Wikipedia, synthetic Jawi, public articles, translatedcode instructions, Google-translated Tamil and Google-translated Punjabi. The objective was todevelop a tokenizer capable of handling longer subwords for languages such as Malay, Mandarin,Tamil, Jawi, English, and Arabic. The decision to use BPE was motivated by certain limitationsobserved in SentencePiece, where newline characters caused issues, some Tamil and Jawi characterswere missing, and the processing speed for very long texts was suboptimal. By opting for BPE, weaddressed these challenges, ensuring a robust and efficient tokenizer for our language model training.The BPE tokenizer was trained on a deduplicated text dataset of 85GB, with a vocabulary size of32,000.The decision to train our own BPE tokenizer was driven by the goal of minimizing token sizes duringboth input and output. To illustrate the efficiency gained, we conducted a comparison using theMalaysian Ultrachat AstroAwani dataset. Our pre-trained BPE tokenizer achieved a notable reductionof up to 43% in token size when compared to tokenizers employed by Llama2 and Mistral. Thisreduction in token size enhances the efficiency and resource optimization of our language model,offering advantages in terms of computational performance and memory usage during processing.3Complete pre-training tokenizer implementation at here5 Tokenizing DataOur dataset comprises a substantial 349GB of text in JSONL format, amounting to 90 billion tokens.The following table breaks down the distribution of token sizes within the dataset:Distribution Tokens (B)deduped text Dataset 31.7Filtered StarCoder [3] 40.98Unfiltered MS Madlad 400 [4] 14.98Instruction-tuned Dataset 1.58Malaysia journals and research papers 1.14To optimize data processing efficiency, we use MosaicML Streaming library [5], by using hashingtechnique. Recognizing the complexity of our dataset, we implemented data distribution processingto transform it into the MosaicML streaming format,This involves splitting the original JSONL file into smaller ones, and each of these smaller filesundergoes multiprocessing for conversion into the Mosaic format. Subsequently, these smaller Mosaicfiles are merged into a single Mosaic file. It’s worth noting that MosaicML streaming accesses onefolder at a time, necessitating the consolidation of smaller files into a unified format for seamlesstraining data access.6 Infrastructure SetupFor infrastructure deployment and efficient node management, we opted for Azure Kubernetes Service(AKS). This choice allows us to manage nodes effortlessly through YAML configurations, ensuringa smooth and standardized deployment process. Leveraging Azure Kubernetes provides benefitssuch as improved internal networking and the convenience of attaching remote storage using theWriteManyAccess approach. With everything configured through YAML files, our deployment onAzure Kubernetes ensures a well-organized and easily manageable infrastructure for our languagemodel training. Below is the figure of the infrastructure setup,We use NFS with WriteManyAccess to centralize both the dataset and checkpoint directory. Thissimplifies the management of these components, making it easy to access them from multiple nodes.The use of NFS ensures efficient loading of checkpoints from various nodes.4Our approach to training involves leveraging the Ray distributed framework [6] for multi-node andmulti-GPU setups. Ray provides a user-friendly setup process as only the Ray workers need toconnect to the Ray master. Following this straightforward connection, developers can initiate thetraining session with ease by simply calling the Ray master. This simplicity in setup and executionstreamlines the training process, allowing for efficient utilization of multiple nodes and GPUs toenhance the scalability and performance of our language model.Our Ray cluster setup to efficiently manage a total of 80 A100 80GB GPUs. Specifically, 40 GPUsare allocated for training the 5-billion-parameter model, while 20 GPUs each are dedicatedto the 1.1-billion and 3-billion-parameter models. The concurrent training of all models, eachwith distinct parameter sizes, is facilitated by the Ray cluster, enabling a synchronized and optimizedtraining process.All deployment manifests at here.7 Pre-Training PhaseTo pretrain our language model, we employ a causal language model with the objective of minimizingcross-entropy,P (x1, x2, . . . , xT ) =T∏t=1P (xt|x1, x2, . . . , xt−1)The training is conducted on a context length of 4096 using Mistral architecture [7], and the hyperpa-rameters utilized in this process are detailed below,Hyperparameter ValueDeepSpeed ZeRO-3 OffloadBatch Size 24Learning Rate 1e-4Learning Rate scheduler WarmupDecayLRWarmup Steps 2000AdamW decay rate 0.1Precision bfloat16Complete pre-training 4096 context length implementation at here.8 Pre-Training HiccupThe occurrence detailed in the following paragraphs pertains specifically to the 5-billion-parametermodel, as evidenced by the graph below,5To address a stability concern during training, we temporarily reduced the learning rate by 30% andreverted to older checkpoints. Once stability was regained, we reverted back to the initial learningrateAdditionally, we encountered GPU failures during the training process. Restarting the Nvidia plugindaemonset proved effective in resolving the issue in most instances. However, in a particular occur-rence, a node restart became necessary, requiring a stop-and-start action within the Kubernetes nodethrough the Azure console to restore normal functioning. These challenges and the correspondingmitigation strategies underscore the complexities inherent in large-scale language model training in acloud provider.9 Pre-Training EvaluationWe evaluated our language model using the same tatabahasa test set from Malaysian Mistral [2] andcompared its performance with other existing pretrained models,Model Tatabahasa 0 shot Tatabahasa 1 shot Tatabahasa 3 shotsaisingapore/sealion3b 21.777 21.776 24.641aisingapore/sealion7b 25.788 27.507 26.074mesolitica/mallam-1.1B-4096 25.758 25.788 28.08mesolitica/mallam-3B-4096 24.567 24.641 28.653mesolitica/mallam-5B-4096 24.074 27.794 28.653We also compared with other models and published the benchmark at mesolitica/malay-llm-leaderboard.10 Supervised Instruction Fine-tuning ProcedureWe fine-tuned 1.1 billion and 5 billions parameters models with 20480 context length using aninstruction dataset from Malaysian Mistral [2], and use latest checkpoints from 7. Below are thehyperparameters we use during the fine-tuning process:Hyperparameter ValueDeepSpeed ZeRO-3 OffloadBatch Size 6Learning Rate constant 2e-5Complete fine-tuning 20480 context length implementation at here.11 Result FinetuningWe obtained our results using the fine-tuned model accessible at mesolitica/mallam-1.1b-20k-instructions-v2.11.1 Multiturn Malaysian context QA6User: KWSP tu apaAssistant: KWSP ialah sebuah syarikat yang menyediakan pelbagai perkhidmatankewangan kepada individu dan organisasi. Mereka menawarkan beberapa produkseperti akaun simpanan, pelaburan, insurans, pinjaman perumahan dan banyaklagi. Adakah terdapat sesuatu yang khusus yang anda ingin ketahui tentangKWSP?User: macam mana ko tahu semua niAssistant: Untuk mengetahui lebih lanjut mengenai KWSP, anda boleh merujuk lamanweb rasmi mereka di www.kwsp.gov.my atau menghubungi cawangan terdekat untukmendapatkan maklumat terkini. Selain itu, anda juga boleh mengikuti mediasosial mereka seperti Facebook, Twitter dan Instagram untuk mendapatkanperkembangan terkini dalam industri ini. Jika ada soalan lain, janganteragak-agak untuk bertanya!User: tak mahu, saya nak tanya awakAssistant: Tak perlu risau, saya sedia membantu dengan sebarang pertanyaanberkaitan KWSP. Apa sahaja yang anda perlukan, sila beritahu saya. Saya akancuba menjawab sebaik mungkin.11.2 Multiturn Coding QAUser: macam mana nak code sentiment python3Assistant: Untuk mengkodekan sentimen dalam Python 3, anda boleh menggunakanperpustakaan seperti nltk atau scikit-learn. Berikut adalah contoh bagaimanauntuk melakukan ini:‘‘‘pythonimport nltkfrom sklearn import svmfrom scipy.stats import norm# Contoh data latihan dan ujiandata = {’text’: [’Saya suka filem itu!’,’Filem itu sangat bagus!’],# tambahkan ciri lain di sini jika diperlukan}# Buat model SVM dengan algoritma Naive Bayesmodel_svm = svm.SVC(kernel=’linear’) # Gantikan’svm.SVC()’ dengan fungsi kernelyang sesuai# Latih model menggunakan data latihanmodel_svm.fit(data[’text’])# Ramalkan label bagi teks barunew_text = \"Ini adalah tajuk berita.\"predicted_label = model_svm.predict(new_text)print(\"Label ramalan:\", predicted_label)‘‘‘Dalam kod di atas, kami mengimport modul ‘nltk‘’ dari‘sklearn.feature_extraction.text‘ untuk mendapatkan ciri-ciri teks. Kemudian,kami membuat objek ‘SVC‘ dari ‘sklearn.naive_bayes‘ untuk melatih modelpengelasan binari pada set data yang diberikan. Akhirnya, kami memanggilkaedah predict pada objek ‘SVC‘ untuk meramalkan label bagi teks baruberdasarkan ciri-ciri yang telah dipelajari sebelum ini.User: ada contoh untuk tensorflow sentiment analysis?Assistant: Tentu saja! Berikut adalah contoh bagaimana menggunakan TensorFlowuntuk melakukan analisis sentimen pada dataset MNIST:‘‘‘pythonimport pandas as pdfrom sklearn.model_selection import train_test_split7from sklearn.feature_extraction.text import CountVectorizerfrom sklearn.metrics import accuracy_score# Baca dataset MNISTdf = pd.read_csv(’mnist_data.csv’) # Gantikan’mnist_data.csv’ dengan nama failCSV sebenarX = df.iloc[:, :-1].values # Ambil lajur pertama sebagai data latihany = df.iloc[:, -1].values # Ambil lajur kedua sebagai data ujian# Pra-proses datacount_vectorizer = CountVectorizer()X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)# Inisialisasi model vektorisasimodel = tf.keras.models.Sequential([CountVectorizer(), # Tambah lapisan vektor tambahantf.keras.layers.Embedding(len(y_train), 100, input_length=100), # Tambahlapisan embedding tambahantf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True)),# Tambah lapisan Bidirectional tambahantf.keras.layers.Dense(10, activation=\"softmax\") # Tambah lapisan output tambahan])# Kompilasi modelmodel.compile(optimizer=’adam’, loss=’categorical_crossentropy’,metrics=[accuracy_score])# Latih modelmodel.fit(X_train, y_train, epochs=50, batch_size=32)# Nilai prestasi modelloss, accuracy = model.evaluate(X_test, y_test)print(\"Akurasi:\", accuracy)‘‘‘Pastikan anda menggantikan ’mnist_data.csv’ dengan lokasi dan nama fail CSVsebenar yang mengandungi dataset MNIST. Kod ini membaca dataset MNIST,memprosesnya menjadi ciri berangka, melatih model rangkaian saraf, danmenilai prestasinya menggunakan set pengesahan. Hasil penilaian kemudiandicetak. Semoga membantu!Despite being trained on a relatively small coding dataset, the model has demonstrated a commendablecapability to generate satisfactory results. This highlights the model’s capacity to generalize andglean valuable insights even when exposed to limited coding examples. The effectiveness of themodel in producing meaningful outcomes with a smaller dataset underscores its adaptability andefficiency in handling coding-related tasks.11.3 Malay instructionUser: macam mana nak buat nasi ayam yg sedapAssistant: Untuk membuat Nasi Ayam yang lazat, anda boleh mengikutilangkah-langkah berikut:Bahan-bahan:200g daging ayam (dipotong kecil)1 biji bawang besar, dihiris halus2 ulas bawang putih, dikisar3 sudu besar rempah kari1 cawan air rebusan ayamGaram dan gula secukup rasaMinyak masak untuk menumisDaun ketumbar segar untuk hiasan8Bawang goreng untuk taburanSos cili atau sos tomato untuk dihidangkan bersama nasi panasCara penyediaan:Panaskan minyak dalam kuali dan tumis bawang besar hingga naik bau. Masukkanbawang putih dan kacau sehingga wangi.Masukkan rempah kari dan kacau rata supaya bahan menjadi pekat.Masukkan daging ayam yang telah dipotong kecil ke dalam kuali dan biarkan iamasak selama beberapa minit sehingga bertukar warna.Masukkan air rebusan ayam dan biarkan mendidih.Selepas itu, masukkan garam dan gula secukup rasa. Kacau lagi sehingga semuabahan bercampur dengan baik.Biarkan nasi sejuk sebelum dihidangkan.Selamat mencuba!12 AcknowledgementSpecial thanks to Malaysia-AI volunteers especially Wan Adzhar Faiq Adzlan, Ammar Azman, M.Amzar, Muhammad Farhan and Syafie Nizam for contributing dataset to train MaLLaM.We would like to express our gratitude to NVIDIA Inception for generously providing us with theopportunity to train our model on the Azure cloud. Their support has played a crucial role in thesuccess of our research, enabling us to leverage advanced technologies and computational resources.We extend our thanks to the wider research community for their valuable insights and collaborativediscussions, which have greatly influenced our work. This paper reflects the collective efforts andcontributions from both NVIDIA Inception and the broader research community.13 ConclusionIn conclusion, our open-source language model represents a significant stride toward bridging theresearch gap in Malaysia. By making our model accessible to the research community, we aimto contribute to the advancement of natural language processing in the Malaysian context. Theavailability of this model provides researchers, developers, and practitioners with a valuable resourceto explore and address linguistic challenges specific to Malaysia. We anticipate that the open-sourcenature of our model will foster collaboration and inspire further innovations, ultimately promoting aricher and more nuanced understanding of the Malaysian language landscape.References[1] Abhinand Balachandran. Tamil-llama: A new tamil language model based on llama 2, 2023.[2] Husein Zolkepli, Aisyah Razak, Kamarul Adha, and Ariff Nazhan. Large malaysian languagemodel based on mistral for enhanced local language understanding, 2024.[3] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, ChenghaoMou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii,Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, JoãoMonteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee,Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang,Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, MananDey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, SashaLuccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, NadavTimor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra,Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor,Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, SeanHughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may thesource be with you!, 2023.9[4] Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo,Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat.Madlad-400: A multilingual and document-level large audited dataset, 2023.[5] The Mosaic ML Team. streaming. <https://github.com/mosaicml/streaming/>, 2022.[6] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang,Melih Elibol, Zongheng Yang, William Paul, Michael I. Jordan, and Ion Stoica. Ray: A distributedframework for emerging ai applications, 2018.[7] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra SinghChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, LucileSaulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, ThibautLavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023.10",
    "Link": "http://arxiv.org/abs/2401.14680"
}