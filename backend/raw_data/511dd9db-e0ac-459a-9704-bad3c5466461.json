{
    "Title": "Towards Trustable Language Models: Investigating Information Quality of\n  Large Language Models",
    "Authors": "Rejeleene, Rick, Talburt, John, Xu, Xiaowei",
    "Year": "No year available",
    "Abstract": "Large language models (LLM) are generating information at a rapid pace,\nrequiring users to increasingly rely and trust the data. Despite remarkable\nadvances of LLM, Information generated by LLM is not completely trustworthy,\ndue to challenges in information quality. Specifically, integrity of\nInformation quality decreases due to unreliable, biased, tokenization during\npre-training of LLM. Moreover, due to decreased information quality issues, has\nled towards hallucination, fabricated information. Unreliable information can\nlead towards flawed decisions in businesses, which impacts economic activity.\nIn this work, we introduce novel mathematical information quality evaluation of\nLLM, we furthermore analyze and highlight information quality challenges,\nscaling laws to systematically scale language models.Comment: 31 page",
    "Keywords": "No keywords available",
    "Publisher": "",
    "Publication Date": "No publication date available",
    "Journal": "No journal available",
    "Citation Count": 0,
    "Full Text": "Towards Trustable Language Models: Investigating InformationQuality of Large Language ModelsRick Rejeleene Xiaowei Xurrejeleene@ualr.edu xuxu@ualr.eduDepartment of Information Science Department of Information ScienceUniversity of Arkansas University of ArkansasLittle Rock 72204 Little Rock 72204John Talburtjrtalburt@ualr.eduDepartment of Information ScienceUniversity of ArkansasLittle Rock 72204Abstract:Large language models (LLM) are generating information at a rapid pace, requiringusers to increasingly rely and trust the data. Despite remarkable advances of LLM,Information generated by LLM is not completely trustworthy, due to challenges ininformation quality. Specifically, integrity of Information quality decreases due tounreliable, biased, tokenization during pre-training of LLM. Moreover, due todecreased information quality issues, has led towards hallucination, fabricatedinformation. Unreliable information can lead towards flawed decisions in businesses,which impacts economic activity. In this work, we introduce novel mathematicalinformation quality evaluation of LLM, we furthermore analyze and highlightinformation quality challenges, scaling laws to systematically scale language models.Keywords: Language Models, Machine Learning, Attention-modules,Information-Quality1 IntroductionRecently, there’s been wide deployment of large language models in various fieldssuch as medicine, law, code generator, search, with chatbots being most prominent.In domains such as software engineering, productivity is improved due tocode-completion (Cruz-Benito et al. 2021) (Solaiman et al. 2019) (Moradi Dakhel etal. 2023), In writing such as grammar assistance, autocomplete, creative writing,poetry generation, question-answering systems.Large language models use conditional probability to generate sequences of tokensfor generating sentences. ChatGPT, a transformer based LLM, is creating excitementbeyond measure among researchers, engineers raising questions about trust anddata quality (Brants et al. n.d.). Transformers (Vaswani et al. 2017a) a languagemodel introduced has led towards state of the art natural language processing tasks.Attention mechanisms, which make the core of language models are playing anessential role as part of the architecture in language models (Bahdanau, Cho, andBengio 2014) (Hu 2020; Brauwers and Frasincar 2023). Attention Mechanisms enableLLM to recall, generate an accurate sequence of tokens due to their ability to focuson relevant parts of input sequence during generation of tokens. In this work, wedescribe trust, data quality of large language models, we describe issues aroundtrust, data-quality, and research directions for increasing trust of language models.In Summary, our contributions are as follows:● We propose a novel mathematical formulation of information quality usingconsistency, relevance and accuracy metric, thus introducing a pipeline forLLM to evaluate information quality in natural language processing● We explore importance of trust, data-quality for large language models, whichaffects economy, we highlight how unreliable data leads towards poorperformance of LLM, data quality issues such as diversity of training, bias,tokenization are needed in larger datasets for LLM● We postulate why quality of LLM is facing limitations, we find due to dataquality issues, and we find datasets such as general, specialized datasets areenabling LLM to improve performance in specific domains. We also explorescaling laws like Chinchilla and Broken Neural Scaling laws, which enableLLM to systematically scale language models● We find limitations of LLM such as hallucination, common sense reasoning,inconsistency, and we suggest research direction for improving LLM throughinvestigating theory and principles of LLM, reducing dependence on humanlabellers and improving data quality2. Why Information quality generated by LLM play a role in Economy?Trust plays a central role in economic transactions, for the majority of professions inbusinesses. Key decisions are taken, based on the information quality available.Firstly from consumers, when they make a decision about a product, they desirereliable information to purchase, secondly, product reviews, data informs theirchoices.Large language models are producing information based on training data, modelssuch as GPT series are trained on trillions of tokens, amassing the entire internet,when LLM are used to accomplish tasks such as code-generation, tutoring studentsas education bots, users require them to rely and trust them. Unreliable informationleads towards poor data quality, thus hampering both customers and professions tolose trust. Uncertainty hinders towards lack of growth, development. Therefore it is ofurgent requirement to explore information quality, trust of large language models.Increasing information quality might lead towards increased reliability of largelanguage models, this might have indirect effects in the global economy such asstronger consumer confidence, effective partnership, adoption of language modeltechnology all over the globe, increased transparency, thereby boosting economicactivity of all businesses.3. Why are Language Models facing Information Quality issues?Reasons why language models are facing information quality issues is due toprocess of training, involving tokenization, quality of data which involves lack ofdiversity, bias, requiring larger dataset as LLM is being scaled, increasing in size.Large language models since introduction of transformers (Vaswani et al. 2017a)have been increasing in size, data, performance of tasks in majority of naturallanguage processing tasks. Transformers, an encoder-decoder type of architecture,contained training data of 800 million words from Book-Corpus with 110 millionparameters with 6 layers. BERT consists of encoder only architecture fromTransformers, with 12 layers, 110 million parameters, trained bidirectionally.Generative Pre-trained Transformers (GPT) consists of decoder-only architecture,with 12 layers, with 110 million parameters, trained unidirectionally, with furtherimprovements leading towards GPT-2, containing 48 layers, 1.5 billion parameters,trained on 40GB of WebText, with GPT-3 increasing size, with 96 layers, with a totalof 175 billion parameters, containing 12 attention heads, 768 dimensional states,with increasing more size, GPT-4 (OpenAI 2023) consisting of 1.8 trillion parameterswith 120 layers, trained on 13 trillion tokens.Tokenization (Toraman et al. 2023) is an important pre-processing technique forlarge language models, where inputs are broken down into subunits before sendingto encoder-decoder layers of language models. Tokenization methods applied inlanguage models such as character-level, byte-pair encoding (BPE), Wordpiece,morphological-level, word-level are applied in language models. Variability (Kaddouret al. 2023), token’s glitching leading towards unexpected behavior, highercomputational costs as it adds additional layer of training, information loss andquality.Method Tokenized TextCharacter-level \"L\", \"a\", \"n\", \"g\", \"u\", \"a\", \"g\", \"e\", \" \", \"M\", \"o\", \"d\", \"e\", \"l\", \"s\", \" \", \"a\", \"r\",\"e\", \" \", \"i\", \"m\", \"p\", \"r\", \"e\", \"s\", \"s\", \"i\", \"v\", \"e\"BPE \"[CLS]\", \"Language\", \"Models\", \"are\", \"impress\", \"##ive\", \"[SEP]\"WordPiece \"[CLS]\", \"Language\", \"Models\", \"are\", \"impress\", \"##ive\", \"[SEP]\"Morphological \"[CLS]\", \"Language\", \"Model\", \"s\", \"are\", \"impress\", \"##ive\",\"[SEP]\"Word-level \"[CLS]\", \"Language\", \"[UNK]\", \"are\", \"[UNK]\", \"[SEP]\"Table 1: Methods of Tokenization on text, “Language models are Impressive”Character level tokenization takes place at character level, so there is no vocabularyfor training, can represent characters, reducing model size, however it loses wordlevel meaning and outputs long sequences, loses compositionality of words. BPE isable to handle vocabulary of subwords frequently used, and able to handle unseenwords, a major disadvantage is that it is less interpretable towards morphologicaltokenization. Word-piece tokenizable is based on language modelling, also suffersfrom morphological tokenization and contains inconsistencies.Meaningful outputs are found from morphological tokenization, which capturessemantics. World level tokenization loses subword information, while capturingsimple whitespace tokenization. When large language models are trained oninconsistent tokenization, vocabulary size, it impacts quality of data and performanceof language models. Poor quality data (Wiseman, Shieber, and Rush 2017) results inlow performance in the model, SBNATION data, scored validation perplexity of 33.34and BLEU score of 1.78, due to noisy quality of SBNATION data. Training data oflanguage models are massive, however the training data is frequently lackingdata-diversity (Bender et al. 2021)Language models (Kaplan et al. 2020) performance depends on scaling them, whichis based on model parameters N, size of dataset D, amount of compute C fortraining. Based on three scale factors, N, D, C, performance has power-lawrelationship. Training a single LLM requires hundreds of thousands of computinghours, resulting in a cost of millions of dollars, consuming energy equal to severalUS families annually. Another challenge which has led towards language modelssuffering from data quality is bias, (OpenAI 2023) where there is systematicmisrepresentation, attribution errors, factual distortion, due to several factors. Onemajor factor is due to training data, when the training algorithm places moreimportance on certain features or data points. The training data is gathered fromwebsites, books, social media platforms, conversational data, during traininglanguage models are exposed to billions of sentences, phrases, allowing them tolearn relationship between words, grammar, meaning and context, when LLM istasked with generalization, they acquire these biases which impacts results due tobias in data quality.Figure 1: Language Models Data QualityFour dimensions of data quality (R. Y. Wang and Strong 1996) involves intrinsic dataquality, contextual data quality, representational data quality, accessibility category,where intrinsic data includes accuracy, cleanliness, addressing missing values.Contextual data quality refers to dimensions in data such as relevance, timelines,completeness, appropriateness, representational data quality refers to data which ispresented in intelligible, clear to understand. Accessibility category refers to datawhich is obtainable, secure. For Language models, these dimensions affect theperformance of large language models.As Language models are becoming more pervasive (Batarseh and Freeman 2022;Felderer and Ramler 2021) and dependent upon AI, however quality assurance ofthe data and systems is necessary by data quality, which can be measured bycorrectness, model relevance, robustness, security, data privacy, efficiency, fairness,interpretability. These issues can be characterized by quality characters, artefacttype, processes. Information Quality of language models can be measured also byconsistency, relevance, accuracy metrics. We define Information quality as a functionwith a combination of parameters describing consistency, relevance and accuracy.Mathematical formulation of Information Quality Evaluation LLM:Motivated by issues in evaluating information quality produced from large languagemodels, We introduce dimensions to measure information quality dimensions suchas consistency, relevance and accuracy. be information generated𝐿𝑒𝑡 { 𝑥1, 𝑥2 ..., 𝑥𝑛}by Large Language models. To evaluate the quality of Large Language models, wepropose three dimensions: accuracy, consistency and relevance. We chose threedimensions due to the following reasons. Firstly, choosing more than three mightdilute the information quality dimension, secondly our formulation is domain-agnostic– together accuracy, consistency, relevance. Lastly large language models areintended to be used for specific, human expectations also known as alignment. Ourformulation for evaluating Information quality aligns with the alignment of LLM.Figure 2: Information Quality DimensionsWe propose linear formulation of information quality due to simplicity, tunability, andnormalization. Weighted sums are equal to 1.𝐼𝑄 ( 𝐿 ) =  𝑓( (𝑤1∗ 𝑐𝑜𝑛𝑠𝑖𝑠𝑡𝑒𝑛𝑐𝑦 ∗  𝑤2𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑐𝑒 ∗ 𝑤3 𝑎𝑐𝑐𝑢𝑟𝑎𝑐𝑦))/ 𝑤1𝑤3∑ )𝑤1𝑤3∑ 𝐼𝑞(𝐿) = 1Here , represents information quality of the information produced by large𝐼𝑄(𝐿)language models, are context-specific, weights of the dimensions.𝑤1, 𝑤2𝑤3Accuracy, consistency, relevance are weighted combinations, which allowscontext-specific evaluation, which normalizes the score.4. State of the Art Large Language Models:Language has been studied by linguists, computer scientists, and statisticians.Language is human expression communicated as symbols governed by a set ofgrammatical rules. Researchers are investigating capable Artificial Intelligence (AI)for comprehending, grasping language. We explore state of the art language modelsintroduced.One significant major approach for language understanding and generation hasbeen, Language modeling (Jurafsky and Manning 2012) (Shannon 1948), wherenext word probability is based on analyzing text data, various approaches areformulated in Language modeling such as n-gram, unigram, bidirectional,exponential, neural language models, continuous space. Statistical language models(SLM) were introduced in 1990s, based on markov assumption, however due to size,they suffered from curse of dimensionality, Neural (Bengio, Ducharme, and Vincent2000) Language model introduced concept of distribution representation of wordsusing neural networks, unified solution for NLP tasks was proposed, leading towardsstate of the art performance.Recently, (Zhao et al. 2023) Transformer based language models formulated aspre-trained language models are emerging with impressive capabilities,demonstrating strong capabilities in natural language processing tasks. Abilities suchas in-context learning, which have been observed, resulting from scaling theselanguage models, the abilities are not present in smaller language models.Transformer language models are the underlying architecture for Large LanguageModels (LLM) such as (Devlin et al. 2018), Bidirectional Encoder Representationsfrom Transformers (BERT), (Radford et al. 2018) Generative Pre-trainedTransformers (GPT) series (Min et al. 2021) In recent years, BERT has emerged asa powerful language model, surpassing the previous state-of-the-art models in amultitude of natural language processing tasks.Transformers map sequence of input vectors to sequence of output vectors of samelength to . Architecturally, consisting of linear layer,𝑎1,  𝑎2..... 𝑎𝑛( ) 𝑏1,  𝑏2..... 𝑏𝑛( ) feedforward network, self-attention layer, which maps two vectors of same length.Transformers (Fan et al. 2020) use attention mechanisms to capture temporalrelations simultaneously processing tokens in parallel. Transformer models are ableto handle long coherent outputs, enabling them to generate paragraphs.Pre-trainedlanguage models have attracted significant interest among researchers andengineers. They are trained on a large dataset and fine-tuned for specificdownstream tasks. Downstream tasks such as text generation, summarization,name-entity recognition, relation extraction, and sentiment analysis.Pre-trained models became prominent due to the introduction of ELMo (Embeddingsfrom Language Models) (Iyyer et al., n.d.) due to the leverage of unlabelled data tocreate word embeddings. ELMo, word embedding method for representing sequenceof words as corresponding sequence of vectors (Iyyer et al., n.d.) is based on(Hochreiter and Schmidhuber 1997) bidirectional LSTM. Pre-trained models comeunder three categories, Autoregressive, Masked Language, Encoder-Decoder.Popular examples of Autoregressive models are GPT, GPT-2, GPT-3, GPT-3.5where the objective is to predict which word comes next, given previous words. Afew examples of masked language models are BERT, RoBERTa, XLM-R, where theobjective is to predict masked words given other words in a sequence. Recentlyintroduced state of the art language models are masked language models – Llama,Llama 2 (Touvron, Martin, et al. 2023) (Touvron, Lavril, et al. 2023) are largelanguage models with scale of 7 billion to 70 billion parameters. Trained on trillions oftokens. LLama is trained on publicly available dataset.Figure 3: Types of Pre-trained Language Model (Bepler and Berger 2021)BART and T5 are Encoder-Decoder models, where the objective includes corruptinga sequence and then predicting the original sequence. While researchers andengineers are interested in finding the best performing model, It is difficult toconclude which type of pre-trained model is best performing. Pre-trained modelsperform efficiently depending on a task. (Li et al. 2021) Autoregressive models suchas GPT are highly effective for generating text, used widely for text summarizationand language generation. Masked language models such as BERT excel forunderstanding relationships between words in a sentence and are widely used forname entity recognition and sentiment analysis. Encoder-Decoder models such asBART demonstrate efficient understanding of the structures of sentences and arewidely used for tasks such as text summarization and machine translation. Atransformer language model’s key component is (Vaswani et al. 2017b) an attentionmechanism. Attention mechanisms are trained on large datasets, from numeroussources. Due to utilization of a multi-source datasets for natural language processingtasks, attention mechanisms provide a broader, more representative sample oflinguistic phenomena present in the sequence of sentences. This enables languagemodels to learn and generalize to new examples. The datasets collected for traininglarge language models are sourced from the entire web. These datasets areprocessed using techniques such as tokenization, stemming, and lowercasing forfiltering out irrelevant information as higher quality data is necessary for good results.Popular benchmark datasets for running machine learning algorithms include (A.Wang et al. 2018), General Language Understanding Evaluation dataset (GLUE),which is a benchmark for performing evaluation for popular natural languageprocessing tasks such as sentiment analysis. The Stanford Question Answering(SQuAD) (Rajpurkar et al. 2016) a commonly used question-answering dataset innatural language processing for performing evaluation in question answering, as wellas Multi-Genre Natural Language Inference ((Williams, Nangia, and Bowman 2017),which is a popularly used for evaluating natural inference dataset. GPT-4 with Vision(OpenAI 2023) enables users to instruct, interact with image inputs, incorporatingmultimodal LLMs, which takes images, texts together as inputs (Mishkin et al. 2022),limitations remain in GPT-4V where answers might be factually incorrect,hallucinations occur in tested medical, scientific image data.LanguageModelBERT GPT3 BART ChatGPT LLama2Size Base: 110MLarge: 340M175B Base: 110MLarge: 340M175B 7B to 70BTrainingTimeBase: 8 xV100 x 12daysLarge: 64TPU Chips x4 daysMonths 1 day Months MonthsPerformance OutperformsTransformerModelOutperformsevery modelOutperformsBERT onNLG tasksOutperforms onConversationOutperformLLama1Data 16 GB BERTdata (Bookcorpus +Wikipedia)3.3 billionwords570 GBCommoncrawlWebText2Books1Books2Wikipedia160 GB 570 GBCommon crawlWebText2Books1Books2WikipediaLarger datasetthan GPT3Method BidirectionalTransformerwith MaskedLanguageModellingand NextSentencePredictionDecoderOnlyTransformerInput textcorrupted byarbitrarynoiseDecoder onlyTransformerwith RL usingHumanfeedbackMaskedLanguageModelingTable 2: Comparison of popular SOTA Large Language ModelsScaling laws of Language ModelsIn the above table, Transformer based language models displayed an increase inperformance with an increase in size of parameters in language models.Researchers found increasing size of dataset (Kaplan et al. 2020) improvedperformance in many natural language processing tasks. Moreover, not only size oftraining data, language model parameter size increases, training time and powerconsumption are also increasing, researchers have proposed architecturescombining with a sparsely activated mixture of experts (Du et al. 17--23 Jul 2022) toreduce power consumption.Due to higher power consumption, researchers investigated parameter size oflanguage models to discover scaling laws (Hoffmann, Borgeaud, Mensch,Buchatskaya, Cai, Rutherford, de Las Casas, Hendricks, Welbl, Clark, Hennigan,Noland, Millican, van den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae,et al. 2022), introduced Gopher, language modeling at scale (J. Rae, Irving, andWeidinger, n.d.). (Kaplan et al. 2020) discovered there is power law relationshipbetween number of parameters in autoregressive language models andperformance. Scaling (Kaplan et al. 2020) laws for language models can becharacterized by four parameters, size of model, size of training dataset, cost oftraining, performance after training, N, D, C, L (number of parameters, dataset size,computing cost, loss). Chinchilla scaling, a particular scaling law states (Kaplan et al.2020; Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de Las Casas,Hendricks, Welbl, Clark, Hennigan, Noland, Millican, van den Driessche, Damoc,Guy, Osindero, Simonyan, Elsen, Vinyals, et al. 2022) for training large languagemodels auto-regressively, one epoch with cosine learning rate, The scaling law is asfollows:C = 𝐶0𝑁𝐷L = 𝐴𝑁α +  𝐵𝐷α +  𝐿0C is cost of training the modelN is number of parameters of the modelD is number of tokens in training setL is average negative log-likelihood per tokenrepresents loss of ideal generative process on test data 𝐿0captures Transformer language model with N parameters underperform ideal𝐴𝑁αgenerative processcaptures fact model trained on D tokens underperforms ideal generative process𝐵𝐷αChinchilla scaling law (Jared, Sam, and Tom, n.d.) given increased budget in floatingpoint operations per second (FLOP), to achieve compute-optimal, number of modelparameters (N) and number of tokens for training model (D) should be scaling inapproximately equal proportion.Broken Scaling Laws (BSNL)Power laws frequently occur as distributions in many phenomena, such as frequencyof words in most languages, frequency of family names, size of poweroutages.Power law occurs when there is a functional relationship between twoquantities, where relative one change occurs in another quantity to a power ofchange.Figure 4: Functional form of Broken Neural Scaling Law (Caballero et al. 2022)In language models, scaling law relates to parameters of neural networks, whereneural networks are described using parameters such as size of model, size oftraining data, cost of training, performance of training. Smoothly broken power lawwhich models extrapolates scaling behaviour of deep neural networks (Caballero etal. 2022), smoothly broken power law is referred as broken neural scaling law, howevaluation metric varies as amount of computation, number of model parameters,training dataset size, upstream performance. BSNL is able to extrapolate scalingbehaviour, which other functional forms are incapable of expressing such asnon-monotonic transitions present in scaling behaviour such as double descent,sharp inflection point.𝑦 =  𝑎 +  (𝑏𝑥−𝑐0) 𝑖=1𝑛∏ ( 1 + ( 𝑥𝑑𝑖))1/𝑓𝑖)−𝑐𝑖*𝑓𝑖Where y is performance metric, which can be used for describing accuracyX is scale factor, which can be used to describe dataset size, compute, parametersa, b, are constants𝑐0 , 𝑐1,  𝑐𝑛    indicates where scaling behavior changes𝑑1........  𝑑𝑛    controls sharpness of transitions𝑓1........  𝑓𝑛    BSNL offers many advantages in neural networks. One major advantage is theability to model expressiveness in large language models, modeling sharp, smoothbreaks while allowing scale in large language models. It also allows non-monotonicrelationships like double descent, accurate fit neural network scaling curves. WhileBSNL offers advantages, it suffers from limitations such as requiring huge datapoints, harder to fit, and difficult to interpret results.5. Large Language Models and Data Quality for Performance:Large language models are giving state of art performance in downstream taskssuch as named-entity recognition, text generation, question-answering, translation,coreference resolution, involved in the majority of natural language processes(Brown et al. 2020; Bubeck et al. 2023).Outstanding capabilities of LLM are due to properties such as expressivity,scalability, multimodality, memory capacity, compositionality (Bommasani et al.2021). Due to the five properties, LLM is able to distill, accumulate knowledge frommany sources, domains, organize, effective, scalable representation, flexiblegeneralization towards novel context. GPT-3 (Brown et al. 2020), a Large languagemodel has 175 billion parameters, which was trained on 570 gb of text data, GPT-3demonstrated zero-shot generalization to downstream tasks, when increased in sizeof data and parameters from GPT-2 containing 1.5 billion parameters. LargeLanguage Model Meta AI (Llama) (Touvron, Martin, et al. 2023) is from 7 billion to 70billion parameters, trained on 2 trillion tokens of data on publicly available datafocused on factual data.As Large Language models are trained on large quantities of data, (Kaplan et al.2020) where GPT based models such as WuDao (Yuan et al. 2021) is trained on 4.9TB of multi-modal data, and scaling of dataset is only increasing due to emergentabilities of language models (Wei, Tay, et al. 2022). For training and increasingperformance of LLM data quality and size is necessary. However, the available,public dataset is small compared to petabytes of consumer, business, personal datacollected in industrial information, therefore there is a scalability problem of handlinglarge datasets. Language models require both structured and unstructured dataduring training, (Orr et al. 2020) (Poerner, Waltinger, and Schütze 2020)demonstrated integrating structured and unstructured data can help to generalizerare concepts, improve recall and accuracy.Moreover, effectively integrating both structured and unstructured datasets forincreasing performance of large language models is a challenge. While LLM is ableto be trained on unstructured data, additional steps are required for data with sourcecontaining entity or image data. In industrial scale, to integrate multi-modal datasetfor text, image, more steps are required. Data privacy is another issue in integratingthe data, where web crawlers are scraped for datasets without consent of users, howto preserve privacy, yet maintain high performance of large language models remaina persistent issue, where emerging regulation, guidelines ensure safe, responsiblemanagement of data.Data Quality of Language Models: Data quality which is used to train large languagemodels greatly affects performance (Lee et al. 2021). Data used in LLM consists ofbias of many types such as social bias, false information, duplication (Blodgett andO’Connor 2017) (Fleisig et al. 2023) misinformation, which impacts training data.Due to constant change in businesses, products, data is evolving, requiring to becontinuously updated and refined (Kiela et al. 2021), which has emergent abilities(Fetahu, Anand, and Anand 2015), distribution shift, meaning concept shift (MayeeChen et al. 18--24 Jul 2021) (Oortwijn, Bloem, and Sommerauer 2021). LLM whendeployed has found to undesirable behavior on critical, fine-grained sub-populationof data, such as bias, which needs to be detected and mitigated, tools to detect arehighly required (Hohman et al. 2018) (Goel et al. 2021)6. Language Models and Datasets for Training:Large language models are pre-trained on numerous corpus of dataset. Pre-trainingis a necessary step for allowing a language model to learn statistical properties,structure of human language present in data corpus. During pre-training, LLM learnsthe relationship between syntax, meaning, structure present between words insentences. Vector representation of words are captured in semantic and syntacticways through word embeddings. Word embeddings represent each word ashigh-dimensional vectors, which are used for optimizing for pre-training. Pre-trainingallows LLM to model as universal language model, which can be applied todownstream NLP tasks, that can be transferable, Pre-training in language modelssuch as (Devlin et al. 2018) BERT involves masked language modeling, nextsentence prediction, which gives BERT ability for bidirectional representation andcontextual representation. Language understanding, generation are established dueto pre-training (Brown et al. 2020), even demonstrating few-shot abilities. Datasetsare key towards these abilities of LLM, high-quality data and how datasets arepre-processed is necessary. Datasets which are used for training can be describedas general data and specialized data (Chowdhery et al. 2022; S. Zhang et al. 2022)Datasets such as multi-lingual data, scientific data, code data have enabled LLM tosolve specific tasks (Taylor et al. 2022)General Data for LLM: General text data contains dataset for general purpose LLM,thich includes the majority of dataset such as webpages, books, conversational text(Raffel et al. 2020). Webpage datasets contain large amount of data, which isabsorbed by using (Smith et al. 2013; Patel 2020) common-crawl data, due to hugevolume of internet data, LLM gains diverse capacities, however there are limitationsas crawled data might contain high-quality text found in Wikipedia, and low-qualitytext like email spam mails. Conversational text data enables LLM to enhance theability of LLM for conversational NLP tasks such as question-answering.Conversational datasets such as reddit corpus (Baumgartner et al. 2020) (Roller etal. 2020), social media corpus are used. Book datasets (Gao et al. 2020) provide animportant source to learn longer texts in corpus, which allows LLM to modellong-term dependency, generate narrative, coherent texts.Specialized Data for LLM:While general datasets are used for General use LLM,specific corpus involving specialization is used to improve downstream tasks such asMultilingual text, Scientific text, Code text, which can be used for LLM basedapplications for specialized tasks. Specialized tasks such as code-generation,code-question-answer, scientific question-answering. PaLM, BLOOM (Chowdhery etal. 2022; BigScience Workshop et al. 2022) involves multilingual datasets whichincludes 46 and 122 languages with their pre-training corpus, due to multilingualdataset, PaLM, BLOOM demonstrates state of the art performance in multilingualtasks such as translation, multilingual summarization. Scientific research involvespublishing findings from research, however there’s growing number of scientificpublications (Taylor et al. 2022), which can be incorporated into LLM, that has ledtowards increased performance in scientific reasoning tasks (Saier, Krause, andFärber 2023). Scientific corpus is collected from arXiv papers, scientific textbooks,physics webpages, tutorials, scholarly articles and other resources. These datasetscontain mathematical formalism, symbols, and different formats, which requirepreprocessing so that they can be processed by language models. Code datasetshave been collected, researched for program synthesis, which can be used forPre-trained language models on Code (Piccolo et al. 2023; Mark Chen et al. 2021)(Simon 1963) GPT-J (B. Wang and Komatsuzaki 2021) and other LLM (Austin et al.2021) has lead towards improvement in quality of the software programs.Demonstrated generated programs trained on code-specialized datasets havesuccessfully (Mark Chen et al. 2021) passed unit test cases, solve competitiveprogramming questions, coding datasets are collected from Stack Exchange, Github,which includes solutions and troubleshooting software errors.Data-Preprocessing:Data preprocessing for LLM involves removing noisy, redundant, irrelevant, toxicdata, which affects performance. Data-juicer for LLM (D. Chen et al. 2023) involvesfeatures which allows 50 processing operations and tools, for quality filtering inlow-quality data from corpus, two strategies can be applied, classifier, heuristics.Classifier identifies lower quality data and filters from the corpus, (Du et al. 17--23Jul 2022), binary classifiers can be used with curated data such as wikipedia withpositive classification.Figure 5: Pre-training steps in Language ModelsLanguage models such as Gopher (J. Rae, Irving, and Weidinger, n.d.) applied aheuristic based approach to eliminate lower quality text, using well-designed rules,which filters applied on language based filtering, metric based filtering, statistic,keyword based. Language models, when applied to Tamil language tasks, can uselanguage filters to remove non-Tamil tokens in the dataset, perplexity frequently usedas evaluation for quality of language models, could be used as a metric for filteringunnatural sentences. Low quality data can be removed also using symbol to wordratio, punctuation distribution, statistical filtering, keywords such as offensive words,toxicity in sentences can be removed using keyword filtering.De-duplication occurs at document-level, sentence level, dataset level resulting inlower quality data, sentences which contain repeated words, phrases must beremoved as these may result in modeling repetitive patterns in language modeling.Document level duplication can be detected by using n-gram, finding similar content,dataset level duplication can be removed by finding duplicate texts from the trainingset. Performance of LLM greatly increases due to removed of duplicates at threelevels. Protecting privacy of users is important (Carlini et al. 2022) as LLM aretrained on datasets which might contain personal information. Methods such askeyword detection, such to remove personal information name, address, phonenumbers can be applied to remove sensitive data contained in user-generatedcontent from web forums.Tokenization as described in Figure 4 is important steps, where data ispreprocessed. During step of tokenization, individual tokens are segmented from rawdataset, which are used as inputs for LLM, methods such as sequence labeling withconditional random fields, word based tokenization is used, other method such assubword tokenization have been used in Transformer based models, which appliesBytePair Encoding tokenization, WordPiece tokenization, Unigram tokenization.BytePair tokenization introduced as a data compression algorithm (Gage, n.d.;Sennrich, Haddow, and Birch 2015), basic symbols are iteratively combined withfrequent pairs of two consecutive tokens in corpus as new tokens, as merging.Merging is based on co-occurrence frequency of two contiguous tokens. Byte-levelBPE is used to improve tokenization quality for multilingual corpus. Wordpiecetokenization is similar to Byte level BPE, instead the merge step involves selectingdifferent criteria for merge, where in each merge, it selects the pair that leads to themost increase in likelihood of training data. Moreover other methods such asunigram tokenization are used in language models such as T5 and mBART.Pre-training data from a large mixture of data can enable LLM to acquire higherscope of knowledge, Gopher (J. W. Rae et al. 2021) uses ablation experiment ondistribution of dataset to examine impact of mixed source on different tasks.High-quality data for adequately training the model results is good performance(Chung et al. 2022)7. Issues and Limitations of Language Models:Although Large language models have demonstrated state of the art performance,they face major challenges and limitations. The major issue has been the problem ofhallucination (McKenna et al. 2023; Lee et al. 2018; del Campo and Leach 2022) inlarge language models, where given a text-corpus when LLM is prompted to producefactual text, LLM produces hallucinationed text. Hallucinations text is whengenerated text is in conflict with source (intrinsic) or cannot be verified by availablesource, (extrinsic). Due to this, LLM generates untruthful informationHallucination has been found in all major language models, even prone in GPT-4,even occurs in multi-modal vision-based language models (Bang et al. 2023). LLMutilize knowledge in solving tasks, which lacks ability to accurately control use ofinternal or external knowledge. Due to hallucination, it is not recommended to deployin real-world applications involving healthcare or other areas, which requires a highlevel of reliability. To mitigate this, strategies such as use of high-quality data, use ofhuman-feedback through reinforcement learning has been applied. Applying externalknowledge for checking credibility of information has reduced hallucination issueSelfCheck-GPT (Manakul, Liusie, and Gales 2023) detects hallucinations bymeasuring information inconsistency with sampled output, TruthfulQA, HaluEvaluses LLM generated and human annotation to evaluate ability of LLM to recognizehallucination in task-specific (S. Lin, Hilton, and Evans, n.d.; Li et al. 2023)scenarios.While LLM are trained on large quantities of dataset, another major limitation hasbeen, when tasked with challenging recent events or knowledge, which goes beyondtraining dataset, LLM is limited to provide reasonable answers. Knowledge recencyis an issue as cost of training is high for fine-tuning with newest information, evenwhen fine-tuned on recent knowledge, it is likely to cause catastrophic forgetting(Kaushik et al. 2021). Recently introduced framework, EasyEdit (P. Wang et al. 2023)has been released to facilitate research of knowledge editing for LLM, how to updateeffectively within LLM remains an open research problem. While LLM has alsodemonstrated reasoning capabilities (Bubeck et al. 2023), many tasks requirereasoning and relying on logical relations and evidence about factual knowledge ona particular question, chain of thought (Z. Zhang et al. 2022) prompting has beenproposed for enhancing complex reasoning capacity of LLM, where intermediatereasoning steps can be manually created, automatically generated into prompts toguide LLMs to perform multi-step reasoning, reformulating tasks such as codegeneration improves performance. However, for tasks such as common sensereasoning, LLM and human performance is compared on Commonsense QA. LLMhas accuracy of 55.9% (Wei, Wang, et al. 2022) (Chowdhery et al. 2022) (Dhingra etal. 2023), while human accuracy on dataset is about 89%, BERT performed below10% on non-entailment category. LLM might miss or generate inaccurateintermediate steps, leading towards wrong final results. Thus, Reasoninginconsistencies are observed, even though LLM might produce correct answers, itmight produce a wrong answer after a correct answer, (Yao et al. 2023) Tree ofthoughts introduced mitigates this issue by empowering decision making process byexploring, self-evaluating reasoning paths. LLMs are found to have limitations whentasked with numerical computation, especially when tokens are not encountered inthe dataset.LLMs are desired to produce output, which conform to human needs and values i.ehuman alignment (Zhao et al. 2023; Liu et al. 2023; OpenAI 2023), where LLM canbe applied and used in real-world applications, due to this, TruthfulQA (S. Lin, Hilton,and Evans, n.d.), harmlessness CrowS-Pairs (Nangia et al. 2020) datasets are usedto evaluate ability of LLMs towards human-alignment, LLM consists of ability toreceive feedback from external environment, perform action according to behaviourinstruction, generation action plans, manipulate agents (W. Huang et al. 17--23 Jul2022), LLM also consists of ability to manipulate tools received by API calls, such asbeing applied to search engine, calculator, compiler, to enhance performance (Parisi,Zhao, and Fiedel 2022). In addition to manipulating existing tools, LLM possesscapability to make their own tools for specific tasks autonomously, where modelsindependently explore, manipulate, and self-create tools, which is giving them theability to solve real-world tasks.LLM in addition to the above three abilities such as human alignment, toolmanipulation, interaction with external environment (Gilardi, Alizadeh, and Kubli2023), LLM is able to do data annotation, self-improvement (Gamil 2023; J. Huang etal. 2022), Evaluation methods using benchmarks are automated, however theysuffer from training, test data contamination, human evaluation measure real worldperformance, however expensive and not reproducible, model based evaluation areefficient, however suffer from bias. (Shaikh et al. 2021) Data imbalance, a frequentlyoccurring problem, where in the training dataset, one category of data exceeds theother category, thus causing imbalance in the dataset. Lack of enough data samplesacross class labels results in poor classification performance.To align LLM with human values such as harmlessness, honesty, helpfulness,honesty, Reinforcement learning from human feedback (RLHF) is used, which usesa reward model from human feedback, implemented as a reward function to optimiseagent’s policy using reinforcement learning (RL) using proximal policy optimization.Human feedback is gained by asking humans to rank instance’s of LLM’s behaviourfrom outputs generated. Human feedback is executed through ranking based(Ziegler et al. 2019), question-based approach (Nakano et al. 2021) and rule basedapproach (Glaese et al. 2022), which Sparrow (Glaese et al. 2022) selects based onrules to test whether model-generated responses meet alignment criteria. WhileRLHF has mitigated hallucinations to a certain degree, it suffers from limitations suchas requiring training multiple LMs as model being aligned, reward model, referencemodel, which requires tedious process in algorithmic and consumes so muchmemory. Another major limitation is that RLHF is extremely complex and timeconsuming to deploy.8. Transformers: Architectural Foundation for Large Language ModelsTransformers are the foundation for large language models (Vaswani et al. 2017a;Zhao et al. 2023). Transformers introduced as a novel architecture for neuralmachine translation established as state of art performance on English-Germantranslation task. Following initial success in natural processing tasks – transformersarchitectures were rapidly adopted across diverse range of domains beyond naturallanguage processing, even establishing as state of art performance in field ofcomputer vision, in tasks such as image segmentation, multi-modal text and imagegeneration and image recognition, using Vision-Transformers (Dosovitskiy et al.2020). Transformers replaced convolutional, recurrent layer with attentionmechanisms. Attention mechanisms were introduced to solve (Bahdanau, Cho, andBengio 2014) fixed-length vector problem in neural machine translation (NMT).Fixed-length vector which decoder generates translation was a bottleneck. Byallowing a encoder-decoder model to automatically soft-search for source sentences,which are relevant to target word, state of the art performance on NMT wasachieved.Figure 6: Attention Mechanism (Bahdanau, Cho, and Bengio 2014)Attention mechanisms allows Transformers to focus on relevant parts of the input,when generating outputs, this enables them to capture dependencies, and allows forparallelization. However, attention mechanisms face limitations such as requiringhigher computation performance, lacking deeper reasoning, semantic understanding,and requiring a large dataset. Transformers consist of encoder-decoder architecture(Vaswani et al. 2017b), where encoder maps input sequence of symbolrepresentations x to sequence of continuous representation z. Given a step z,decoder generates output sequence y of symbols one element at a time. For eachstep, the model is auto-regressive, depending on the previous element at a time, forgenerating next. Architecturally transformers contain self-attention, point-wise, fullyconnected layers for encoder and decoder.Each layer consists of two sub-layers, a multi-head self-attention mechanism,second is position wise fully connected, feed-forward network, with a residualconnection around each two sub-layers, layer normalisation, the output of thesub-layer is is a function of the𝐿𝑎𝑦𝑒𝑟𝑁𝑜𝑟𝑚(𝑥 +  𝑆𝑢𝑏𝑙𝑎𝑦𝑒𝑟(𝑥)),  𝑆𝑢𝑏𝑙𝑎𝑦𝑒𝑟(𝑥)sublayer itself. Sublayers give product outputs of dimensions, .𝑑𝑚𝑜𝑑𝑒𝑙= 512Decoder consists of 7 identical stack layers, in addition to two sub-layers in theencoder, decoder inserts a third sub-layer, which performs multi-head attention overthe output of the encoder stack. The output of the Transformer model is a sequenceof vectors with index to input tokens.Figure 7: Transformer model (Vaswani et al. 2017b)Transformers (T. Lin et al. 2022) faces issues such as requiring high computationalcomplexity due to relying on Self-attention. Furthermore, Transformers are unable tounderstand long-range dependencies in language due to the chunking of texts. Thetext has to be split into segments, before encoders can gain them as inputs, leadingtoward loss of understanding of a sentence, because semantic boundaries are nottaken into consideration for the entire sentence.What is required to make Large Language Models better?We witnessed progress of large language models achieving state of the artperformance in natural language processing, however the challenges remain, whichcould further performance, understanding of large language models. Performance oflanguage models significantly is affected by dataset quality from pre-training,adaptation tuning where data collected is fine-tuned for specific domain, utilization iswhere LLM is applied to various tasks such as question-answering, summarization,translation, and evaluating performance of LLM.Theory and Principles of underlying mechanisms of LLM is largely mysterious toresearchers (Zhao et al. 2023), where how information is distributed, organized,utilized in language models is not understood properly. Scaling has allowed LLM toendow with emergent abilities in unexpected ways, however emergent abilities aremysterious as well, as when and how emergent abilities appear is not understood.Architecturally, stacked multi-head self attention layers has been how LLM isdeployed, sparse attention has been used in (Brown et al. 2020), which hasdemonstrated improving performance and efficiency, by exploiting intrinsicredundancy, capturing dynamically changing attention weights. However,catastrophic forgetting (Korbak et al. 17--23 Jul 2022) is observed, which remains achallenge in neural networks, as originally learned knowledge during training in LLMbecomes damaged, affecting performance and abilities of LLM. Due to LLM(BigScience Workshop et al. 2022) (Zeng et al. 2022) being sensitive to data quality,systematic approaches for optimizing, factors of model effectiveness, efficiencyoptimization, training stability is required for economical reasons. ReinforcementLearning with Human Feedback (RLHF) (Ziegler et al. 2019; OpenAI 2023) hasreduced hallucinations, toxicity generation from language models, However majorlimitation of RLHF is reliance on high-quality human feedback requiring professionallabelers, which is difficult to implement in practice. Therefore, reducing humanlabelors with guaranteed data quality is required and needed.9. ConclusionWith the rise of Large Language models deployed as ChatGPT, Claude, Bard, it isimperative to investigate processes towards building trustable language models. Wehave reviewed why information quality of data plays a key role in economy,highlighting information quality issues, investigating why language modelsperformance is decreasing due to process of training, involving tokenization, qualityof data which involves lack of diversity, bias, requiring larger dataset as LLM is beingscaled, increasing in size. Moreover, we also explored state of the art languagemodels with masked language models, autoregressive, and bidirectional modelsexploring performance. We also explored scaling laws of large language models withchinchilla and broken scaling laws, which helps researchers and engineers to scalesystematically in a scientific way.In addition to scaling laws, we explored information quality issues which LLM’sperformance is limiting, Information quality issues such as bias of many types suchas social bias, false information, duplication, misinformation is a challenge in largelanguage models, to improve performance of large language models, generalizeddataset, and specialized dataset are being used, We found data pre-processingsteps such as filter quality, duplication removal, reduce privacy, tokenization, greatlyaffect performance of large language models. We also found that to make LLM betterin performance, theory and principles of LLM is to be investigated, reducing humanlabellers, and requiring high-quality data is recommended. A promising researchdirection is to increase high-data quality and investigate theory and principles ofLLM.10. References:Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E.,Cai, C., Terry, M., Le, Q., & Sutton, C. (2021). Program Synthesis with LargeLanguage Models.Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural Machine Translation by JointlyLearning to Align and Translate.Bang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., Ji, Z., Yu,T., Chung, W., Do, Q. V., Xu, Y., & Fung, P. (2023). A Multitask, Multilingual,Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity.Batarseh, F. A., & Freeman, L. (2022). AI Assurance: Towards Trustworthy,Explainable, Safe, and Ethical AI. Academic Press.Baumgartner, J., Zannettou, S., Keegan, B., Squire, M., & Blackburn, J. (2020). ThePushshift Reddit Dataset. ICWSM, 14, 830–839.Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On theDangers of Stochastic Parrots: Can Language Models Be Too Big?🦜. Proceedingsof the 2021 ACM Conference on Fairness, Accountability, and Transparency,610–623.Bengio, Y., Ducharme, R., & Vincent, P. (2000). A neural probabilistic languagemodel. Adv. Neural Inf. Process. Syst., 13.Bepler, T., & Berger, B. (2021). Learning the protein language: Evolution, structure,and function. Cell Syst, 12(6), 654-669.e3.BigScience Workshop, :, Le Scao, T., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow,D., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M., Tow, J., Rush, A. M., Biderman,S., Webson, A., Ammanamanchi, P. S., Wang, T., Sagot, B., … Wolf, T. (2022).BLOOM: A 176B-Parameter Open-Access Multilingual Language Model.Blodgett, S. L., & O’Connor, B. (2017). Racial Disparity in Natural LanguageProcessing: A Case Study of Social Media African-American English.Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S.,Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S.,Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D.,… Liang, P. (2021). On the Opportunities and Risks of Foundation Models.Brants, T., Popat, A. C., Xu, P., Och, F. J., & Dean, J. (n.d.). Large Language Modelsin Machine Translation. http://research.google/pubs/pub33278.pdfBrauwers, G., & Frasincar, F. (2023). A general survey on attention mechanisms indeep learning. IEEE Trans. Knowl. Data Eng., 35(4), 3279–3298.Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P.,Neelakantan, A., Shyam, P., Sastry, G., Askell, A., & Others. (2020). Languagemodels are few-shot learners. Adv. Neural Inf. Process. Syst., 33, 1877–1901.Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee,P., Lee, Y. T., Li, Y., Lundberg, S., Nori, H., Palangi, H., Ribeiro, M. T., & Zhang, Y.(2023). Sparks of Artificial General Intelligence: Early experiments with GPT-4.Caballero, E., Gupta, K., Rish, I., & Krueger, D. (2022). Broken Neural Scaling Laws.Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., & Zhang, C. (2022).Quantifying Memorization Across Neural Language Models.Chen, D., Huang, Y., Ma, Z., Chen, H., Pan, X., Ge, C., Gao, D., Xie, Y., Liu, Z., Gao,J., Li, Y., Ding, B., & Zhou, J. (2023). Data-Juicer: A One-Stop Data ProcessingSystem for Large Language Models.Chen, M., Goel, K., Sohoni, N. S., Poms, F., Fatahalian, K., & Re, C. (2021).Mandoline: Model Evaluation under Distribution Shift. In M. Meila & T. Zhang (Eds.),Proceedings of the 38th International Conference on Machine Learning (Vol. 139, pp.1617–1629). PMLR.Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J.,Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G.,Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., … Zaremba, W.(2021). Evaluating Large Language Models Trained on Code.Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham,P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S.,Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., … Fiedel, N.(2022). PaLM: Scaling Language Modeling with Pathways.Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X.,Dehghani, M., Brahma, S., Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X.,Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K., … Wei, J. (2022). ScalingInstruction-Finetuned Language Models.Cruz-Benito, J., Vishwakarma, S., Martin-Fernandez, F., & Faro, I. (2021).Automated Source Code Generation and Auto-Completion Using Deep Learning:Comparing and Discussing Current Language Model-Related Approaches. AI, 2(1),1–16.del Campo, M., & Leach, N. (2022). Machine Hallucinations: Architecture andArtificial Intelligence. John Wiley & Sons.Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training ofDeep Bidirectional Transformers for Language Understanding.Dhingra, S., Singh, M., Vaisakh, S. B., Malviya, N., & Gill, S. S. (2023). Mind meetsmachine: Unravelling GPT-4’s cognitive psychology.Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., & Houlsby, N.(2020). An Image is Worth 16x16 Words: Transformers for Image Recognition atScale.Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu,A. W., Firat, O., Zoph, B., Fedus, L., Bosma, M. P., Zhou, Z., Wang, T., Wang, E.,Webster, K., Pellat, M., Robinson, K., … Cui, C. (2022). GLaM: Efficient Scaling ofLanguage Models with Mixture-of-Experts. In K. Chaudhuri, S. Jegelka, L. Song, C.Szepesvari, G. Niu, & S. Sabato (Eds.), Proceedings of the 39th InternationalConference on Machine Learning (Vol. 162, pp. 5547–5569). PMLR.Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu,A. W., Firat, O., Zoph, B., Fedus, L., Bosma, M., Zhou, Z., Wang, T., Wang, Y. E.,Webster, K., Pellat, M., Robinson, K., … Cui, C. (2021). GLaM: Efficient scaling oflanguage models with mixture-of-experts. 5547–5569.Fan, A., Lavril, T., Grave, E., Joulin, A., & Sukhbaatar, S. (2020). Addressing SomeLimitations of Transformers with Feedback Memory.Felderer, M., & Ramler, R. (2021). Quality Assurance for AI-Based Systems:Overview and Challenges (Introduction to Interactive Session). Software Quality:Future Perspectives on Software Engineering Quality, 33–42.Fetahu, B., Anand, A., & Anand, A. (2015). How much is Wikipedia Lagging BehindNews? Proceedings of the ACM Web Science Conference, Article 28, 1–9.Fleisig, E., Amstutz, A., Atalla, C., Blodgett, S. L., Daumé, H., III, Olteanu, A., Sheng,E., Vann, D., & Wallach, H. (2023). Fair-Prism: Evaluating fairness-related harms intext generation. Proceedings of the 61st Annual Meeting of the Association forComputational Linguistics. Association for Computational Linguistics.Gage, P. (n.d.). A new algorithm for data compression. C Users Journal.Gamil, M. (2023). ChatGPT Basics: An Introduction to the Capabilities and Potentialof Large Language Models. Mostafa Gamil.Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He,H., Thite, A., Nabeshima, N., Presser, S., & Leahy, C. (2020). The Pile: An 800GBDataset of Diverse Text for Language Modeling.Gilardi, F., Alizadeh, M., & Kubli, M. (2023). ChatGPT outperforms crowd workers fortext-annotation tasks. Proc. Natl. Acad. Sci. U. S. A., 120(30), e2305016120.Glaese, A., McAleese, N., Trębacz, M., Aslanides, J., Firoiu, V., Ewalds, T., Rauh,M., Weidinger, L., Chadwick, M., Thacker, P., Campbell-Gillingham, L., Uesato, J.,Huang, P.-S., Comanescu, R., Yang, F., See, A., Dathathri, S., Greig, R., Chen, C.,… Irving, G. (2022). Improving alignment of dialogue agents via targeted humanjudgements.Goel, K., Rajani, N., Vig, J., Tan, S., Wu, J., Zheng, S., Xiong, C., Bansal, M., & Ré,C. (2021). Robustness Gym: Unifying the NLP Evaluation Landscape.Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Comput.Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., deLas Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E.,Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K.,Elsen, E., … Sifre, L. (2022). An empirical analysis of compute-optimal largelanguage model training. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K.Cho, & A. Oh (Eds.), Advances in Neural Information Processing Systems (Vol. 35,pp. 30016–30030). Curran Associates, Inc.Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., deLas Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E.,Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K.,Elsen, E., … Sifre, L. (2022). Training Compute-Optimal Large Language Models.Hohman, F. M., Kahng, M., Pienta, R., & Chau, D. H. (2018). Visual Analytics inDeep Learning: An Interrogative Survey for the Next Frontiers. IEEE Trans. Vis.Comput. Graph.Hu, D. (2020). An Introductory Survey on Attention Mechanisms in NLP Problems.Intelligent Systems and Applications, 432–448.Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., & Han, J. (2022). LargeLanguage Models Can Self-Improve.Huang, W., Abbeel, P., Pathak, D., & Mordatch, I. (2022). Language Models asZero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. In K.Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, & S. Sabato (Eds.),Proceedings of the 39th International Conference on Machine Learning (Vol. 162, pp.9118–9147). PMLR.Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L. (n.d.). Deepcontextualized word representations. CoRR abs/1802.05365 (2018). ArXiv PreprintArXiv.Jared, K., Sam, M. C., & Tom, H. (n.d.). Brown Tom B., Chess Benjamin, ChildRewon, Gray Scott, Radford Alec, Wu Jeffrey, and Amodei Dario. 2020. ScalingLaws for Neural Language Models.Jurafsky, D., & Manning, C. (2012). Natural language processing. Instructor,212(998), 3482.Kaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu, R., & McHardy, R. (2023).Challenges and Applications of Large Language Models.Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S.,Radford, A., Wu, J., & Amodei, D. (2020). Scaling Laws for Neural LanguageModels.Kaushik, P., Gain, A., Kortylewski, A., & Yuille, A. (2021). UnderstandingCatastrophic Forgetting and Remembering in Continual Learning with OptimalRelevance Mapping.Kiela, D., Bartolo, M., Nie, Y., Kaushik, D., Geiger, A., Wu, Z., Vidgen, B., Prasad, G.,Singh, A., Ringshia, P., Ma, Z., Thrush, T., Riedel, S., Waseem, Z., Stenetorp, P., Jia,R., Bansal, M., Potts, C., & Williams, A. (2021). Dynabench: RethinkingBenchmarking in NLP.Korbak, T., Elsahar, H., Kruszewski, G., & Dymetman, M. (2022). ControllingConditional Language Models without Catastrophic Forgetting. In K. Chaudhuri, S.Jegelka, L. Song, C. Szepesvari, G. Niu, & S. Sabato (Eds.), Proceedings of the 39thInternational Conference on Machine Learning (Vol. 162, pp. 11499–11528). PMLR.Lee, K., Firat, O., Agarwal, A., Fannjiang, C., & Sussillo, D. (2018). Hallucinations inNeural Machine Translation.Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., & Carlini, N.(2021). Deduplicating Training Data Makes Language Models Better.Li, J., Cheng, X., Zhao, W. X., Nie, J.-Y., & Wen, J.-R. (2023). HaluEval: ALarge-Scale Hallucination Evaluation Benchmark for Large Language Models. ArXivE-Prints, arXiv:2305.11747.Li, J., Tang, T., Zhao, W. X., & Wen, J.-R. (2021). Pretrained Language Models forText Generation: A Survey.Lin, S., Hilton, J., & Evans, O. (n.d.). TruthfulQA: Measuring How Models MimicHuman Falsehoods.(2021). URL Https://Arxiv. Org/Abs/2109.07958.(Cited on p. 29).Lin, T., Wang, Y., Liu, X., & Qiu, X. (2022). A survey of transformers. AI Open, 3,111–132.Liu, Y., Yao, Y., Ton, J.-F., Zhang, X., Guo, R., Cheng, H., Klochkov, Y., Taufiq, M. F.,& Li, H. (2023). Trustworthy LLMs: A Survey and Guideline for Evaluating LargeLanguage Models’ Alignment.Manakul, P., Liusie, A., & Gales, M. J. F. (2023). SelfCheckGPT: Zero-ResourceBlack-Box Hallucination Detection for Generative Large Language Models.McKenna, N., Li, T., Cheng, L., Hosseini, M. J., Johnson, M., & Steedman, M.(2023). Sources of Hallucination by Large Language Models on Inference Tasks.Min, B., Ross, H., Sulem, E., Veyseh, A. P. B., Nguyen, T. H., Sainz, O., Agirre, E.,Heinz, I., & Roth, D. (2021). Recent Advances in Natural Language Processing viaLarge Pre-Trained Language Models: A Survey.Mishkin, P., Ahmad, L., Brundage, M., Krueger, G., & Sastry, G. (2022). DALLE 2preview: Risks and limitations. Github.Moradi Dakhel, A., Majdinasab, V., Nikanjam, A., Khomh, F., Desmarais, M. C., &Jiang, Z. M. (jack). (2023). GitHub Copilot AI pair programmer: Asset or Liability? J.Syst. Softw., 203, 111734.Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S.,Kosaraju, V., Saunders, W., Jiang, X., Cobbe, K., Eloundou, T., Krueger, G., Button,K., Knight, M., Chess, B., & Schulman, J. (2021). WebGPT: Browser-assistedquestion-answering with human feedback.Nangia, N., Vania, C., Bhalerao, R., & Bowman, S. R. (2020). CrowS-Pairs: AChallenge Dataset for Measuring Social Biases in Masked Language Models.Oortwijn, Y., Bloem, J., Sommerauer, P., & others. (2021). Challenging distributionalmodels with a conceptual network of philosophical terms. Of the 2021 ….OpenAI. (2023). GPT-4 Technical Report.Orr, L., Leszczynski, M., Arora, S., Wu, S., Guha, N., Ling, X., & Re, C. (2020).Bootleg: Chasing the Tail with Self-Supervised Named Entity Disambiguation.Parisi, A., Zhao, Y., & Fiedel, N. (2022). TALM: Tool Augmented Language Models.Patel, J. M. (2020). Introduction to Common Crawl Datasets. In J. M. Patel (Ed.),Getting Structured Data from the Internet: Running Web Crawlers/Scrapers on a BigData Production Scale (pp. 277–324). Apress.Piccolo, S. R., Denny, P., Luxton-Reilly, A., Payne, S. H., & Ridge, P. G. (2023).Evaluating a large language model’s ability to solve programming exercises from anintroductory bioinformatics course. PLoS Comput. Biol., 19(9), e1011511.Poerner, N., Waltinger, U., & Schütze, H. (2020). Inexpensive Domain Adaptation ofPretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA.Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improvinglanguage understanding by generative pre-training. cs.ubc.ca. https://www.cs.ubc.ca/amuham01/LING530/papers/radford2018improving.pdfRae, J., Irving, G., & Weidinger, L. (n.d.). Language modelling at scale: Gopher,ethical considerations, and retrieval. DeepMind Blog.Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J.,Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan, T., Menick, J.,Cassirer, A., Powell, R., van den Driessche, G., Hendricks, L. A., Rauh, M., Huang,P.-S., … Irving, G. (2021). Scaling Language Models: Methods, Analysis & Insightsfrom Training Gopher.Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li,W., & Liu, P. J. (2020). Exploring the limits of transfer learning with a unifiedtext-to-text transformer. J. Mach. Learn. Res., 21(1), 5485–5551.Rajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P. (2016). SQuAD: 100,000+Questions for Machine Comprehension of Text.Roller, S., Dinan, E., Goyal, N., Ju, D., Williamson, M., Liu, Y., Xu, J., Ott, M.,Shuster, K., Smith, E. M., Boureau, Y.-L., & Weston, J. (2020). Recipes for buildingan open-domain chatbot.Saier, T., Krause, J., & Färber, M. (2023). unarXive 2022: All arXiv PublicationsPre-Processed for NLP, Including Structured Full-Text and Citation Network.Sennrich, R., Haddow, B., & Birch, A. (2015). Neural Machine Translation of RareWords with Subword Units.Shaikh, S., Daudpota, S. M., Imran, A. S., & Kastrati, Z. (2021). Towards ImprovedClassification Accuracy on Highly Imbalanced Text Dataset Using Deep NeuralLanguage Models. NATO Adv. Sci. Inst. Ser. E Appl. Sci., 11(2), 869.Shannon, C. E. (1948). A mathematical theory of communication. Bell Syst. Tech. J.,27(4), 623–656.Simon, H. A. (1963). Experiments with a Heuristic Compiler. J. ACM, 10(4),493–506.Smith, J. R., Saint-Amand, H., Plamada, M., Koehn, P., Callison-Burch, C., & Lopez,A. (2013). Dirt cheap web-scale parallel text from the Common Crawl. Proceedingsof the 51st Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers), 1374–1383.Solaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., Wu, J., Radford,A., Krueger, G., Kim, J. W., Kreps, S., McCain, M., Newhouse, A., Blazakis, J.,McGuffie, K., & Wang, J. (2019). Release Strategies and the Social Impacts ofLanguage Models.Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., Poulton,A., Kerkez, V., & Stojnic, R. (2022). Galactica: A Large Language Model for Science.Toraman, C., Yilmaz, E. H., Şahı̇nuç, F., & Ozcelik, O. (2023). Impact of Tokenizationon Language Models: An Analysis for Turkish. ACM Trans. Asian Low-Resour. Lang.Inf. Process., 22(4), 1–21.Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière,B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., & Lample,G. (2023). LLaMA: Open and Efficient Foundation Language Models.Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N.,Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M.,Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., … Scialom, T. (2023). Llama2: Open Foundation and Fine-Tuned Chat Models.Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,\\Lukasz, & Polosukhin, I. (2017). Attention is All you Need. Adv. Neural Inf. Process.Syst., 30.Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. R. (2018). GLUE: AMulti-Task Benchmark and Analysis Platform for Natural Language Understanding.Wang, B., & Komatsuzaki, A. (2021). GPT-J-6B: A 6 billion parameter autoregressivelanguage model.Wang, P., Zhang, N., Xie, X., Yao, Y., Tian, B., Wang, M., Xi, Z., Cheng, S., Liu, K.,Zheng, G., & Chen, H. (2023). EasyEdit: An Easy-to-use Knowledge EditingFramework for Large Language Models.Wang, R. Y., & Strong, D. M. (1996). Beyond Accuracy: What Data Quality Means toData Consumers. Journal of Management Information Systems, 12(4), 5–33.Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D.,Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P.,Dean, J., & Fedus, W. (2022). Emergent Abilities of Large Language Models.Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., &Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large languagemodels. 24824–24837.Williams, A., Nangia, N., & Bowman, S. R. (2017). A Broad-Coverage ChallengeCorpus for Sentence Understanding through Inference.Wiseman, S., Shieber, S. M., & Rush, A. M. (2017). Challenges in Data-to-DocumentGeneration.Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., & Narasimhan, K.(2023). Tree of Thoughts: Deliberate Problem Solving with Large Language Models.Yuan, S., Zhao, H., Du, Z., Ding, M., Liu, X., Cen, Y., Zou, X., Yang, Z., & Tang, J.(2021). WuDaoCorpora: A super large-scale Chinese corpora for pre-traininglanguage models. AI Open, 2, 65–68.Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zheng, W., Xia,X., Tam, W. L., Ma, Z., Xue, Y., Zhai, J., Chen, W., Zhang, P., Dong, Y., & Tang, J.(2022). GLM-130B: An Open Bilingual Pre-trained Model.Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab,M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura,P. S., Sridhar, A., Wang, T., & Zettlemoyer, L. (2022). OPT: Open Pre-trainedTransformer Language Models.Zhang, Z., Zhang, A., Li, M., & Smola, A. (2022). Automatic Chain of ThoughtPrompting in Large Language Models.Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang,J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X.,Liu, Z., … Wen, J.-R. (2023). A Survey of Large Language Models.Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D.,Christiano, P., & Irving, G. (2019). Fine-Tuning Language Models from HumanPreferences.",
    "Link": "http://arxiv.org/abs/2401.13086"
}