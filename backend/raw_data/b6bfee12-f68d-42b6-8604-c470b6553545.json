{
    "Title": "Replica conditional sequential monte carlo",
    "Authors": "Doucet, A, Proceedings of the 36th International Conference on Machine Learning, Shestopaloff, AY",
    "Year": "No year available",
    "Abstract": "© 2019 International Machine Learning Society (IMLS). We propose a Markov chain Monte Carlo (MCMC) scheme to perform state inference in non-linear non-Gaussian state-space models. Current state-of-the-art methods to address this problem rely on particle MCMC techniques and its variants, such as the iterated conditional Sequential Monte Carlo (cSMC) scheme, which uses a Sequential Monte Carlo (SMC) type proposal within MCMC. A deficiency of standard SMC proposals is that they only use observations up to time t to propose states at time t when an entire observation sequence is available. More sophisticated SMC based on lookahead techniques could be used but they can be difficult to put in practice. We propose here replica cSMC where we build SMC proposals for one replica using information from the entire observation sequence by conditioning on the states of the other replicas. This approach is easily parallelizable and we demonstrate its excellent empirical performance when compared to the standard iterated cSMC scheme at fixed computational complexity",
    "Keywords": "No keywords available",
    "Publisher": "",
    "Publication Date": "No publication date available",
    "Journal": "No journal available",
    "Citation Count": 0,
    "Full Text": "Replica Conditional Sequential Monte CarloAlexander Y. Shestopaloff 1 2 Arnaud Doucet 3 2AbstractWe propose a Markov chain Monte Carlo(MCMC) scheme to perform state inference innon-linear non-Gaussian state-space models. Cur-rent state-of-the-art methods to address this prob-lem rely on particle MCMC techniques and itsvariants, such as the iterated conditional Sequen-tial Monte Carlo (cSMC) scheme, which usesa Sequential Monte Carlo (SMC) type proposalwithin MCMC. A deficiency of standard SMCproposals is that they only use observations up totime t to propose states at time t when an entireobservation sequence is available. More sophisti-cated SMC based on lookahead techniques couldbe used but they can be difficult to put in practice.We propose here replica cSMC where we buildSMC proposals for one replica using informationfrom the entire observation sequence by condi-tioning on the states of the other replicas. Thisapproach is easily parallelizable and we demon-strate its excellent empirical performance whencompared to the standard iterated cSMC schemeat fixed computational complexity.1. IntroductionWe consider discrete-time state-space models. They canbe described by a latent Markov process (Xt)t≥1 and anobservation process (Yt)t≥1, (Xt, Yt) being X × Y-valued,which satisfy X1 ∼ µ(·) andXt+1|{Xt = x} ∼ f(·|x) Yt|{Xt = x} ∼ g(·|x)(1)for t ≥ 1. Our goal is to sample from posterior distributionof the latent statesX1:T := (X1, ..., XT ) given a realizationof the observations Y1:T = y1:T . This distribution admits a1School of Mathematics, University of Edinburgh, Edinburgh,UK 2The Alan Turing Institute, London, UK 3Department ofStatistics, University of Oxford, Oxford, UK. Correspondenceto: Alexander Y. Shestopaloff <ashestopaloff@turing.ac.uk>.Proceedings of the 36 th International Conference on MachineLearning, Long Beach, California, PMLR 97, 2019. Copyright2019 by the author(s).density given byp(x1:T |y1:T ) ∝ µ(x1)g(y1|x1)T∏t=2f(xt|xt−1)g(yt|xt).(2)This sampling problem is now commonly addressed us-ing an MCMC scheme known as the iterated cSMC sam-pler (Andrieu et al., 2010) and extensions of it; see, e.g.,(Shestopaloff & Neal, 2018). This algorithm relies on aSMC-type proposal mechanism. A limitation of these al-gorithms is that they typically use data only up to time tto propose candidate states at time t, whereas the entiresequence y1:T is observed in the context we are interestedin. To address these issues, various lookahead techniqueshave been proposed in the SMC literature; see (Chen et al.,2013) for a review. Alternative approaches relying on aparametric approximation of the backward information fil-ter used for smoothing in state-space models (Briers et al.,2010) have also been recently proposed in (Scharth & Kohn,2016; Guarniero et al., 2017; Ruiz & Kappen, 2017; Henget al., 2017). When applicable, these iterative methods havedemonstrated good performance. However, it is unclearhow these ideas could be adapted to the MCMC frameworkinvestigated here. Additionally these methods are difficultto put in practice for multimodal posterior distributions.In this paper, we propose a novel approach which allowsus to build proposals for cSMC that allows considering allobserved data in a proposal, based on conditioning on repli-cas of the state variables. Our approach is based purely onMonte Carlo sampling, bypassing any need for approximat-ing functions in the estimate of the backward informationfilter.The rest of this paper is organized as follows. In Section2, we review the iterated cSMC algorithm and outline itslimitations. Section 3 introduces the replica iterated cSMCmethodology. In Section 4, we demonstrate the methodol-ogy on a linear Gaussian model, two non-Gaussian statespace models from (Shestopaloff & Neal, 2018) as well asthe Lorenz-96 model from (Heng et al., 2017).2. Iterated cSMCThe iterated cSMC sampler is an MCMC method for sam-pling from a target distribution of density π (x1:T ) :=Replica Conditional Sequential Monte CarloAlgorithm 1 Iterated cSMC kernel K (x1:T , x′1:T )cSMC step.1. At time t = 1(a) Sample b1 uniformly on [N ] and set xb11 = x1.(b) For i ∈ [N ] \\{b1}, sample xi1 ∼ q1 (·).(c) Compute w1(x0, xi1) for i ∈ [N ].2. At times t = 2, . . . , T(a) Sample bt uniformly on [N ] and set xbtt = xt.(b) For i ∈ [N ] \\{bt}, sampleait−1 ∼ Cat{wt−1(xajt−2t−2 , xjt−1); j ∈ [N ]}.(c) For i ∈ [N ] \\{bt}, sample xit ∼ qt( ·|xait−1t−1 ).(d) Compute wt(xait−1t−1 , xit) for i ∈ [N ].Backward sampling step.1. At times t = T(a) Sample bT ∼ Cat{wT (xajT−1T−1 , xjT ); j ∈ [N ]}.2. At times t = T − 1, ..., 1(a) Sample bt ∼Cat{βt+1(xjt , xbt+1t+1 )wt(xajt−1t−1 , xjt ); j ∈ [N ]}.Output x′1:T = xb1:T1:T :=(xb11 , . . . , xbTT).πT (x1:T ). It relies on a modified SMC scheme target-ing a sequence of auxiliary target probability densities{πt (x1:t)}t=1,...,T−1 and a sequence of proposal densitiesq1 (x1) and qt(xt|xt−1) for t ∈ {2, ..., T}. These target den-sities are such that πt(x1:t)/πt−1(x1:t−1) ∝ βt(xt−1, xt).2.1. AlgorithmWe define the ‘incremental importance weights’ for t ≥ 2aswt(xt−1, xt) :=πt (x1:t)πt−1 (x1:t−1) qt(xt|xt−1)∝ βt(xt−1, xt)qt(xt|xt−1)(3)and for t = 1 asw1(x0, x1) :=π1(x1)q1(x1). (4)We introduce a dummy variable x0 to simplify notation. Welet N ≥ 2 be the number of particles used by the algorithmand [N ] := {1, ..., N}. We introduce the notation xt =(x1t , . . . , xNt)∈ XN , at =(a1t , . . . , aNt)∈ {1, . . . , N}N ,x1:T = (x1,x2, ...,xT ), a1:T−1 = (a1,a2, ...,aT−1) andx−btt = xt\\xbtt , x−b1:T1:T ={x−b11 , . . . ,x−bTT}, a−btt−1 =at−1\\abtt−1, a−b2:T1:T−1 ={a−b21 , . . . ,a−bTT−1}and set bt =abt+1t for t = 1, ..., T − 1.It can be shown that the iterated cSMC kernel, describedin Algorithm 1, is invariant w.r.t. π(x1:T ). Given the cur-rent state x1:T , the cSMC step introduced in (Andrieu et al.,2010) samples from the following distributionΦ(x−b1:T1:T ,a−b2:T1:T−1∣∣∣xb1:T1:T , b1:T ) = δx1:T (xb1:T1:T )×N∏i=1,i6=b1q1(xi1) T∏t=2N∏i=1,i6=btλ(ait−1, xit∣∣xt−1), (5)whereλ(ait−1 = k, xit∣∣xt−1) = wt−1(xakt−1t−2 , xkt−1)∑Nj=1 wt−1(xajt−1t−2 , xjt−1)× qt(xit∣∣xkt−1). (6)This can be combined to a backward sampling step in-troduced in (Whiteley, 2010); see (Finke et al., 2016;Shestopaloff & Neal, 2018) for a detailed derivation. Itcan be shown that the combination of these two steps de-fined a Markov kernel that preserves the following extendedtarget distributionγ(x1:T ,a−b2:T1:T−1, b1:T ) :=π(xb1:T1:T )NT× Φ(x−b1:T1:T ,a−b2:T1:T−1∣∣∣xb1:T1:T , b1:T ) (7)as invariant distribution. In particular, it follows that ifx1:T ∼ π then x′1:T ∼ π. The algorithm is described inAlgorithm 1 where we use the notation Cat{ci; i ∈ [N ]} todenote the categorical distribution of probabilities pi ∝ ci.Iterated cSMC has been widely adopted for state spacemodels, i.e. when the target is π(x1:T ) = p(x1:T |y1:T ). Thedefault sequence of auxiliary targets one uses is πt(x1:t) =p(x1:t|y1:t) for t = 1, ..., T − 1 resulting in the incrementalimportance weightswt(xt−1, xt) ∝f(xt|xt−1)g(yt|xt)qt(xt|xt−1)(8)for t ≥ 2 andw1(x0, x1) ∝µ(x1)g(y1|x1)q1(x1)(9)for t = 1. Typically we will attempt to select a proposalwhich minimizes the variance of the incremental weight,which at time t ≥ 2 is qoptt (xt|xt−1) = p(xt|xt−1, yt) ∝g(yt|xt)f(xt|xt−1) or an approximation of it.Replica Conditional Sequential Monte Carlo2.2. Limitations of Iterated cSMCWhen using the default sequence of auxiliary targets for statespace models, iterated cSMC does not exploit a key featureof the problem at hand. The cSMC step typically uses aproposal at time t that only relies on the observation yt, i.e.qt(xt|xt−1) = p (xt|xt−1, yt), as it targets at time t the pos-terior density p (x1:t|y1:t). In high-dimensions and/or in thepresence of highly informative observations, the discrepancybetween successive posterior densities {p (x1:t|y1:t)}t≥1will be high. Consequently the resulting importance weights{wt(xait−1t−1 , xit); i ∈ [N ]} will have high variance and theresulting procedure will be inefficient.Ideally one would like to use the sequence of marginalsmoothing densities as auxiliary densities, that is πt(x1:t) =p (x1:t|y1:T ) for t = 1, ..., T − 1. Unfortunately, this is notpossible as p (x1:t|y1:T ) ∝ p (x1:t|y1:t−1) p (yt:T |xt) can-not be evaluated pointwise up to a normalizing constant.To address this problem in a standard SMC framework, re-cent contributions (Scharth & Kohn, 2016; Guarniero et al.,2017; Ruiz & Kappen, 2017; Heng et al., 2017) performan analytical approximation p̂ (yt:T |xt) of the backwardinformation filter p (yt:T |xt) based on an iterative parti-cle mechanism and target instead {p̂ (x1:t|y1:T )}t≥1 wherep̂ (x1:t|y1:T ) ∝ p (x1:t|y1:t−1) p̂ (yt:T |xt) using proposalsof the form qt (xt|xt−1) ∝ f (xt|xt−1) p̂ (yt:T |xt). Thesemethods can perform well but it requires a careful design ofthe analytical approximation and is difficult to put in prac-tice for multimodal posteriors. Additionally, it is unclearhow these could be adapted in an iterated cSMC frameworkwithout introducing any bias.Versions of iterated cSMC using an independent approxi-mation to the backward information filter based on ParticleEfficient Importance Sampling (Scharth & Kohn, 2016) havebeen proposed (Grothe et al., 2016) though they still requirea choice of analytical approximation and use an approxima-tion to the backward information filter which is global. Thiscan become inefficient in high dimensional state scenarios.3. Replica Iterated cSMCWe introduce a way to directly use the iteratedcSMC algorithm to target a sequence of approximations{p̂ (x1:t|y1:T )}t≥1 to the marginal smoothing densities ofa state space model. Our proposed method is based onsampling from a target over multiple copies of the space asdone in, for instance, the Parallel Tempering or EnsembleMCMC (Neal, 2010) approaches. However, unlike in thesetechniques, we use copies of the space to define a sequenceof intermediate distributions in the cSMC step informedby the whole dataset. This enables us to draw samples ofX1:T that incorporate information about all of the observeddata. Related recent work includes (Leimkuhler et al., 2018),where information sharing amongst an ensemble of replicasis used to improve MCMC proposals.3.1. AlgorithmWe start by defining the replica target for some K ≥ 2 byπ̄(x(1:K)1:T ) =K∏k=1p(x(k)1:T |y1:T ). (10)Each of the replicas x(k)1:T is updated in turn by runningAlgorithm 1 with a different sequence of intermediate tar-gets which we describe here. Consider updating replicak and let p̂(k)(yt+1:T |xt) be an estimator of the backwardinformation filter, built using replicas other than the k-thone, x(−k)t+1 = (x(1)t+1, . . . , x(k−1)t+1 , x(k+1)t+1 , . . . , x(K)t+1). Forconvenience of notation, we take p̂(k)(yT+1:T |xT ) := 1.At time t, the cSMC does target approximation of themarginal smoothing distribution p (x1:t|y1:T ) as in (Scharth& Kohn, 2016; Guarniero et al., 2017; Ruiz & Kappen, 2017;Heng et al., 2017). This is of the form p̂(k) (x1:t|y1:T ) ∝p (x1:t|y1:t) p̂(k) (yt+1:T |xt). This means that the cSMC forreplica k uses the novel incremental weights at time t ≥ 2w(k)t (xt−1, xt) :=p̂(k) (x1:t|y1:T )p̂(k) (x1:t−1|y1:T ) qt(xt|xt−1)(11)∝ g(yt|xt)f(xt|xt−1)p̂(k) (yt+1:T |xt)p̂(k) (yt:T |xt−1) qt(xt|xt−1)andw(k)1 (x0, x1) ∝ g(y1|x1)µ(x1)p̂(k)(yt+1:T |x1)/q1(x1).We would like to use the proposal minimizing the vari-ance of the incremental weight, which at time t ≥ 2 isqoptt (xt|xt−1) ∝ g(yt|xt)f(xt|xt−1)p̂(k) (yt+1:T |xt) or anapproximation of it.The full replica cSMC update for π̄ is described in Algo-rithm 2 and is simply an application of Algorithm 1 to asequence of target densities for each replica. A proof of thevalidity of the algorithm is provided in the SupplementaryMaterial.Algorithm 2 Replica cSMC updateFor k = 1, . . . ,K1. Build an approximation p̂(k) (yt+1:T |xt)of p (yt+1:T |xt) using the replicas(x(1)′t+1, . . . , x(k−1)′t+1 , x(k+1)t+1 , . . . , x(K)t+1) for t =1, ..., T − 1.2. Run Algorithm 1 with target π(x1:T ) = p(x1:T |y1:T )and auxiliary targets πt(x1:t) = p̂(k) (x1:t|y1:T ) fort = 1, . . . , T − 1 with initial state x(k)1:T to returnx(k′)1:T .Output x(1:K)′1:T .Replica Conditional Sequential Monte CarloOne sensible way to initialize the replicas is to set them tosequences sampled from standard independent SMC passes.This will start the Markov chain not too far from equilibrium.For multimodal distributions, initialization is particularlycrucial, since we need to ensure that different replicas arewell-distributed amongst the various modes at the start ofthe run.3.2. Setup and TuningThe replica cSMC sampler requires an estimatorp̂(k) (yt+1:T |xt) of the backward information filter basedon x(−k)t+1 . For our algorithm, we propose an estimatorp̂(k) (yt+1:T |xt) that is not based on any analytical approx-imation of p (yt+1:T |xt) but simply on a Monte Carlo ap-proximation built using the other replicas,p̂(k) (yt+1:T |xt) ∝∑j 6=kf(x(j)t+1|xt)p(x(j)t+1|y1:t), (12)where p (xt+1|y1:t) denotes the predictive density of xt+1.The rationale for this approach is that at equilibrium the com-ponents of x(−k)t+1 are an iid sample from a product of K − 1copies of the smoothing density, p (xt+1|y1:T ). Therefore,as K increases, (12) converges to∫f (xt+1|xt)p (xt+1|y1:t)p (xt+1|y1:T ) dxt+1∝∫f (xt+1|xt) p (yt+1:T |xt+1) dxt+1= p (yt+1:T |xt) . (13)In practice, the predictive density is also unknown and weneed to use an approximation of it. Whatever being theapproximation p̂ (xt+1|y1:t) of p (xt+1|y1:t) we use, thealgorithm is valid. We note that for K = 2, any approxima-tion of the predictive density results in the same incrementalimportance weights.We propose to approximate the predictive density in (13) bya constant over the entire latent space, i.e. p̂(xt+1|y1:t) = 1.We justify this choice as follows. If we assume that wehave informative observations, which is typical in manystate space modelling scenarios, then p(xt+1|y1:T ) will tendto be much more concentrated than p(xt+1|y1:t). Thus,over the region where the posterior has high density, thepredictive density will be approximately constant relativeto the posterior density. This suggests approximating thepredictive density in (13) by its mean with respect to theposterior density,∫f (xt+1|xt)p (xt+1|y1:t)p (xt+1|y1:T ) dxt+1≈∫f (xt+1|xt) p (xt+1|y1:T ) dxt+1∫p (xt+1|y1:t) p (xt+1|y1:T ) dxt+1≈1K∑Kk=1 f(x(k)t+1|xt)1K∑Kk=1 p(x(k)t+1|y1:t). (14)Since the importance weights in cSMC at each time aredefined up to a constant, sampling is not affected by thespecific value of 1K∑Kk=1 p(x(k)t+1|y1:t). Therefore, whendoing computation it can simply be set to any value, whichis what we do.We note that while the asymptotic argument doesn’t holdfor the estimator in (14), when the variance of the predictivedensity is greater than the variance of the posterior density,we expect the estimators in (12) and (14) to be close for anyfinite K.An additional benefit to approximating the predictive densityby a constant is reduction in the variance of the mixtureweights in (12). To see why this can be the case, considerthe following example. Suppose the predictive density ofxt+1 is N (µ, σ20) and the posterior density is N (0, σ21),where σ21 < σ20 . Computing the variance of the mixtureweight, we getVar(1p(xt+1|y1:t))=2πσ20√2σ21ν1exp[µ2(1σ20+1(σ20)2ν1)]− 2πσ20σ21ν2exp[µ2(1σ20+1(σ20)2ν2)]. (15)whereν1 =(12σ21− 1σ20)ν2 =(1σ21− 1σ20). (16)From this we can see that variance increases exponen-tially with the squared difference of predictive and posteriormeans, µ2. As a result, we can get outliers in the mixtureweight distribution. If this happens, many of the replicaswill end up having low weights in the mixture. This willreduce the effective number of replicas used. Using a con-stant approximation will weight all of the replicas uniformly,and allow us to construct better proposals, as illustrated inSection 4.1.A natural extension of the proposed method is to updatesome of the replicas with other than replica cSMC updates.Samples from these replicas can then be used in estimates ofthe backward information filter when doing a replica cSMCReplica Conditional Sequential Monte Carloupdate. This makes it possible to parallelize the method, atleast to some extent. For instance, one possibility is to doparallel independent cSMC updates on some of the replicas.Performing other than replica cSMC updates on some ofthe replicas can be useful in multimodal scenarios. If allreplicas are located in an isolated mode, and the replicacSMC updates use an estimate of the backward informationfilter based on replicas in that mode, then the overall Markovchain will tend not to transition well to other modes. Usingsamples from other types of updates in the estimate of thebackward information filter can help counteract this effectby making transitions to other high-density regions possible.4. ExamplesWe consider four models to illustrate the performance ofour method. In all examples, we assume that the modelparameters are known. The first is a simple linear Gaussianmodel. We use this model to demonstrate that it is sensibleto use a constant approximation to the predictive densityin our estimator of the backward information filter. Wealso use the linear Gaussian model to better understand theaccuracy and performance of replica cSMC. The secondmodel, from (Shestopaloff & Neal, 2018), demonstratesthat our proposed replica cSMC method is competitive withexisting state-of-the-art methods at drawing latent state se-quences in a unimodal context. The third model, also from(Shestopaloff & Neal, 2018), demonstrates that by updatingsome replica coordinates with a standard iterated cSMCkernel, our method is able to efficiently handle multimodalsampling without the use of specialized “flip” updates. Thefourth model is the Lorenz-96 model from (Heng et al.,2017), which has very low observation noise, making it achallenging case for standard iterated cSMC.To do our computations, we used MATLAB on an OS Xsystem, running on an Intel Core i5 1.3 GHz CPU. As aperformance metric for the sampler, we used autocorrela-tion time, which is a measure of approximately how manysteps of an MCMC chain are required to obtain the equiva-lent of one independent sample. The autocorrelation timeis estimated based on a set of runs as follows. First, weestimate the overall mean using all of the runs. Then, weuse this overall mean to estimate autocovariances for eachof the runs. The autocovariance estimates are then aver-aged and used to estimate the autocorrelations ρ̂k. Theautocorrelation time is then estimated as 1 + 2∑Mm=1 ρ̂mwhere M is chosen such that for m > M the autocorrela-tions are approximately 0. The code to reproduce all theresults is publicly available at https://github.com/ayshestopaloff/replicacsmc.4.1. A Linear Gaussian ModelLet Xt = (X1,t, . . . , Xd,t)′ for t = 1, . . . , T . The la-tent process for this model is defined as X1 ∼ N (0,Σ1),Xt|{Xt−1 = xt−1} ∼ N (Φxt−1,Σ) for t = 2, . . . , T ,whereΦ =φ1 0 · · · 00 φ2. . ........ . . φd−1 00 · · · 0 φd , Σ =1 ρ · · · ρρ 1. . ........ . . 1 ρρ · · · ρ 1 ,Σ1 =σ21,1 ρσ1,1σ1,2 · · · ρσ1,1σ1,dρσ1,2σ1,1 σ21,2. . ........ . . σ21,d−1 ρσ1,d−1σ1,dρσ1,dσ1,1 · · · ρσ1,dσ1,d−1 σ21,d ,with σ21,i = 1/(1− φ2i ) for i = 1, . . . , d. The observationsare Yi,t|{Xi,t = xi,t} ∼ N (xi,t, 1) for i = 1, . . . , d andt = 1, . . . , T . We set T = 250, d = 5 and the model’sparameters to ρ = 0.7 and φi = 0.9 for i = 1, . . . , d.We generate a sequence from this model to use for ourexperiments.Since this is a linear Gaussian model, we are able to com-pute the predictive density in (12) exactly using a Kalmanfilter. So for replica k, we can use the following importancedensities,q1(x1) ∝ µ(x1)∑j 6=kf(x(j)2 |x1)p(x(j)2 |y1),qt(xt|xt−1) ∝ f(xt|xt−1)∑j 6=kf(x(j)t+1|xt)p(x(j)t+1|y1:t),qT (xT |xT−1) ∝ f(xT |xT−1), (17)where t = 2, . . . , T − 1. Since these densities are Gaussianmixtures, they can be sampled from exactly. However, aspointed out in the previous section, this approach can beinefficient. We will show experimentally that using a con-stant approximation to the predictive density in (12) actuallyimproves performance. In all experiments, we intialize allreplicas to a sample from an independent SMC pass with thesame number of particles as used for cSMC updates. Also,the different runs in our experiments use different randomnumber generator seeds.We first check that our replica method produces answersthat agree with the posterior mean computed by a Kalmansmoother. To do this, we do 10 replica cSMC runs with 100particles and 2 replicas for 25, 000 iterations, updating eachreplica conditional on the other. We then look at whether theposterior mean of xi,t computed using a Kalman smootherlies within two standard errors of the overall mean of 10Replica Conditional Sequential Monte Carlo0 50 100 150 200 250Time (t)05101520Autocorrelation time(a) Replica cSMC, 2 replicas.0 50 100 150 200 250Time (t)05101520Autocorrelation time(b) Replica cSMC, 75 replicas.0 50 100 150 200 250Time (t)05101520Autocorrelation time(c) Replica cSMC, 75 replicas,constant approximation to predic-tive.Figure 1. Estimated autocorrelation times for each latent variable.Different coloured lines correspond to different latent state compo-nents. The x-axis corresponds to different times.replica cSMC runs. We find this happens for about 91.4%of the xi,t. This indicates strong agreement between the an-swers obtained by replica cSMC and the Kalman smoother.Next, we investigate the effect of using more replicas. To dothis, we compare replica cSMC using 2 versus 75 replicas.We do 5 runs of each sampler. Both samplers use 100 parti-cles and we do a total of 5, 000 iterations per run. For thesampler using 75 replicas, we update replica 1 at every itera-tion and replicas 2 to 75 in sequence at every 20-th iteration.For the sampler using 2 replicas, we update both replicas atevery iteration. In both samplers, we update replica 1 withreplica cSMC and the remaining replica(s) with iteratedcSMC. After discarding 10% of each run as burn-in, we useall runs for a sampler to compute autocorrelation time.We can clearly see in Figures 1a and 1b that using morereplicas improves performance, before adjusting for compu-tation time. We note that for this simple example, there isno benefit from using replica cSMC with a large number ofreplicas if we take into account computation time.To check the performance of using the constant approxi-mation versus the exact predictive density, we run replicacSMC with 75 replicas and the same settings as earlier, ex-cept using a constant approximation to the predictive density.Figure 1c shows that using a constant approximation to thepredictive density results in peformance better than whenusing the true predictive density. This is consistent with ourdiscussion in Section 3.2.0 500 1000 1500Time (t)05101520Autocorrelation timeFigure 2. Estimated autocorrelation times for each latent variable.Different coloured lines correspond to different latent state compo-nents. The x-axis corresponds to different times.The linear Gaussian model can also be used to demonstratethat due to looking ahead, a fixed level of precision can beachieved with much fewer particles with replica cSMC thanwith standard iterated cSMC. In scenarios where the stateis high dimensional and the observations are informative, itis difficult to efficiently sample the variables xi,1 with stan-dard iterated cSMC using the initial density as the proposal.We do 20 runs of 2, 500 iterations of both iterated cSMCwith 700 particles and of replica cSMC with 35 particlesand 2 replicas, with each replica updated given the other.We then use the runs to estimate the standard error of theoverall mean over 20 runs. For the variable x1,1 sampledwith iterated cSMC we estimate the standard error to be ap-proximately 0.0111 whereas for replica cSMC the estimatedstandard error is a similar 0.0081, achieved using only 5%of the particles.Finally, we verify that the proposed method works wellon longer time series by running it on the linear Gaussianmodel but with the length of the observed sequence set toT = 1, 500. We use 2 replicas, each updated given the other,and do 5 runs of 5, 000 iterations of the sampler to estimatethe autocorrelation time for sampling the latent variables. InFigure 2 we can see that the replica cSMC method does notsuffer from a decrease in performance when used on longertime series.4.2. Two Poisson-Gaussian ModelsIn this example, we consider the two models from(Shestopaloff & Neal, 2018). Model 1 uses the same la-tent process as Section 4.1 with T = 250, d = 10 andYi,t|{Xi,t = xi,t} ∼ Poisson(exp(c + σxi,t)) for i =1, . . . , d and t = 1, . . . , T where c = −0.4 and σ = 0.6.For Model 2, we again use the latent process in Section4.1, with T = 500, d = 15 and Yi,t|{Xi,t = xi,t} ∼Poisson(σ|xi,t|)) for i = 1, . . . , d and t = 1, . . . , T whereσ = 0.8. We assume the observations are independent giventhe latent states.We generate one sequence of observations from each model.A plot of the simulated data along dimension i = 1 is shownReplica Conditional Sequential Monte Carlo0 50 100 150 200 250Time (t)0510152025303540y(a) Data for Model 1.0 100 200 300 400 500Time (t)0246810y(b) Data for Model 2.Figure 3. Simulated data from the Poisson-Gaussian models.0 50 100 150 200 250Time (t)0510152025Adjusted autocorrelation time(a) Iterated cSMC+Metropolis.0 50 100 150 200 250Time (t)0510152025Adjusted autocorrelation time(b) Replica cSMC.Figure 4. Model 1. Estimated autocorrelation times for each la-tent variable, adjusted for computation time. Different colouredlines corresponds to different latent state components. The x-axiscorresponds to different times.in Figure 3. We set the importance densities qt for thereplica cSMC sampler to the same ones as in Section 4.1,with a constant approximation to the predictive density.MODEL 1We use replica cSMC with 5 replicas, updating one replicaconditional on the other. We start with both sequencesinitialized to 0. We set the number of particles to 200. Wedo a total of 5 runs of the sampler with 5, 000 iterations,each run with a different random number generator seed.Each iteration of replica cSMC takes approximately 0.80seconds. We discard 10% of each run as burn-in.Plots of autocorrelation time comparing replica cSMC tothe best method in (Shestopaloff & Neal, 2018) for sam-pling each of the latent variables are shown in Figure 4.The benchmark method takes approximately 0.21 secondsper iteration. We can see that the proposed replica cSMCmethod performs relatively well when compared to theirbest method after adjusting for computation time. The fig-ure for iterated cSMC+Metropolis was reproduced usingcode available with (Shestopaloff & Neal, 2018).MODEL 2For this model, the challenge is to move between the manydifferent modes of the latent state due to conditioning on0 200 400 600 800 1000Time (t)-6-4-20246x1,300(a) Trace plot for x1,300.0 200 400 600 800 1000Time (t)-10-5051015x3,208x4,208(b) Trace plot for x3,208x4,208.Figure 5. Trace plots for Model 2.|xi,t| in the observation density. The marginal posteriorof xi,t has two modes and is symmetric around 0. Addi-tional modes appear due to uncertainty in the signs of statecomponents.We use a total of 50 replicas and update 49 of the 50 replicaswith iterated cSMC and one replica with replica cSMC. Thisis done to prevent the Markov chain from being stuck ina single mode while at the same time enabling the replicacSMC update to use an estimate of the backward informa-tion filter based on replicas that are distributed across thestate space. We initialize all replicas using sequences drawnfrom independent SMC passes with 1, 000 particles, andrun the sampler for a total of 2, 000 iterations. Both replicacSMC and iterated cSMC updates use 100 particles.In Figure 5 we plot every other sample of the same func-tions of state as in (Shestopaloff & Neal, 2018) of thereplica updated with replica cSMC. This is the the coor-dinate x1,300 with true value −1.99 and x3,208x4,208 withtrue value −4.45. The first has two well-separated modesand the second is ambiguous with respect to sign. We seethat the sampler is able to explore different modes, with-out requiring any specialized “flip” updates or having touse a much larger number of particles, as is the case in(Shestopaloff & Neal, 2018).We note that the replicas doing iterated cSMC updates tendto get stuck in separate modes for long periods of time,as expected. However, as long as these replicas are well-distributed across the state space and eventually exploreit, the bias in the estimate of the backward informationfilter will be low and vanish asymptotically. The samplesfrom the replica cSMC update will consequently be a goodapproximation to samples from the target density. Furtherimprovement of the estimate of the backward informationfilter based on replicas in multimodal scenarios remains anopen problem.4.3. Lorenz-96 ModelFinally, we look at the Lorenz-96 model in a low-noiseregime from (Heng et al., 2017). The state function for thisReplica Conditional Sequential Monte Carlo0 20 40 60 80 100Time (t)-4-202468yFigure 6. Simulated data from Lorenz-96 model along coordinatei = 1.model is the Itô process ξ(s) = (ξ1(s), . . . , ξd(s)) definedas the weak solution of the stochastic differential equation(SDE)dξi = (−ξi−1ξi−2 + ξi−1ξi+1− ξi +α)dt+ σfdBi (18)for i = 1, . . . , d, where indices are computed modulod, α is a forcing parameter, σ2f is a noise parameter andB(s) = (B1(s), . . . , Bd(s)) is d-dimensional standardBrownian motion. The initial condition for the SDE isξ(0) = N (0, σ2fId). We observe the process on a reg-ular grid of size h > 0 as Yt ∼ N (Hξ(th), R), wheret = 0, . . . , T . We will assume that the process is onlypartially observed, with Hii = 1 for i = 1, . . . , p and 0otherwise, for p = d− 2.We discretize the SDE (18) by numerically integrating thedrift using a fourth-order Runge-Kutta scheme and addingBrownian increments. Let u be the mapping obtained bynumerically integrating the drift of (18) on [0, h]. Thisdiscretization produces a state space model with X1 ∼N (0, σ2fI), Xt|{Xt−1 = xt−1} ∼ N (u(xt−1), σ2fhI) fort = 2, . . . , T + 1 and Yt|{Xt = xt} ∼ N (Hxt, R) fort = 1, . . . , T + 1. We set d = 16, σ2f = 10−2, R = 10−3Ipand α = 4.8801. The process is observed for 10 time units,which corresponds to h = 0.1, T = 100, and a step size of10−2 for the Runge-Kutta scheme. A plot of data generatedfrom the Lorenz-96 model along one of the coordinates isshown in Figure 6.We compare the performance of replica cSMC with tworeplicas, updating each replica conditional on the other,to an iterated cSMC scheme. For iterated cSMC, we usethe model’s initial density as q1 and the model’s transitiondensity as qt for t ≥ 2. For replica cSMC, we use thefollowing importance densities for replica k,q1(x1) ∝ f(x1)∑j 6=kφ(x1|x(j)2 ),qt(xt|xt−1) ∝ f(xt|xt−1)∑j 6=kφ(xt|x(j)t+1),qT (xT |xT−1) ∝ f(xT |xT−1), (19)where t = 2, . . . , T−1 and φ is the p-dimensional Gaussian0 200 400 600 800 1000MCMC sample-0.35-0.3-0.25-0.2-0.15x1,45(a) Standard cSMC trace, x1,45.0 200 400 600 800 1000MCMC sample-0.35-0.3-0.25-0.2-0.15x1,45(b) Replica cSMC trace, x1,45.Figure 7. Lorenz-96 model. Comparison of standard cSMC andreplica cSMC.density with meanHu−1(x(j)t+1) and variance σ2fhIp, that is,the mean is computed by running the Runge-Kutta schemebackward in time starting at the replica state x(j)t+1. Weinitialize the iterated cSMC sampler and each replica inthe replica cSMC sampler with a sequence drawn from anindependent SMC pass with 3, 000 particles. We run replicacSMC with 200 particles for 30, 000 iterations (0.7 secondsper iteration) and compare to standard iterated cSMC with600 particles, which we also run for 30, 000 iterations (0.7seconds per iteration), thus making the computational timeequal.Figure 7 shows the difference in performance of the twosamplers by trace plots of x1,45 (true value −0.23), fromone of the runs, plotting the samples every 30th iteration.We can see that replica cSMC performs noticeably betterwhen compared to standard iterated cSMC.5. ConclusionWe presented a novel sampler for latent sequences of a non-linear state space model. Our proposed method leads toseveral questions. The first is whether there are other waysto estimate the predictive density that does not result inmixture weights with high variance. Another question is todevelop better guidelines on choosing the number of replicasto use in a given scenario. It would also be interesting tolook at applications of replica cSMC in non time-seriesexamples. Finally, while the proposed method offers anapproach for sampling in models with multimodal statedistributions, further improvement is needed.Replica Conditional Sequential Monte CarloReferencesAndrieu, C., Doucet, A., and Holenstein, R. Particle Markovchain Monte Carlo (with discussion). Journal of the RoyalStatistical Society: Series B (Statistical Methodology), 72(4):269–342, 2010.Briers, M., Doucet, A., and Maskell, S. Smoothing algo-rithms for state-space models. Annals of the Institute ofStatistical Mathematics, 62(1):61–89, 2010.Chen, R., Ming, L., and Liu, J. S. Lookahead strategies forsequential Monte Carlo. Statistical Science, 28(1):69–94,2013.Finke, A., Doucet, A., and Johansen, A. On embeddedhidden Markov models and particle Markov chain MonteCarlo methods. Technical report, arXiv:1610.08962,2016.Grothe, O., Kleppe, T., and Liesenfeld, R. Bayesian analysisin non-linear non-Gaussian state-space models using par-ticle Gibbs. Technical report, arXiv:1601.01125, 2016.Guarniero, P., Johansen, A. M., and Lee, A. The iteratedauxiliary particle filter. Journal of the American Statisti-cal Association, 112(520):1636–1647, 2017.Heng, J., Bishop, A., Deligiannidis, G., and Doucet, A.Controlled sequential Monte Carlo. Technical report,arXiv:1708.08396, 2017.Leimkuhler, B., Matthews, C., and Weare, J. Ensemble pre-conditioning for Markov chain Monte Carlo simulation.Statistics and Computing, 28:277–290, 2018.Neal, R. MCMC using ensembles of states for problemswith fast and slow variables such as Gaussian processregression. Technical report, arXiv:1101.0387, 2010.Ruiz, H. C. and Kappen, H. J. Particle smoothing for hid-den diffusion processes: adaptive path integral smoother.IEEE Transactions on Signal Processing, 65(12):3191–3203, 2017.Scharth, M. and Kohn, R. Particle efficient importancesampling. J. Econometrics, 190(1):133–147, 2016.Shestopaloff, A. and Neal, R. Sampling latent states forhigh-dimensional non-linear state space models with theembedded HMM method. Bayesian Analysis, 13(3):797–822, 2018.Whiteley, N. Discussion of particle Markov chain MonteCarlo methods. Journal of the Royal Statistical Society:Series B (Statistical Methodology), 72(4):306–307, 2010.",
    "Link": "https://core.ac.uk/download/372816600.pdf"
}