{
    "Title": "Joint morphological-lexical language modeling for processing morphologically rich languages with application to dialectal Arabic",
    "Authors": "Afify, Mohamed, Deng, Yonggang, Erdogan, Hakan, Erdoğan, Hakan, Gao, Yuqing, Sarıkaya, Ruhi, Sarikaya, Ruhi",
    "Year": "No year available",
    "Abstract": "Language modeling for an inflected language\n\nsuch as Arabic poses new challenges for speech recognition and\n\nmachine translation due to its rich morphology. Rich morphology\n\nresults in large increases in out-of-vocabulary (OOV) rate and\n\npoor language model parameter estimation in the absence of large\n\nquantities of data. In this study, we present a joint\n\nmorphological-lexical language model (JMLLM) that takes\n\nadvantage of Arabic morphology. JMLLM combines\n\nmorphological segments with the underlying lexical items and\n\nadditional available information sources with regards to\n\nmorphological segments and lexical items in a single joint model.\n\nJoint representation and modeling of morphological and lexical\n\nitems reduces the OOV rate and provides smooth probability\n\nestimates while keeping the predictive power of whole words.\n\nSpeech recognition and machine translation experiments in\n\ndialectal-Arabic show improvements over word and morpheme\n\nbased trigram language models. We also show that as the\n\ntightness of integration between different information sources\n\nincreases, both speech recognition and machine translation\n\nperformances improve",
    "Keywords": "No keywords available",
    "Publisher": "'Institute of Electrical and Electronics Engineers (IEEE)'",
    "Publication Date": "No publication date available",
    "Journal": "No journal available",
    "Citation Count": 0,
    "Full Text": "T-ASL-01578-2007 \n \n1 \n  \nAbstract— Language modeling for an inflected language \nsuch as Arabic poses new challenges for speech recognition and \nmachine translation due to its rich morphology. Rich morphology \nresults in large increases in out-of-vocabulary (OOV) rate and \npoor language model parameter estimation in the absence of large \nquantities of data.  In this study, we present a joint \nmorphological-lexical language model (JMLLM) that takes \nadvantage of Arabic morphology. JMLLM combines \nmorphological segments with the underlying lexical items and \nadditional available information sources with regards to \nmorphological segments and lexical items in a single joint model.  \nJoint representation and modeling of morphological and lexical \nitems reduces the OOV rate and provides smooth probability \nestimates while keeping the predictive power of whole words. \nSpeech recognition and machine translation experiments in \ndialectal-Arabic show improvements over word and morpheme \nbased trigram language models. We also show that as the \ntightness of integration between different information sources \nincreases, both speech recognition and machine translation \nperformances improve.  \n \nIndex Terms— Language Modeling, Maximum Entropy \nModeling, Morphological Analysis, Joint Modeling.  \nI. INTRODUCTION \nhere are numerous widely spoken inflected languages. \nArabic is one of these highly inflected languages. In \nArabic, affixes are appended to the beginning or end of a stem \nto generate new words. Affixes indicate case, gender, tense, \nnumber, and many other attributes that can be associated with \nthe stem. Most natural language processing applications use \nword based vocabularies that are unaware of the \nmorphological relationships between words. For inflected \nlanguages this leads to a rapid growth of the vocabulary size. \nFor example, a parallel corpus (pairwise sentence translations) \nof 337K utterances between English and dialectal Iraqi Arabic \nhas about 24K and 80K unique words for English and Iraqi \n \nR. Sarikaya is the contact author and he is with IBM T.J. Watson Research \nCenter, Yorktown Heights, NY 10598 USA. (Tel: 914-945-3919, Fax: 914-\n945-4490; e-mail: sarikaya@us.ibm.com). \nM. Afify was with IBM T.J. Watson Research Center when the work was \ncarried out. He is now with Cairo University, Egypt. \nY. Deng and Y. Gao are with IBM T.J. Watson Research Center, \nYorktown Heights, NY 10598 USA. \nH. Erdogan is with Sabanci University, Istanbul Turkey.  \n \nArabic, respectively.  \nA standard n-gram language model computes the \nprobability of a word sequence, },...,{ 1 KwwW = , as a product \nof conditional probabilities of each word  given its history. \nThis probability is typically approximated by n-1 most recent \nwords, \n \n \n \nThere is an inverse relationship between the predictive power \nand robust parameter estimation of n-grams. As n increases the \npredictive power increases, however due to data sparsity \nlanguage model parameters may not be robustly estimated. \nTherefore, setting n to 2 or 3 appears to be a reasonable \ncompromise between these competing goals.  Robust \nparameter estimation problem is however more pronounced for \nArabic due to its rich morphology compared to non-inflected \nlanguages. One would suspect that words may not be the best \nlexical units in this case and, perhaps, morphological units \nwould be a better choice.  \nIn addition to its morphological structure, Arabic has \ncertain lexical rules for gender and number matching. For \nexample, the adjective in ftAh gydh (good girl in English) \ndiffers from the same adjective in wld gyd (good boy in \nEnglish) to match the gender, and also in ftAtAn gydtAn (two \ngood girls in English) to match the number. By examining \nerror patterns in speech recognition and machine translation \noutputs we observed that many sentences contain lexical \nmismatch errors between words. Using the above example, the \nutterance wld gyd might in some cases be recognized as wld \ngydh, where the correct adjective is replaced by the adjective \nof the wrong gender. This makes a lot of sense in speech \nrecognition because many of the lexically mismatched items \ndiffer in only one phone and are thus acoustically confusable. \nAdding gender information in the language model could help \nin reducing these errors.   This motivated us to introduce \nlexical attributes in the language model. Lexical attributes of \nthe vocabulary, e.g. number, gender, and type are manually \nmarked. These attributes will be discussed in more detail in \nSection V. \nIn this work, we present a new language modeling \ntechnique called Joint Morphological-Lexical Language \nModel (JMLLM) for inflected languages in general and Arabic \nJoint Morphological-Lexical Language \nModeling for Processing Morphologically Rich \nLanguages with Application to Dialectal Arabic \nRuhi Sarikaya, Mohamed Afify, Yonggang Deng, Hakan Erdogan, Yuqing Gao \nT \n∏\n=\n+−−\n≈\nK\ni ni\nwiwiwPWP 1\n)1,...1|()(     \nT-ASL-01578-2007 \n \n2 \nin particular and apply it to speech recognition and machine \ntranslation tasks. JMLLM models the dependencies between \nmorpheme and word n-grams, attribute information associated \nwith morphological segments1 and words. These dependencies \nare represented via a tree structure called Morphological-\nLexical Parse Tree (MLPT). MLPT is used by JMLLM to \ntightly integrate information sources provided by a \nmorphological analyzer with the lexical information in a single \njoint language model. MLPT is a generic structure and it can \ninclude, if available, other information sources about the \nlexical items (i.e. lexical attributes, syntactic/semantic \ninformation), or the sentence (i.e. dialog state).  \nJMLLM is simply a joint distribution defined over the \nvocabularies of the leaves (morphemes in our case) and non-\nterminal nodes of the MLPT. In our implementation, we use a \nmaximum entropy model to represent this joint distribution \nwith a set of features given in Section V-D. Loosely speaking, \nthis maximum entropy model can be viewed as an \ninterpolation of distributions of the nodes of the tree and hence \nprovides a desirable smoothing effect on the final distribution. \nJMLLM also improves the dictionary’s coverage of the \ndomain and reduces the out-of-vocabulary (OOV) rate by \npredicting morphemes while keeping the predictive power of \nwhole words. This model statistically estimates the joint \nprobability of a sentence and its morphological analysis.  \nIn the above presentation of the model and also in the \nmodel description in Section V, to be precise, we restrict our \ndiscussion to a certain configuration of the tree. For example, \nwe associate morphemes to leaves, and limit the internal tree \nnodes to the morphological attributes, lexical items and their \nattributes. However any sensible choice of the leaf nodes or \ninternal tree nodes can be covered by the presented model. \nEven though in our implementation we use a deterministic \nparse provided by a rule based segmentation method, the \nproposed model also accommodates the case of probabilistic \nparses.  \nThe rest of the paper is organized as follows. Section II \nprovides an overview of prior work addressing language \nmodeling for morphologically rich languages. Section III \ndescribes our morphological segmentation method. A short \noverview of maximum entropy modeling is given in Section \nIV. The proposed JMLLM is presented in Section V. Section \nVI describes the speech recognition and statistical machine \ntranslation (SMT) architecture. Experimental results and \ndiscussions are provided in Section VII, followed by the \nconclusions. \nII. RELEVANT PREVIOUS WORK  \nRecently, there has been a number of new studies aimed at \naddressing robust parameter estimation and rapid vocabulary \ngrowth problems for morphologically rich languages by using \nthe morphological units to represent the lexical items \n[1,2,3,4,34].  Even though Arabic is receiving much of the \nattention, there are many other morphologically rich languages \n \n1\n We use “morphological segment” and morpheme interchangeably. \nfacing the same language modeling issues [25, 26, 30, 34].  In \nall of the mentioned studies above the use of morphological \nknowledge at the modeling stage is limited to only segmenting \nthe words into shorter morphemes. In these models the \nrelationship between the lexical items and morphemes is not \nmodeled explicitly. Instead, two separate language models are \nbuilt on the word based original corpus and segmented corpus \nand they are interpolated. However, in most of these studies \nmorpheme sequence generation process in speech recognition \nor machine translation decoding is further constrained [4] by \nsome rule based mechanisms exploiting the knowledge of the \nmorphological segmentation algorithm. For example, if lexical \nitems are segmented into one or more prefixes followed by a \nstem, which is also followed by one or more suffixes, then a \nsuffix cannot follow a prefix without having a stem coming \nbefore it. \nFactored Language Models (FLMs) [5, 14] are different \nthan the previous methods and are similar to JMLLM  to some \nextent. Unlike other approaches, in both FLM and JMLLM the \nrelationship between lexical and morphological items are \nexplicitly modeled within a single model. In an FLM words are \ndecomposed into a number of features and the resulting \nrepresentation is used in a generalized back-off scheme to \nimprove robustness of probability estimates for rarely \nobserved word n-grams.  In an FLM, each word is viewed as a \nvector of k factors: },...,{ 1 Kiii ffw = . An FLM provides the \nprobabilistic model ),...,|( 1 NfffP  where the prediction of \nfactor f is based on N parents },...,{ 1 Nff . For example, if w \nrepresents a word token and t represents a part-of-speech \n(POS) tag, the model, ),,|( 112 −−− iiii twwwP , predicts the current \nword based on traditional n-gram model as well as POS tag of \nthe previous word. The main advantage of FLMs compared to \nprevious methods is that they allow users to put in linguistic \nknowledge to explicitly model the relationship between word \ntokens and POS, or morphological information. Like n-gram \nmodels, smoothing techniques are necessary in parameter \nestimation. In particular, a generalized back-off scheme is used \nin training an FLM. Our approach uses maximum entropy \nmodeling as opposed to direct maximum likelihood modeling \nused in FLMs.  \nIII. MORPHOLOGICAL ANALYSIS  \nApplying morphological segmentation to data improves the \ndomain coverage of the dictionary used for speech recognition \nor machine translation and reduces the OOV rate. Even though \nTABLE I \nPREFIX AND SUFFIX LIST FOR DIALECTAL IRAQI  ARABIC IN BUCKWALTER \nREPRESENTATION  \nPrefix List chAl, bhAl, lhAl, whAl, wbAl, wAl, bAl, \nhAl, EAl, fAl, Al, cd, ll, b, f, c, d, w \nSuffix List thmA, tynA, hmA, thA, thm, tkm, tnA, tny, \nwhA, whm, wkm, wnA, wny, An, hA, hm, \nhn, km, kn, nA, ny, tm, wA, wh, wk, wn, yn, \ntk, th, h, k, t, y  \n \nT-ASL-01578-2007 \n \n3 \n \n !S! \n\u0001\u0002\u0003\u0004\u0005\u0006\u0007 \u0001\b\u0006\t\n\u000b\f\r \n\u000e\u000f\u0010\u0011 \u0001\b\u0006\t\n\u0012 \nNMP NFS V \nstem prefix stem \n\u0001\u0013\u0014\u0015\u0006\f\u000f\u0016 \nNFS \náÇ \u0004\u0005\u0017 äæ í  \u0013\u0014\u0015\u0017 \u0018\u000f\u0016 \nsuffix prefix stem stem prefix \n \n \nFig. 1.  Morphological Lexical Parse Tree (MLPT) for a dialectal-Arabic \nsentence. \n \nthere is a large volume of segmented data available for \nModern Standard Arabic (MSA), we do not know of any such \ndata for training a statistical morphological analyzer to \nsegment Iraqi Arabic language. In fact, Iraqi Arabic is so \ndifferent than MSA, we are not aware of any study leveraging \nthe MSA text resources to improve Iraqi Arabic language \nmodeling or machine translation. \nIn this section, we present a word segmentation algorithm \nthat is used to generate the morphological decomposition \nneeded by the proposed language models. This algorithm was \ninitially proposed in [4]. Starting from predefined lists of \nprefixes and suffixes (affixes) the segmentation algorithm \ndecomposes each word in a given vocabulary into one of three \npossible forms: {prefix+stem, stem+suffix, \nprefix+stem+suffix}, or leaves it unchanged. Although affixes \nin Arabic are composite, i.e. a word can start (end) with \nmultiple prefixes (suffixes), we found in preliminary \nexperiments that allowing multiple affixes leads to a large \ninsertion rate in the decoded output and results in worse \noverall performance. For this reason, we decided to only allow \na single prefix and/or suffix for each stem. In our \nimplementation, we use the sets of prefixes and suffixes given \nin Table I, in Buckwalter transliteration [6], for dialectal Iraqi \nArabic. \nThe most straightforward way to perform the  \ndecomposition is to do blind segmentation using the longest \nmatching prefix and/or suffix in the list. However, the \ndifficulty with blind segmentation is that sometimes the \nbeginning (ending) part of a word agrees with a prefix (suffix).  \nThis leads to illegitimate Arabic stems. For example, the word \nAlqY2 (threw in English), a verb that should not be \ndecomposed, has its initial part agreeing with the popular \nprefix Al. In this case blind segmentation leads to the \ndecomposition Al-qY and hence to the invalid stem qY.  In \norder to avoid this situation we employ the following \nsegmentation algorithm. The algorithm still relies on blind \nsegmentation but accepts a segmentation only if the following \nthree rules apply: \n(1) The resulting stem has more than two characters. \n(2) The resulting stem is accepted by the Buckwalter   \nmorphological analyzer [6].  \n(3) The resulting stem exists in the original dictionary. \nThe first rule eliminates many of the illegitimate \nsegmentations. The second rule ensures that the word is a valid \nstem in the Buckwalter morphological analyzer list. The \nBuckwalter morphological analyzer provides a decent \ncoverage of Modern Standard Arabic (MSA). It was found \nexperimentally that for a news corpus it only misses about 5% \nof the most frequent 64K words and that most of the missed \nwords are typos and foreign names. Unfortunately, the fact that \nthe stem is a valid Arabic stem does not always imply that the \nsegmentation is valid. The third rule, while still not offering \nsuch guarantee, simply prefers keeping the word intact if its \nstem does not occur in the lexicon. The rationale is that we \n \n2\n Using Buckwalter Arabic transliteration. \nshould not allow a segmentation that may cause an error, if it is \nnot going to reduce the size of the lexicon.  \n     Even after applying the above rules there could still be \nsome erroneous decompositions, and we indeed found a very \nsmall number of them by visual inspection of the decomposed \nlexicon. However, we do not provide a formal “error rate” of \nthe segmentation because this would require a manually \nsegmented reference lexicon. A useful heuristic that can \nmitigate the effect of these residual errors is to keep the top-N \nfrequent decomposable words intact. A value of N=5000 was \nexperimentally found to work well in practice.  \nUsing a morphological segmentation algorithm will \nproduce affixes in the speech recognition and machine \ntranslation outputs. These affixes should be glued to the \nfollowing or previous word to form meaningful words. To \nfacilitate such gluing each prefix and suffix is marked with a - \n(e.g. we have prefix Al- or suffix –yn). Two gluing schemes \nare used. The first is very simple and just sticks any word that \nstarts(ends) with a - to the previous(following) word. The \nsecond tries to apply some constraints to prevent sequences of \naffixes and to ensure that these affixes are not attached to \nwords that start(end) with a prefix(suffix). No noticeable \ndifference is seen between the two approaches. \nA few words about the morphological decomposition \nalgorithm are worth mentioning here. First, this is more of a \nword segmentation algorithm than a morphological \ndecomposition algorithm in a strict linguistic sense. However, \nit is very simple to apply and all it needs is a list of affixes and \na lexicon. In previous work [4], we found that using this \nalgorithm to tokenize the lexicon and the language model data \nleads to significant reduction in word error rate. This was a \nmajor motivation in using it in more elaborate language model \nschemes as discussed in the rest of this paper. \nIV. MAXIMUM ENTROPY MODELING \n \nThe Maximum Entropy method is a flexible statistical \nmodeling tool that has been widely used in many areas of \nnatural language processing [9, 12, 27]. Maximum entropy \nmodeling produces a probability model that is as uniform as \npossible while matching empirical feature expectations \nexactly. This can be interpreted as making as few assumptions \nT-ASL-01578-2007 \n \n4 \nas possible in the model. Maximum entropy modeling \ncombines multiple overlapping information sources (features). \nFor an observation o (e.g. a morpheme or word) and a history \n(context) h, the probability model is given by:  \n \n∑ ∑\n∑\n=\n'\n)),'(exp(\n)),(exp(\n)|(     \no j\ni\nhojfj\nhoifi\nhoP λ\nλ\n \nNotice that the denominator includes a sum over all possible \noutcomes, o', which is essentially a normalization factor for \nprobabilities to sum to 1. The functions\ni\nf  are usually referred \nto as feature functions or simply features. In the context of \nnatural language processing using binary feature functions is \nvery popular. These binary feature functions are given as:  \n {   \notherwise   ,0 \n1)( and  if  ,1 ),( === hiqioohoif  \nwhere \nio  is the outcome associated with feature if  and  \n)( hq\ni\n is an indicator function on history.  \nFor example, a bigram feature \nif   representing the word \nsequence “ARABIC LANGUAGE\" in maximum entropy \nmodeling would have  io  = “LANGUAGE\" and )(hqi  would \nbe the question “Does the context h contain the word \n“ARABIC\" as the previous word of the current word ?\".  The \nmodel parameters are denoted by iλ , which can be considered \nas weights associated with feature functions.  There are several \nmethods to smooth maximum entropy models to avoid \novertraining [9]. The most effective smoothing method, as \nshown in [9], is an instance of fuzzy maximum entropy \nsmoothing. This type of smoothing amounts to adding a zero-\nmean Gaussian prior to each parameter. The only smoothing \nparameters to be determined are variance terms for each \nGaussian. In our experiments, we used the same variance value \nfor all model parameters. This fixed value was optimized on a \nheld-out set using Powell's algorithm [31]. \nBesides Maximum Entropy method, another alternative \nmachine learning approach that can be used for our task is \nmemory based learning (MBL) [28]. MBL can represent \nexceptions that are crucial for linguistics. Similar to MBL, \neach instance, including exceptions is represented as a feature \nin maximum entropy modeling. However, unlike MBL, \nmaximum entropy method may forget about individual \ninstances if there is feature selection/pruning during model \ntraining. In this study, we did not perform any feature \nselection. If there is any exception represented in the form of a \nfeature, they will not be lost. However, maximum entropy \nmethod weighs a set of features (evidences) to prefer one \noutcome over the other. If the contribution of the feature \nbelonging to an exception is not sufficiently high, then the \nexception may not be predicted correctly. This phenomenon \nmay look like a disadvantage at first, but it can also show the \nstrength of maximum entropy modeling. That is, a set of \nevidences related to an outcome are weighted to assign a \nprobability to that outcome. The weights are learned via \nimproved iterative scaling (IIS) algorithm [9]. The main \nreason for using the maximum entropy method is its flexibility \nin integrating overlapping information sources into the model. \nThis is a desirable feature for integrating morphological and \nlexical attributes in the language model.  \nBecause of the aforementioned advantages we use the \nmaximum entropy method for implementing JMLLM. The \nmaximum entropy method allows JMLLM to incorporate \nlexical items, morphemes as well as attributes associated with \nthese lexical items and morphemes into the language model. \nThe maximum entropy method has been used in language \nmodeling before, in the context of n-gram models [9], whole \nsentence models [13], syntactic structured language models [7] \nand semantic structured language models [8]. So far, the use of \nmorphology for language modeling has been largely limited to \nsegmenting words into morphemes to build a morpheme based \nlanguage model. Language specific information such as \nmorphological and lexical attributes is overlooked. \nAdditionally, joint modeling of these information sources \nrather than using them as sources to derive features has not \nbeen considered.  Integrating all available information sources \nsuch as morphological and language specific features in a \nsingle model could be very important to improve both speech \nrecognition and machine translation performance. Next, we \npresent the maximum entropy based Joint Morphological-\nLexical Language Modeling (JMLLM) method.  \nV. JOINT MORPHOLOGICAL-LEXICAL LANGUAGE MODELING \nThis section describes in detail the JMLLM models. Before \ndiscussing the models we will present the morphological-\nlexical parse tree (MLPT) which represents the information \nsources and their dependencies used in the model. We will \nalso discuss two implementations of the JMLLM which we \nrefer to as JMLLM-leaf and JMLLM-tree.  \nA. Morphological-Lexical Parse Tree \nThe MLPT consists of a tree structured joint representation \nof the lexical and morphological items in a sentence and their \nassociated attribute information. An example of an MLPT for \nan Arabic sentence is given in Fig. 1. The leaves of the tree are \nmorphemes that are predicted by the language model. Each \nmorpheme has one of the three attributes: {prefix, stem, \nsuffix} as generated by the morphological analysis mentioned \nin Section III. In addition to the morphological attributes, each \nword can take three sets of attributes: {type, gender, number}. \nWord type can be considered as POS, but here we consider \nonly nouns (N), verbs (V) and remaining words are labeled as \n“other” (O). Gender can be masculine (M) or feminine (F). \nNumber can be singular (S), plural (P) or double (D) (this is \nspecific to Arabic).  For example, the label “NMP” for the first \nword, \u0001\u0002\u0003\u0004, shows that this word is a noun (N), male (M), and \nplural (P).   \nT-ASL-01578-2007 \n \n5 \nThe MLPT given in Fig. 1 is built by starting with a \nsequence of decomposable words, which is in the middle row. \nThen, a morphological analysis is applied to the word \nsequence to generate the morpheme sequence along with their \nmorphological attributes. We have a lexical attribute table \nprepared by human annotators for all the words in the training \ndata. This table contains lexical attributes mentioned above. \nThe result of the morphological analysis together with the \nlexical attributes is used to fill the corresponding nodes in the \ntree.  \nThe dependencies represented in MLPT are integrated in \nJMLLM. We hypothesize that as we increase the amount of \ninformation represented in the MLPT and the tightness of \nintegration, the JMLLM performance should improve. \nApplying morphological segmentation to data improves the \ndictionary’s coverage of the domain and reduces the OOV \nrate. For example, splitting the word, \u0005\u0006\u0007\b\t\u0002\n as \u000b\u0002\n (prefix) and \n\u0005\u0006\u0007\f (stem) as in Fig. 1, allows to decode other combinations of \nthis stem with the prefixes and suffixes provided in Table I.  \nThese additional combinations will hopefully cover those \nwords in the test data that have not been seen in the \nunsegmented training data.  \nB. JMLLM Basics \nIn language modeling, we are interested in estimating the \nprobability of the morpheme sequence, P(M). Formally, we \ncan compute P(M) by summing over all possible MLPT \nparses:  \n \n∑=\nC\nCMPMP ),()(\n \nwhere C denotes a parse tree that includes all the information \nin the non-terminal nodes of an MLPT. Note that any MLPT is \ncomposed of two parts, M and C.  Here, MC  is the most likely \nparse tree (in statistical parsing) or the proposed single parse \nof the morpheme sequence (in rule based segmentation). Note \nthat we do not need to specify the way the parsing is done, \nwhether it is deterministic, as used in this paper, or statistical.  \nGiven a proposed parse tree MC , we can calculate )( MCP  or \n),( MCMP  based on all possible parses seen in the training \ndata. The reasoning behind using ),( MCMP  as the language \nmodel score is that it relies not only on the morphological \nhistory, but also on lexical, and attribute history in the \nsentence and can be more indicative of the meaningfulness of \nthe morpheme sequence M. Using the joint probability of the \nword sequence and syntactic parse tree [35] or semantic parse \ntree [8] as the language model score yielded encouraging \nimprovements. We also adopt the same approach in this paper \nby estimating the probability of MLPT for the language model \nscore. \nAnother reasonable choice for language model score is to \nconsider the parse tree MC  as given information and calculate \nthe conditional probability  )|( MCMP  as the language model \nscore. The relation between the conditional and joint \nprobabilities is given as:    \n)()|(),(    \nMMM\nCPCMPCMP =\n \nHere, we interpret )( MCP as the probability of a parse among \nall possible parses in the language of interest, and calculating \n)( MCP  is possible regardless of the method of generating the \nparse tree MC , whether the parsing is deterministic or \nprobabilistic. However in this paper, we do not need to \ncalculate )( MCP  separately, since we either calculate \n)|( MCMP  or ),( MCMP  directly in our models. \nWe refer to the model predicting )|( MCMP  as JMLLM-leaf \nsince it predicts the morpheme sequence (at the leaves of \nMLPT) given the parse information. JMLLM-leaf represents a \n“loose integration” of information between morpheme \nsequence and its parse tree since it assumes the parse tree MC  \nas part of the “world” information. Another interpretation of \nJMLLM-leaf is that the parse probability )( MCP  is assumed to \nbe 1 in the expression for the joint probability ),( MCMP , thus \nit is assumed that )( MCP  does not affect the computation of \n),( MCMP .  \nThe model predicting the joint probability ),( MCMP  is \ncalled JMLLM-tree since all the information in the MLPT is \nused directly to calculate the joint probability. The joint \nprobability is estimated by multiplying the probability of the \nnon-terminal nodes with the probability of the morpheme \nsequence. This model represents a “tight integration” of all \navailable information sources in the MLPT.  \nThe first step in building the JMLLM is to represent MLPT \nas a sequence of morphemes, morphological attributes, words, \nand word attributes using a bracket notation [8]. Converting \nthe MLPT into a text sequence allows us to group lexically \nrelated morphological segments and their attributes. In this \nnotation, each morpheme is associated (association is denoted \nby “=\") with an attribute (i.e. prefix/stem/suffix) and the \nlexical items are represented by opening and closing tokens, \n[WORD and WORD] respectively. Lexical attributes are \nrepresented as an additional layer of labels over the words. \nThe parse tree given in Fig. 1 can be converted into a token \nsequence in text format as  shown below. Note that Arabic is \nread from right to left. \n \n[!S! [NMP \u0001\u0002\u0003\u0004=stem NMP] [NFS [\r\b\u000e\u000f\u0010\t\u0011 \r\b\u000e\u000f\u0012=stem áÇ=prefix \n\u000f\u0010\t\u0011\r\b\u000e ] NFS] [V [\u0013\u0014\u0015\u0016\b\u0017 äæ=suffix  \u0015\u0016\f =stem í =prefix \u0013\u0014\u0015\u0016\b\u0017]  \nV] [NFS [\u0005\u0006\u0007\b\t\u0002\n \u0005\u0006\u0007\b\t\u0002\n=stem  \u000b\u0002\n=prefix  \u0005\u0006\u0007\b\t\u0002\n] NFS] !S!] \nT-ASL-01578-2007 \n \n6 \n \nThis representation uniquely defines the MLPT given in Fig. 1. \nHere, lexical attributes can be used as joint labels as in “NFS” \nor three separate labels: “N, F, S”.  \nNext, we explain how the bracket representation can be \nused to train two different JMLLM models and determine \nfeatures. \nC. JMLLM for Morphological-Lexical Parse Tree Leaf \nPrediction: JMLLM-leaf \n In this model, we decompose the conditional probability \nexpression )|( MCMP  as follows:  \n \n∏\n=\n=\nN\ni\niiM hmPCMP\n1\n)|()|(\n \nHere, mi denotes the ith morpheme in the morpheme sequence \nM, where M has N morphemes. hi represents the history for the \nmorpheme mi and includes all tokens appearing before mi in \nthe bracket notation given above. Thus, in the history part, we \ncan use the non-terminal nodes of the MLPT parse tree along \nwith the previous morphemes. This model loosely integrates \nthe parse and the morpheme sequence by assuming a \nconditional dependence of M on the non-terminal nodes of the \nparse tree. \nAlthough we may use all parse tree information in our \nhistory, since MC  is assumed to be given, we only use a \nsubset, corresponding to the tokens appearing before mi in the \nbracket notation. This enables the models we develop to be \nused in real-time decoding (if real-time parsing can be done as \nwell) or lattice rescoring. We explain features used in JMLLM-\nleaf and JMLLM-tree in Section V.E.   \nD. JMLLM for Entire Morphological-Lexical Parse Tree \nPrediction: JMLLM-tree \nIn the previous section, we decomposed the probability \ncomputation into two parts. However, it is possible to jointly \ncalculate the probability of the morpheme sequence and the \nMC  within a single model. JMLLM-tree directly calculates \n),( MCMP  and, thus “tightly integrates” the parse and language \nmodel probabilities. To facilitate the computation of the joint \nprobability, we use the bracket notation introduced earlier to \nexpress an MLPT. This representation makes it easy to define \na joint statistical model since it enables the computation of the \nprobability of both morpheme and word tokens using similar \ncontext information. Unlike loose-integration, tight-integration \nrequires every token in the bracket representation to be an \noutcome of the joint model. Thus, the model outcome \nvocabulary, \nLAMAWM VVVV ∪∪∪=ℜ , is the union of morpheme, \nword, morphological attribute and lexical attribute vocabulary. \nNote that for each item in the word and lexical attribute \nvocabularies there is an opening and closing bracket version. \nWe represent the joint probability ),(\nM\nCMP   as: \n∏\n=\n−\n=\nT\ni\niiM tttPCMP\n1\n11 ),...,|(),(      \nwhere it  is a token in the bracket notation and T is the total \nnumber of tokens.  We note that the feature set for training the \nJMLLM models stays the same and is independent of the \n“tightness of integration”.   \nE. Features Used for JMLLM \nJMLLM can employ any type of questions one can derive \nfrom MLPT to predict the next morpheme. In addition to \ntrigram questions about previous morphemes, questions about \nthe attributes of the previous morphemes, parent lexical item \nand attributes of the parent lexical item can be used. The set of \nquestions used in the model are as follows: \n \n• Unigram history (empty history). \n• Previous morpheme: 1−im  (bigram feature) \n• Previous two morphemes: \n21\n,\n−− ii\nmm  (trigram feature). \n• Immediate parent word ( iw ) for the current morpheme \n( im ). \n• Previous parent word ( 1−iw ) \n• Morphological attributes for the previous two morphemes \n(\n21\n,\n−− ii\nmama ). \n• Lexical attributes for the current parent word ( iwa ). \n• Lexical attributes for the previous parent word ( 1−iwa ). \n• Previous token: \n1−i\nt  (token bigram feature). \n• Previous two tokens: \n21\n,\n−− ii\ntt  (trigram token features). \n•  Previous morpheme and its parent word (\n11\n,\n−− ii\nwm ).  \nThe history given in )|( hoP  consists of answers to these \nquestions.  Clearly, there are numerous questions one can ask \nfrom the MLPT in addition to the list given above. The “best” \nfeature set depends on the task, information sources and the \namount of data. In our experiments, we have not exhaustively \nsearched for the best feature set but rather used a small subset \nof these features (listed above) which we believe are helpful \nfor predicting the next morpheme.  It is also worth noting that \nwe did not use morpheme 4-gram features nor word 3-gram \nfeatures. Therefore, morpheme trigram language model can be \nconsidered as a fair baseline to compare JMLLMs to. \nThe language model score for a given morpheme using \nJMLLM is conditioned not only on the previous morphemes \nbut also on their attributes, the lexical items and their \nmorphological and lexical attributes. Therefore, the language \nmodel scores are expected to be smoother compared to n-gram \nmodels especially for unseen morpheme n-grams. For \nexample, during decoding we want to estimate the probability \nT-ASL-01578-2007 \n \n7 \nof “P(estimate | probability, smooth)”. However, assume that \nwe observe neither “smooth probability estimate” nor \n“probability estimate” in the training data. In n-gram modeling \nwe back off to unigram probability for “estimate”. On the \nother hand, in JMLLM, the n-gram features (trigram, bigram \nand unigram) are only 3 of the 11 features we listed above.  \nTypically, in addition to unigram feature there will be several \nfeatures that are active (e.g., lexical attributes, morphological \nattributes, or parent lexical item for the current word or \nprevious word). The probabilities of these features are added \nto the unigram probability, which may result in a smoother \nprobability estimate than the unigram probability alone. \nHowever, we do not know of a way to quantify this \nsmoothness. \nVI. SYSTEM ARCHITECTURES \nA. Speech Recognition Architecture \nThe speech recognition experiments are conducted on an \nIraqi Arabic speech recognition task, which covers the military \nand medical domains. The acoustic training data consist of \nabout 200 hours of speech collected in the context of IBM’s \nDARPA supported speech-to-speech (S2S) translation project \n[10].  \nThe speech data is sampled at 16kHz and the feature \nvectors are computed every 10ms. First, 24-dimensional \nMFCC features are extracted and appended with the frame \nenergy. The feature vector is then mean and energy \nnormalized. Nine vectors, including the current vector and four \nvectors from its right and left contexts, are stacked leading to a \n216-dimensional parameter space. The feature space is finally \nreduced from 216 to 40 dimensions using a combination of \nlinear discriminant analysis (LDA) and maximum likelihood \nlinear transformation (MLLT). This 40-dimensional vector is \nused in both training and decoding. \nWe use 33 graphemes representing speech and silence for \nacoustic modeling. These graphemes correspond to letters in \nArabic plus silence and short pause models. Short vowels are \nimplicitly modeled in the neighboring graphemes. The reason \nfor using grapheme models instead of the more popular phone \nmodels is as follows. Arabic transcripts are usually written \nwithout short vowels, and hence using phone models requires \nrestoring these short vowels; a process known as vowelization. \nDoing this manually is very tedious, and automatic \nvowelization is error-prone especially for dialectal Arabic. In \nnumerous experiments with vowelization of the training data \nand hence building phone models we were not able to \noutperform the grapheme system. This is in contrast to MSA \nwhere it was found that phone models are better than the \ngraphemes [33]. This was achieved largely because of an \naccurate vowelization process supplied by the Buckwalter \nanalysis. Each grapheme is modeled with a 3-state left-to-right \nhidden Markov model (HMM).  \nAcoustic model training proceeds as follows. Feature \nvectors are first aligned, using initial models, to model states. \nA decision tree is then built for each state using the aligned \nfeature vectors by asking questions about the phonetic context; \nquinphone questions are used in this case. The resulting tree \nhas about 2K leaves. Each leaf is then modeled using a \nGaussian mixture model. These models are first bootstrapped \nand then refined using three iterations of forward-backward \ntraining. The current system has about 75K Gaussians.   \nThe language model training data has 2.8M words with \n98K unique words and it includes acoustic model training data \nas a subset. The pronunciation lexicon consists of the \ngrapheme mappings of these unique words. The mapping to \ngraphemes is one-to-one and there are very few pronunciation \nvariants that are supplied manually mainly for numbers. A \nstatistical trigram language model using Modified Kneser-Ney \nsmoothing [23, 29] has been built for both the unsegmented \ndata, which is referred to as Word-3gr, and the \nmorphologically analyzed data, which is called Morph-3gr. \nA static decoding graph is compiled by composing the \nlanguage model, the pronunciation lexicon, the decision tree, \nand the HMM graphs. This static decoding scheme, which \ncompiles the recognition network off-line before decoding, is \nbecoming very popular in speech recognition [32]. The \nresulting graph is further optimized using determinization and \nminimization to achieve a relatively compact structure. \nDecoding is performed on this graph using a Viterbi beam \nsearch.  \nB. Statistical Machine Translation System \nStatistical machine translation training starts with a \ncollection of parallel sentences. We train 10 iterations of IBM \nModel-1 followed by 5 iterations of word-to-word HMM [11]. \nModels of two translation directions, from English to Iraqi \nArabic and from Iraqi Arabic to English, are trained \nsimultaneously for both Model-1 and HMM. More \nspecifically, let ( )feC n fe ,)(→  be the number of times (soft \ncount, collected in the E-step of the Expectation Maximization \n(EM) algorithm) that the English word e generates the foreign \nword f in the direction from English to Arabic at iteration n. \nSimilarly let ( )efC n ef ,)( →  be the corresponding number of \ntimes that f generates e in the other direction. To estimate the \ntranslation lexicon from English to foreign language in the M-\nstep of  the EM algorithm, we linearly combine counts from \ntwo directions and use that to re-estimate the  word-to-word \ntranslation probability )|( eft  at iteration (n+1):  \n \n( ) ( )efCfeCfeC n efn fen ,)1(,)( )()()( →→ −+⋅=→ αα  \n∑ →\n→\n=\n+\n'\n)(\n)(\n)1(\n)'(\n)()|(\nf\nn\nn\nn\nfeC\nfeC\neft  \nwhere ]1,0[∈α is a scalar controlling the contribution of \nstatistics from the other direction. A higher value of \nα indicates less proportion of soft counts borrowed from the \nother direction. We fix α  value to be 0.5 for a balanced \nlexicon. Similarly, we can re-estimate word-to-word \ntranslation probability )|( fet  at iteration (n+1). \nAfter HMM word alignment models are trained, we \nT-ASL-01578-2007 \n \n8 \nperform a Viterbi word alignment procedure in two directions \nindependently. By combining word alignments in two \ndirections using heuristics [17], a single set of static word \nalignments is then formed. Phrase translation candidates are \nderived from word alignments. All phrase pairs which respect \nthe word alignment boundary constraint are identified and \npooled together to build phrase translation tables in two \ndirections using the maximum likelihood criterion with \npruning. We set the maximum number of words in Arabic \nphrases to be 5. This will finish the phrase translation training \npart. \nThe translation engine is a phrase based multi-stack \nimplementation of log-linear models similar to Pharaoh [15]. \nGiven an English input e, the decoder is formulated as a \nstatistical decision making process that aims to find the \noptimal foreign word sequence f* by integrating multiple \nfeature functions: \n∑\n=\n=\nK\nk\nefhf kk\n1\n),(argmax*\nf\nλ\n \nwhere \nkλ  is the weight of feature function kh . Like most other \nmaximum entropy based translation engines, active features in \nour decoder include translation models in two directions, IBM \nModel-1 style lexicon weights in two directions, language \nmodel, distortion model, and sentence length penalty. These \nfeature weights (\nkλ ) are tuned discriminatively on the \ndevelopment set to directly maximize the translation \nperformance measured by an automatic error metric (such as \nBLEU [18]) using the downhill simplex method [16]. The \ndecoder generates an N-best list, which can be re-scored  using \na different model, such as an improved language model, in a \npost-processing stage to generate the final translation output. \nVII. EXPERIMENTAL RESULTS \nA. Speech Recognition Experiments \nWe mentioned that the language model training data has \n2.8M words with 98K unique lexical items. The \nmorphologically analyzed training data has 58K unique \nvocabulary items. The test data consists of 2719 utterances \nspoken by 19 speakers. It has 3522 unsegmented lexical items, \nand morphological analysis reduces this figure to 3315.  \nIn order to evaluate the performance of JMLLM, a lattice \nwith a low lattice error rate is generated by a Viterbi decoder \nusing the word trigram model (Word-3gr) language model. \nFrom the lattice at most 200 (N=200) sentences are extracted \nfor each utterance to form an N-best list. These utterances are \nrescored using the JMLLM and the morpheme trigram \nlanguage model (Morph-3gr). The language model rescoring \nexperiments are performed for the entire corpus, which has \n460K utterances and half the corpus, which has 230K \nutterances. The last column in Table II presents results for the \n460K corpus. The first entry (18.4%) is the oracle error rate of \nthe N-best list. Morph-3gr error rate is 0.9% better than that of \nthe Word-3gr. Log-linear interpolation of these language \nmodels provides a small improvement (0.3%) over Morph-3gr. \nIn a previous study [20], we reported results for “loosely \nintegrated” JMLLM (JMLLM-leaf) which are provided here. \nJMLLM-leaf obtains 30.5%, which is 1.7% and 0.8% better \nthan Word-3gr and Morph-3gr, respectively. Interpolating \nJMLLM-leaf with Word-3gr improves the WER to 29.8%, \nwhich is 1.2% better than that of the interpolation of Word-3gr \nand Morph-3gr. The interpolation weights are set equally to \n0.5 for each LM.  Adding the Morph-3gr in a three way \ninterpolation does not provide further improvement.   \n      In this study, we also provide results for “tightly \nintegrated” JMLLM (JMLLM-tree). JMLLM-tree provides an \nadditional 0.6% improvement over JMLLM-leaf. Interpolating \nJMLLM-tree with Morph-3gr and Word-3gr improves the \nWER by 0.5% and 0.7%, respectively compared to JMLLM-\ntree. Again three-way interpolation does not provide additional \nimprovement. Even though JMLLMs are not built using 4-\ngram morpheme features, it is valuable to report the Morph-\n4gr results. The Morph-4gr language model achieves 30.6% \nWER. \nIn order to investigate the impact of different amounts of \ntraining data on the proposed methods, the experiments \ndescribed above are repeated with 230K utterance corpus. The \nresults are provided in the middle column of Table II. Morph-\n3gr still outperformed Word-3gr.  However, the results with \nhalf the data reveal that Morph-3gr becomes more effective \nthan the Word-3gr, when interpolated with both JMLLM-leaf \nand JMLLM-tree. We believe this is because of the fact that \ndata sparseness has a more severe impact on Word-3gr than it \nhas on Morph-3gr. Interpolating JMLLM-tree with Morph-3gr \nprovided the best result (35.9%), which is 1.7% better than \nWord-3gr + Morph-3gr. \nIn summary, for the complete training corpus, JMLLM-tree \nalone achieves a 2.3% and 1.4% absolute error reductions \ncompared to Word-3gr and Morph-3gr, respectively. When \ninterpolated with Word-3gr, JMLLM-tree obtains 1.8% \nabsolute error reduction compared to interpolated Word-3gr \nand Morph-3gr. Standard p-test3 shows that these \nimprovements are significant at p<0.001 level.  \n \n3\n We used the Matched Pairs Sentence-Segment Word Error (MAPSSWE) \ntest, available in standard SCLITE’s statistical system comparison program \nfrom NIST with the option “mapsswe”. \nTABLE II \nSPEECH RECOGNITION LANGUAGE MODEL RESCORING EXPERIMENTS WITH \nTHE 460K SENTENCE COMPLETE CORPUS AND 230K SENTENCE HALF THE \nCORPUS  \nLanguage Models \nHalf The \nCorpus WER \n(%)  \nComplete Corpus \nWER (%) \nN-best Oracle 22.1 18.4 \nWord Trigram (Word-3gr) 38.7 32.2 \nMorpheme Trigram (Morph-3gr) 37.7 31.3 \nWord-3gr +  Morph-3gr 37.6 31.0 \nJMLLM-leaf 37.1 30.5 \nJMLLM-leaf + Morph-3gr 36.4 30.1 \nJMLLM-leaf + Word-3gr 36.6 29.8 \nJMLLM-tree 36.9 29.9 \nJMLLM-tree +  Morph-3gr 35.9 29.4 \nJMLLM-tree +  Word-3gr 36.1 29.2 \n \n \n \nT-ASL-01578-2007 \n \n9 \nB. Machine Translation Experiments  \nThe machine translation task considered here is about \ntranslating English sentences into Iraqi Arabic. The parallel \ncorpus has 430K utterance pairs with 90K words (50K \nmorphemes). The Iraqi Arabic language model training data \nincludes the Iraqi Arabic side of the parallel corpus as a \nsubset. A statistical trigram language model using modified \nKnesser-Ney smoothing [23] has been built for the \nmorphologically segmented data.  A development set (DevSet) \nof 2.2K sentences is used to tune the  feature weights. A \nseparate test set (TestSet) of 2.2K utterances are used to \nevaluate the language models for machine translation.  \nThe translation performance is measured by the BLEU \nscore [18] with one reference for each hypothesis. In order to \nevaluate the performance of the JMLLM, a translation N-best \nlist (N=10) is generated using the baseline Morph-3gr \nlanguage model.  First, on the DevSet all feature weights \nincluding the language model weight are optimized to \nmaximize the BLEU score using the downhill simplex method \n[16]. These weights are fixed when the language models are \nused on the TestSet.  In a previous study [21], we applied \nJMLLM-leaf on a different TestData.  In this study, in addition \nto the results for JMLLM-leaf, we also provide results for \nJMLLM-tree. The translation BLEU (%) scores for the DevSet \nare given in the first column of Table III. The first row (37.89 \nand 38.27) provides the oracle BLEU scores for the N-best list \ngenerated for both DevSet and TestSet. Given the tuned \nweights for Morph-3gr and other translation scores, the N-best \nlist is used to tune the weight for JMLLMs. On the DevSet, the \nbaseline Morph-3gr achieves 29.74, and word-trigram \nrescoring improves the BLEU score to 30.71. Interpolating the \nMorph-3gr and Word-3gr does not provide additional \nimprovement.  JMLLM-leaf achieves 30.63 by itself and \ninterpolating it with Morph-3gr and Word-3gr improves the \nBLEU score marginally.  On the other hand, JMLLM-tree \nachieves 30.80 and interpolation with Morph-3gr improves the \nresult to 31.10. Interpolation with Word-3gr improves the \nscore to 31.28, which is about 1.5 points better than that of the \nMorph-3gr and 0.6 points better than that of the Word-3gr.  \nThe results on DevSet are encouraging but results on the \nTestSet are the true assessment of the proposed language \nmodels. In the second column of Table III the results are \nprovided for the TestSet by fixing the tuned weights on the \nDevSet. JMLLM-leaf improves the results by 0.8 points and \n0.2 points compared to Morph-3gr and Word-3gr, \nrespectively. Interpolating JMLLM-leaf with Morph-3gr and \nWord-3gr improves the results by an additional 0.1 points and \n0.3 points, respectively.  JMLLM-tree improves the result from \n31.40 to 31.71 compared to JMLLM-leaf. Interpolating \nJMLLM-tree with Morph-3gr and Word-3gr improves the \nresults marginally. \nIn summary, JMLLM-tree improves the results by 1.1 \npoints which is significantly4 better compared to Morph-3gr \nand 0.5 points compared Word-3gr on the TestSet. \n \n4\n The improvement is significant at the 80% confidence interval. We use \nthe well-known bootstrapping technique to measure the confidence interval \nfor BLEU.  \nAdditionally, JMLLM-tree consistently outperforms JMLLM-\nleaf for both DevSet and TestSet. \nC. Discussions \nWhen examining the errors from our translation system, we \nsee that some of the poor recognition and translation may be \nexplained by the rather different behavior of the segmentation \nmethod on the training and test data. The OOV rate for the \nunsegmented speech recognition test data is 3.3%, the \ncorresponding number for the morphologically analyzed data \nis 2.7%. Hence, morphological segmentation reduces the OOV \nrate by only 0.6%.  It is worth comparing the vocabulary \nreduction on the training data (41%) to the vocabulary \nreduction on the test set (6%).  Even though the OOV rates for \nboth unsegmented and segmented test data are not that high, \nthe characteristics of the test data appear to be different than \nthe training data in its morphological make-up. We believe this \nis because the training data was collected over several years. In \nthe beginning there was more emphasis on the medical domain \nbut later the emphasis shifted towards the checkpoint, house \nsearch, vehicle search types of dialogs in the military domain. \nHowever, the test data was set aside from the very first part of \nthe collected data. In other words the test data was not \nuniformly/randomly sampled from the entire data. Despite this \napparent mismatch between training and test data the speech \nrecognition results are encouraging. \nFor machine translation experiments, the OOV rate for the \nunsegmented machine translation test data is 8.7%, the \ncorresponding number for the morphologically analyzed data \nis 7.4%. Hence, morphological segmentation reduces the OOV \nrate by 1.3% (15% relative), which again, is not as large a \nreduction as compared to the training data reduction (about \n40% relative reduction). We believe this would limit the \npotential improvement we could get from JMLLM, since \nJMLLM is expected to be more effective compared to word n-\ngram models, when the OOV rate is significantly reduced after \nsegmentation.  Improving the morphological segmentation to \ncover more words can potentially improve the performance of  \nJMLLMs. \nEven though it was not evaluated in this study, one of the \nbenefits of tight-integration using joint modeling becomes \napparent when a set of alternatives are generated for a sentence \nrather than just a single parse.  For example, we may have \nmore than one MLPT for a given sentence because of \nalternative morphological analysis, tagging or \nTABLE III \nSTATISTICAL MACHINE TRANSLATION NBEST LIST RESCORING WITH JMLLM  \nBLEU (%) BLEU (%) \nLanguage Models \nDevSet TestSet \nN-best List Oracle 37.89 38.27 \nMorpheme Trigram (Morph-3gr) 29.74 30.61 \nWord Trigram (Word-3gr) 30.71 31.21 \nMorph-3gr + Word-3gr 30.71 31.25 \nJMLLM-leaf 30.63 31.40 \nJMLLM–leaf +  Morph-3gr 30.69 31.49 \nJMLLM–leaf +  Word-3gr 30.71 31.67 \nJMLLM-tree 30.80 31.71 \nJMLLM-tree + Morph-3gr 31.10 31.73 \nJMLLM-tree + Word-3gr 31.28 31.76 \n \n \nT-ASL-01578-2007 \n \n10 \nsemantic/syntactic parses. Then, tight-integration with joint \nmodeling allows not only to get the best morpheme sequence \nbut also the best morphological analysis and/or tagging and/or \nsemantic/syntactic parses of a sentence.   \nVIII. CONCLUSIONS \nWe presented a new language modeling technique called \nJoint Morphological-Lexical Language Modeling (JMLLM) \nfor inflected languages in general and Arabic in particular. \nJMLLM allows joint modeling of lexical, morphological and \nadditional information sources about morphological segments \nand lexical items. JMLLM has both the predictive power of the \nword based language model and the coverage of the morpheme \nbased language model. It is also expected to have smoother \nprobability estimates than both morpheme and word based \nlanguage models. Two implementations of the JMLLM were \nproposed. One called JMLLM-leaf that loosely integrates the \nparse information while the other tightly integrates the parse \ninformation and is referred to as JMLLM-tree. Speech \nrecognition and machine translation experimental results \ndemonstrate that JMLLM provides encouraging improvements \nover the baseline word and morpheme based trigram language \nmodels. Moreover, tight-integration of all available \ninformation sources in the MLPT provides additional \nimprovements over the loose-integration.  \nREFERENCES \n[1] A. Ghaoui, F. Yvon, C. Mokbel, and G. Chollet, “On the use of \nmorphological constraints in N-gram statistical language model,” \nInterspeech’05, Lisbon, Portugal, 2005.  \n[2] B. Xiang, K. Nguyen, L. Nguyen, R. Schwartz, J. Makhoul, \n“Morphological decomposition for Arabic broadcast news \ntranscription”,  ICASSP’06, Toulouse, France, 2006.  \n[3] G. Choueiter, D. Povey, S.F. Chen, and G. Zweig, “Morpheme-based \nlanguage modeling for Arabic LVCSR”, ICASSP’06, Toulouse, France, \n2006.  \n[4] M. Afify, R. Sarikaya, H-K J. Kuo, L. Besacier, and Y. Gao, “On the \nUse of Morphological Analysis for Dialectal Arabic Speech \nRecognition”, Interspeech’06, Pittsburgh, PA 2006. \n[5] K. Kirchhoff, D. Vergyri, K. Duh, J. Bilmes and A. Stolcke, \n``Morphology-based language modeling for Arabic speech recognition'', \nComputer Speech and Language 20(4), 2006, pp. 589-608. \n[6] T. Buckwalter, “Buckwalter Arabic morphological analyzer version \n1.0”, LDC2002L49 and ISBN 1-58563-257-0, 2002.  \n[7] S. Khudanpur and J. Wu, \"Maximum Entropy Techniques for \nExploiting Syntactic, Semantic and Collocational Dependencies in \nLanguage Modeling,\"  Computer Speech and Language, 14(4):355-372, \n2000  \n[8] H. Erdogan, R. Sarikaya, S.F. Chen, Y. Gao and M. Picheny, “Using \nSemantic Analysis to Improve Speech Recognition Performance,” \nComp.  Speech & Language,, vol. 19(3), pp: 321-343, July 2005.  \n[9] S. F. Chen, R. Rosenfeld. “A survey of smoothing techniques for ME \nmodels”, IEEE Trans. Speech and Audio Process. 8 (1), 37–50, 2000. \n[10] Y. Gao, L. Gu, B. Zhou, R. Sarikaya, H.-K. Kuo. A.-V.I. Rosti, M. \nAfify, W. Zhu, \"IBM MASTOR: Multilingual Automatic Speech-to-\nSpeech Translator\", ICASSP’06, Toulouse, France, 2006.  \n[11] S. Vogel, H. Ney, and C. Tillmann, “HMM-based word alignment in \nstatistical translation”, COLING-96, pages: 836-841, Copenhagen, \nAugust, 1996. \n[12] A. Berger, S. Della Pietra and V. Della Pietra, \"A Maximum Entropy \nApproach to Natural Language Processing,\" Computational Lin-\nguistics, vol. 22, no. 1, March 1996 \n[13] Ronald Rosenfeld, Stanley F Chen, and Xiaojin Zhu. “Whole sentence \nexponential language models: a vehicle for linguistic-statistical \nintegration”, Computer Speech and Language, 15(1), 2001. \n[14] K. Kirchhoff and M. Yang. 2005.“Improved language modeling for \nstatistical machine translation”. ACL’05 workshop on Building and \nUsing Parallel Text, pages 125–128. \n[15] P. Koehn, F. J. Och, and D. Marcu, “Pharaoh: A beam search decoder \nfor phrase based statistical machine translation models”, Proc. of 6th \nConf. of  AMTA, 2004. \n[16] F. J. Och and H. Ney, “Discriminative training and maximum entropy \nmodels for statistical machine translation”, ACL, pages 295–302, \nUniversity of Pennsylvania 2002. \n[17] F. J. Och and H. Ney, “A Systematic Comparison of Various Statistical \nAlignment Models”, Comp. Linguistics, 29(1):9-51, 2003. \n[18] K.  Papineni, S. Roukos, T. Ward, and W-J. Zhu, “BLEU: a Method for \nAutomatic Evaluation of machine translation”, ACL 02, pages 311–318, \n2002. \n[19] P. Liang, B. Taskar, and D. Klein, “Alignment by agreement”, \nHLT/NAACL, pages 104–111, 2006. \n[20] R. Sarikaya, M .Afify and Y. Gao, “Joint Morphological-Lexical \nModeling (JMLLM) for Arabic,” ICASSP’07, Honolulu Hawaii, 2007. \n[21] R. Sarikaya and Y. Deng, “Joint Morphological-Lexical Modeling for \nMachine Translation,” HLT/NAACL’07, Rochester, NY, 2007. \n[22] R. Zens, E. Matusov, and H. Ney, “Improved word alignment using a \nsymmetric lexicon model”, COLING, pages 36–42, 2004. \n[23] S. Chen, J. Goodman, “An Empirical Study of Smoothing Techniques \nfor Language Modeling\", ACL-96, Santa Cruz, CA, 1996. \n[24] P. Geutner, “Using morphology towards better large-vocabulary speech \nrecognition systems”, ICASSP’95, Detroit, MI, 1995. \n[25] M. Kurimo, et.al., “Unlimited vocabulary speech recognition for \nagglutinative languages”,  HLT/NAACL, pp. 104–111, 2006. \n[26] O.W. Kwon, and J. Park, “Korean Large Vocabulary Continuous Speech \nRecognition with Morpheme-based Recognition Units”, Speech \nCommunication, Vol. 39, pp. 287-300, 2003. \n[27] K. Toutanova, C. D. Manning, “Enriching the Knowledge Sources Used \nin a Maximum Entropy Part-of-Speech Tagger”, Joint SIGDAT \nConference on EMNLP/VLC, Hong Kong, 2000. \n[28] J. Zavrel, W. Daelemans, “Memory-Based Learning”, ACL-97, Madrid, \nSpain, 1997. \n[29] R. Kneser and H. Ney. Improved backing-off for m-gram language \nmodeling. ICASSP, vol. 1. pp. 181–184, 1995. \n[30] E. Arisoy, H. Sak and M. Saraclar. Language Modeling for Automatic \nTurkish Broadcast News Transcription. Interspeech-2007. Antwerp \nBelgium, 2007. \n[31] W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. \n1992. Numerical Recipes in C: The Art of Scientific Computing. \nCambridge University Press, second edition. \n[32] M. Riley, E. Bocchieri, A. Ljolje and M. Saraclar, “The AT&T 1x real-\ntime Switchboard speech-to-text system”, NIST RT’02 Workshop, 2002. \n[33] M. Afify, L. Nguyen, B. Xiang, S. Abdou, J. Makhoul, \"Recent Progress \nin Arabic Broadcast News Transcription at BBN\", Interspeech’05, \nLisbon, Portugal, 2005. \n[34] H Erdogan, O Buyuk, K Oflazer, \"Incorporating language constraints in \nsub-word based speech recognition,\" IEEE ASRU Workshop, San Juan, \nPuerto Rico, Dec. 2005.  \n[35] Chelba, Ciprian, Jelinek, Frederick, “Structured language modeling,” \nComputer Speech and Language 14 (4), 283–332, 2000. \n",
    "Link": "https://core.ac.uk/download/pdf/11740487.pdf"
}