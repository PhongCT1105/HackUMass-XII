{
    "Title": "Comparative Profiling",
    "Authors": "Abdelmoniem, AM, Aldous, B, Proceedings of the 4th Workshop on Machine Learning and Systems",
    "Year": "No year available",
    "Abstract": "Generative AI models are at the forefront of advancing creative and analytical tasks, pushing the boundaries of what machines can generate and comprehend. Among these, latent diffusion models represent significant advancements in generating high-fidelity audio and images. This study introduces a systematic approach to study GPU utilisation during the training of these models by leveraging Weights & Biases and the PyTorch Profiler for detailed monitoring and profiling. Our methodology is designed to uncover inefficiencies in GPU resource allocation, pinpointing bottlenecks in the training pipeline. The insights gained aim to guide the development of strategies for enhancing training efficiency, potentially reducing computational costs and accelerating the development cycle of generative AI models. This contribution not only highlights the critical role of resource optimisation in scaling AI technologies but also opens new avenues for research in efficient model training",
    "Keywords": "No keywords available",
    "Publisher": "Association for Computing Machinery (ACM)",
    "Publication Date": "No publication date available",
    "Journal": "No journal available",
    "Citation Count": 0,
    "Full Text": "Comparative Profiling: Insights Into Latent DiffusionModel TrainingBradley AldousQueen Mary University of LondonLondon, United Kingdomb.j.aldous@qmul.ac.ukAhmed M. AbdelmoniemQueen Mary University of LondonLondon, United Kingdomahmed.sayed@qmul.ac.ukAbstractGenerative AI models are at the forefront of advancing cre-ative and analytical tasks, pushing the boundaries of whatmachines can generate and comprehend. Among these, la-tent diffusion models represent significant advancements ingenerating high-fidelity audio and images. This study intro-duces a systematic approach to study GPU utilisation duringthe training of these models by leveraging Weights & Biasesand the PyTorch Profiler for detailed monitoring and profil-ing. Our methodology is designed to uncover inefficienciesin GPU resource allocation, pinpointing bottlenecks in thetraining pipeline. The insights gained aim to guide the de-velopment of strategies for enhancing training efficiency,potentially reducing computational costs and acceleratingthe development cycle of generative AI models. This con-tribution not only highlights the critical role of resourceoptimisation in scaling AI technologies but also opens newavenues for research in efficient model training.CCSConcepts: ‚Ä¢Computingmethodologies‚ÜíDistributedartificial intelligence; ‚Ä¢ General and reference ‚Üí Mea-surement; Performance.Keywords: deep learning, diffusion model, profilingACM Reference Format:Bradley Aldous and Ahmed M. Abdelmoniem. 2024. ComparativeProfiling: Insights Into Latent Diffusion Model Training. In 4thWorkshop on Machine Learning and Systems (EuroMLSys ‚Äô24), April22, 2024, Athens, Greece. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3642970.3655847Permission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copiesare not made or distributed for profit or commercial advantage and thatcopies bear this notice and the full citation on the first page. Copyrightsfor components of this work owned by others than the author(s) mustbe honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specificpermission and/or a fee. Request permissions from permissions@acm.org.EuroMLSys ‚Äô24, Apr 22, 2024, Athens, Greece¬© 2024 Copyright held by the owner/author(s). Publication rights licensedto ACM.ACM ISBN 979-8-4007-0541-0/24/04https://doi.org/10.1145/3642970.36558471 IntroductionThe advent of generative artificial intelligence (AI) has ush-ered in a new era of innovation, enabling the creation ofcontent that blurs the line between human and machine-generated output. At the forefront of this revolution arelatent diffusion models (LDMs), a class of generative modelsthat have demonstrated remarkable capability in producinghigh-fidelity audio and images. Among these, AudioLDM[15] and Stable Diffusion [21] represent significant advance-ments, leveraging the power of deep learning to synthesisecontent that is not only diverse and detailed but also respon-sive to nuanced prompts and inputs.However, the computational demands of training suchsophisticated models are substantial, necessitating efficientutilisation of hardware resources, particularly Graphics Pro-cessing Units (GPUs). GPUs have become the linchpin ofdeep learning research and application, offering the parallelprocessing capabilities required to manage the immense com-putational load of these models. Despite their capabilities,optimising GPU usage to accommodate the intensive re-quirements of generative AI remains a challenge. Inefficientresource utilisation not only prolongs training times but alsoescalates costs and environmental impact, highlighting theneed for focused research using profiling and monitoringtools that can guide the optimisation of GPU usage. In thedevelopment of generative AI, identifying where and whybottlenecks occur in training processes is crucial for makingthem more efficient. Finding these bottlenecks can help inimproving the overall performance and efficiency of modeltraining.This paper aims to identify bottlenecks and inefficienciesin the training process, proposing optimisation strategies toenhance computational efficiency. The key contributions ofthis work are:‚Ä¢ It introduces a systematic approach to studying GPUutilisation during the training of LDMs, using popularprofiling and monitoring tools;‚Ä¢ It identifies specific areas of computational inefficiencyacross two advanced machine learning models, high-lighting CPU-bound overhead and memory manage-ment in AudioLDM, and emphasising the computa-tional load of convolution operations and matrix mul-tiplications in Stable Diffusion;EuroMLSys ‚Äô24, Apr 22, 2024, Athens, Greece Bradley Aldous and Ahmed M. AbdelmoniemInput Encoder z Decoder Outputùë•0 ùë•1 ùë•2Add Noise Add NoiseReverse Reverse......ùë•ùëáLatent SpaceL1/L2 Reconstruction LossFigure 1. High-level architecture diagrams of the two pri-mary components of LDMs. The top diagram shows a stan-dard autoencoder architecture where the input data is passedthrough an encoder and transformed into a latent represen-tation, the decoder reverses this process. The bottom dia-gram details the fundamental process of a diffusion modelwhere input data (ùë•0) is gradually transformed into a stan-dard Gaussian distribution (ùë•ùëá ), and the reverse process grad-ually generates data samples from the noise.‚Ä¢ It showcases the effectiveness of PyTorch‚Äôs DistributedData-Parallel Strategy [14] in significantly reducingtraining time for AudioLDM, demonstrating distributedtraining as a viable strategy to mitigate computationaldemands; and‚Ä¢ It provides insights into optimisation strategies for en-hancing computational efficiency, including potentialareas for future research in optimising specific inef-ficiencies and the exploration of distributed trainingstrategies.2 Background & Motivation2.1 Latent Diffusion ModelsGenerative models have emerged as a cornerstone of AI, en-abling machines to create content that is increasingly indis-tinguishable from that produced by humans [4, 18]. Amongthese, diffusion models [26] have gained prominence fortheir ability to generate high-quality and diverse outputs.However, a major concern with diffusion models is that theiriterative generation process in a high-dimensional data spacecan significantly prolong training and inference times, mak-ing them computationally intensive.To address these limitations, LDMs [21] were developed.These models represent a significant leap forward by oper-ating in a lower-dimensional latent space, thereby reducingcomputational demands while still enabling the creation ofhighly realistic and diverse outputs across various modalities.These models have found extensive applications, rangingfrom generating photo realistic images [21] and high-fidelityaudio [15, 16], to facilitating advancements in drug discoveryand material science [10]. In the domain of content creation,LDMs are instrumental in producing artwork and music thatcan mimic human creativity, thereby transforming industriesby automating creative processes.The architectural foundation of LDMs is a blend of diffusion-based generative modelling and deep learning techniques,characterised by their ability to gradually transform noiseinto structured data. The core principle of diffusion involvesstarting with a random distribution (noise) and iterativelyrefining this noise through a series of learned reverse dif-fusion steps until it resembles the target distribution. Dif-fusion is illustrated in Figure ??. In LDMs, this process isfacilitated by encoding the data into a lower-dimensionallatent space, which reduces the computational complexityand allows the model to capture the essential features ofthe data more efficiently. The autoencoder architecture typi-cally comprises two main components: an encoder that mapshigh-dimensional data to a latent space, and a decoder thatperforms the reverse process to generate data from the latentrepresentations. The architecture is shown in Figure ??. Inan LDM, the diffusion model generates data from randomnoise in the latent space and the data is then decoded into theoriginal data space. The elegance of this approach lies in itsability to produce highly detailed and coherent outputs, mak-ing LDMs particularly effective for tasks requiring nuancedunderstanding and generation of complex data patterns.Generative diffusion models present unique challengesthat diverge significantly from those encountered with tradi-tional deep learning models. The unique iterative refinementprocess of diffusion models requires significant computa-tional resources, making the efficient use of hardware a crit-ical challenge. While the incorporation of latent spaces aimsto reduce the dimensionality and computational overhead,the overall complexity of these models still necessitates ad-vanced profiling and optimisation techniques. Traditionalprofiling tools may not adequately capture the nuances ofLDMs‚Äô performance characteristics, underscoring the needfor specialised approaches to aid in optimisation.Comparative Profiling: Insights Into Latent Diffusion Model Training EuroMLSys ‚Äô24, Apr 22, 2024, Athens, Greece2.2 GPU Utilisation in Deep LearningThe integration of GPUs into the deep learning ecosystemhas been a transformative development, significantly acceler-ating the training and inference processes of neural networks[6]. GPUs, with their highly parallel structure, are exception-ally well-suited to handle the matrix and vector operationsthat are ubiquitous in deep learning algorithms. This capabil-ity has enabled the training of larger, more complex modelsthan was previously possible, pushing the boundaries of ar-tificial intelligence research and application. In the realm ofgenerative models, such as LDMs, efficient GPU utilisationbecomes not just advantageous but essential. Even thoughthey are computationally lighter than many other state-of-the-art methods, these models require substantial computa-tional resources to iteratively refine noise into high-fidelityoutputs, a process that involves millions of parameters andoperations. Optimising GPU usage can lead to significantimprovements in training speed, model performance, andenergy efficiency, which is crucial in a time when compu-tational costs and environmental impacts are of increasingconcern.Despite the critical role of GPUs in deep learning, achiev-ing optimal utilisation poses considerable challenges. Thesechallenges stem from a variety of factors, including memorymanagement, data transfer bottlenecks, and the efficient allo-cation of computational tasks across GPU cores. Addressingthese issues requires a good understanding of both the modelarchitecture and the underlying hardware. The pursuit ofefficient GPU utilisation thus stands at the crossroads oftechnological innovation and responsible computing, high-lighting the need for continuous improvement in the toolsand techniques that support deep learning workflows.2.3 Existing Profiling Tools & TechniquesThe landscape of deep learning has been significantly en-riched by the development and adoption of various profilingtools and techniques, designed to optimise computationalresource allocation and performance. These tools serve ascritical instruments for developers and researchers, enablingthem to dissect and understand the intricacies of model train-ing and execution. Profiling in the context of GPU utilisa-tion involves the detailed monitoring of how deep learningmodels interact with hardware resources, highlighting areaswhere improvements can be made to enhance efficiency andperformance. Tools like NVIDIA‚Äôs Nsight1 and the PyTorchProfiler [19] offer deep insights into the execution of neuralnetworks, providing metrics such as execution time, memoryusage, and GPU compute utilisation. Additionally, Weights& Biases [5] offers a platform for experiment tracking andmonitoring, allowing for the aggregation and visualisation ofperformance data across multiple training runs, facilitatingthe comparison and optimisation of models. The utilisation1https://developer.nvidia.com/nsight-systemsof these tools is paramount in identifying bottlenecks andinefficiencies within the training process.2.4 MotivationDespite the availability and potential of these profiling tools,their application in the optimisation of LDMs, particularlyin the context of sophisticated generative tasks like thoseperformed by AudioLDM and Stable Diffusion, remains anunderexplored area. The integration of detailed profiling andmonitoring into the training workflow of such models is cru-cial for advancing our understanding of their performancecharacteristics and for unlocking new efficiencies in theiruse of GPU resources. While existing profiling studies havemade significant strides in understanding and improving theperformance of deep learning models, they predominantlyfocus on standard benchmarks, such as ResNet and otherimage-based architectures [11, 28]. This approach neglectsthe unique challenges and computational demands posedby LDMs, and more generally audio models, which differmarkedly in architecture and data processing requirements.Moreover, the scarcity of detailed profiling and optimisa-tion studies on LDMs for audio highlights a critical oversightin the development of efficient and accessible generativemodels. By dedicating research efforts to the specific profil-ing of audio LDMs, this study represents a first effort towardsilluminating the computational bottlenecks and inefficienciesinherent in these models and their training configurations.Uncovering these insights is crucial for driving future op-timisations, enabling LDMs to be more efficiently trainedon a wider range of hardware, and democratising access tocutting-edge generative technologies.3 Design Methodology3.1 ModelsThis research focuses on two advanced generative LDMswhich exemplify the power of latent diffusion technology intheir respective domains of audio and visual content genera-tion. Thesemodels were selected for their distinct capabilitiesto generate high-fidelity outputs and their substantial rele-vance to current AI research and applications. This sectionprovides an overview of the technical underpinnings andunique characteristics of each model.3.1.1 StableDiffusion. Thismodel has emerged as a promi-nent model for image generation, known for its ability toproduce detailed and diverse visuals. The specific model usedhere is the unconditional variant [21] trained on 2562 imagesfrom the CelebA-HQ dataset2, which leverages a latent spacerepresentation of images to perform the diffusion process.2https://www.kaggle.com/datasets/badasstechie/celebahq-resized-256x256EuroMLSys ‚Äô24, Apr 22, 2024, Athens, Greece Bradley Aldous and Ahmed M. AbdelmoniemProfilerProfiling Output to Console/File/LogProfiler: Post-ProcessingProfiler: CollectionEnd TrainingLog Weights & BiasesMetrics Every EpochBegin TrainingInitialise Weights& BiasesLoadDataModel Text+ (+)Figure 2. A high-level system diagram showing the experi-mental setup and the relation between the tools and models.Text data was only used with AudioLDM.The versatility and quality of outputs have garnered signifi-cant attention, establishing it as a key tool for creative andcommercial applications alike.3.1.2 AudioLDM. Thismodel stands as a pioneeringmodelin the realm of generative audio, utilising latent diffusionprinciples to synthesise realistic and coherent audio clips.Its architecture is designed to efficiently handle the complextemporal dynamics of audio data, capturing subtle nuancesand variations across a wide range of sounds and musicalgenres. The model operates by encoding raw audio into alower-dimensional latent space, where the diffusion processis applied to generate new audio samples. This approach al-lows for the efficient generation of high-quality audio whilemanaging the computational load, making it an ideal can-didate for exploring optimisations in GPU resource utilisa-tion. This model was trained on the AudioCaps dataset [13].For both of these models, it is the training of the diffusionmodel that is profiled, as this is the generative part of botharchitectures. For AudioLDM, this amounts to 185 millionparameters and, for the Stable Diffusion variant, this is 104million parameters.3.2 Profiling & Monitoring ApproachTo address the computational challenges posed by the train-ing of advanced generative models such as AudioLDM andStable Diffusion, this study employs a methodology centredaround the integration of Weights & Biases [5] and the Py-Torch Profiler [19], tools selected for their robust capabilitiesin tracking and analysing model performance and resourceutilisation.The PyTorch Profiler [19] is configured to monitor theexecution times of the different operations present in thetraining over one epoch, ordered by ‚ÄôCUDA total‚Äô (the to-tal time spent on GPU-related operations when profilingCUDA applications). This enables the identification of spe-cific operations or layers within the models that contributedisproportionately to computational load or inefficiency.3.2.1 Metrics. Concurrently,Weights &Biases [5] is utilisedto track and visualise the metrics discussed below through-out training runs, facilitating a thorough understanding oftraining performance.‚Ä¢ GPU Utilisationmeasures the percentage of time theGPU is actively processing data over a certain period.High utilisation indicates the GPU is being heavilyused.‚Ä¢ GPUMemoryAllocated refers to the amount of GPUmemory (VRAM) assigned to a process. It‚Äôs crucial formanaging large datasets or complex computations thatrequire significant memory.‚Ä¢ GPU Power Usage shows the amount of power theGPU is consuming in real-time. It‚Äôs important for man-aging the energy efficiency of the system and ensuringthe power supply can meet the GPU‚Äôs demands.3.3 Experimental SetupThe hardware setup features 8 NVIDIA A5000 GPUs (24GB)and Intel Xeon Gold 5318Y CPUs, selected for their balanceof memory capacity, computational power, and energy ef-ficiency crucial for training deep learning models. Initialruns were tested on separate servers comprised of NVIDIARTX 2080Ti GPUs, which struggled with the large memoryrequirements of these models.On the software side, the study utilised the environmentsprovided by both implementations. The models and trainingwere constructed and run using PyTorch, which allowedfor seamless coupling with the PyTorch Profiler [19]. Note,for AudioLDM, PyTorch Lightning [8] was used to train themodel, and so their built-in profiler was used, whereas theprofiler was wrapped around the training loop for StableComparative Profiling: Insights Into Latent Diffusion Model Training EuroMLSys ‚Äô24, Apr 22, 2024, Athens, GreeceDiffusion. Weights & Biases [5] is initialised before trainingto ensure full coverage of the process.4 Evaluation4.1 Profiling Results4.1.1 AudioLDM Results. The profiling results reveal adistinct focus on optimisation and convolution operations.The ‚ÄòOptimizer.step‚Äò operation, consuming 37.92% of theCPU total (see Table 1), signifies a heavy computational loadon the CPU for model updates, while not directly involv-ing CUDA operations. This indicates that the optimisationprocess, crucial for adjusting model parameters, is primarilyCPU-bound. The significant CPU total percentage seen in‚ÄòProfilerStep*‚Äò suggests considerable overhead in moni-toring model performance. Conversely, CUDA utilisation isdominated by ‚Äòaten::cudnn_conv‚Äò and ‚Äòaten::mul‚Äò opera-tions, which are integral to the model‚Äôs forward and back-ward passes, highlighting the intensive use of GPU resourcesfor convolutional operations and element-wise multiplica-tions that underpin the model‚Äôs ability to process and gener-ate audio data efficiently.4.1.2 Stable Diffusion Results. The profiling showcasesan emphasis on data loading and memory operations along-side convolutional computations. The ‚ÄòDataLoader‚Äò opera-tion, with a CPU total of 61.85%, underscores the substan-tial CPU resources dedicated to preparing and feeding datainto the model, a crucial step for training efficiency butnot directly involving GPU computations. Memory opera-tions such as ‚Äòaten::copy_‚Äò and ‚Äòaten::to‚Äò also commanda significant portion of CPU resources, indicating the im-portance of data manipulation and transfer in the model‚Äôstraining process. The CUDA landscape is markedly definedby ‚Äòaten::cudnn_conv‚Äò and ‚Äòaten::bmm‚Äò, which account forthe majority of CUDA utilisation, emphasising the model‚Äôsreliance on convolutional and batch matrix multiplicationoperations for generating high-fidelity images. These CUDA-intensive operations are pivotal for the model‚Äôs performance,facilitating complex feature extraction and transformationnecessary for Stable Diffusion‚Äôs generative capabilities.Comparing the AudioLDM and Stable Diffusion models,the profiling results illustrate distinct computational andresource utilisation patterns. AudioLDM emphasises CPU-intensive optimisation and GPU-intensive convolution oper-ations, indicating a balance between parameter updates anddata processing in neural networks. Stable Diffusion, how-ever, allocates a significant portion of CPU resources to dataloading and memory operations, reflecting the importanceof efficient data handling and transfer in its training process.CUDA usage in both models is dominated by convolutionaloperations, but Stable Diffusion additionally highlights therole of batch matrix multiplications, suggesting a diversecomputational strategy for handling generative tasks.Table 1. Profiling results of various operationsAudioLDMOperation CPU Total Self CUDA # CallsOptimizer.step 37.92% 0.00% 3ProfilerStep* 50.40% 0.00% 3aten::cudnn_conv 1.20% 20.37% 388aten::mul 1.43% 15.72% 4896aten::bmm 1.10% 12.12% 846Stable DiffusionOperation CPU Total Self CUDA # CallsDataLoader3 61.85% 0.00% 1876aten::copy_ 21.47% 2.16% 530911aten::to 18.01% 0.00% 244036aten::cudnn_conv 0.81% 25.05% 155625aten::bmm 0.32% 23.30% 170625The comprehensive analysis of our profiling data reveals apronounced disparity in processing times between the CPUand GPU, with the CPU times being approximately twiceas long as those on the GPU for both models. This signifi-cant discrepancy indicates a CPU bottleneck in the trainingpipeline, suggesting that tasks managed by the CPU‚Äîsuchas data preprocessing, loading, and supplying data to theGPU‚Äîare notably more time-consuming. This imbalancelikely causes the GPU to idle while awaiting data, under utilis-ing its computational capacity and impeding overall trainingefficiency. By streamlining CPU operations, there‚Äôs potentialto significantly boost data throughput to the GPU, therebyoptimising the training process and enhancing model per-formance.The observed disparity in the number of operation callsbetween the two models can be attributed to the distincttraining methodologies employed. Specifically, Stable Dif-fusion underwent training via a custom loop with directprofiler integration, while the other was trained utilising thePyTorch Lightning framework, which leverages an in-builtPyTorch profiler. This difference in approach significantlyinfluences the profiling data collected. PyTorch Lightningabstracts and optimises several training pipeline aspects, po-tentially aggregating operations and streamlining executionpaths, which can affect the number and type of calls recordedby the profiler. In contrast, the custom training loop mightcapture a more granular, less optimised sequence of oper-ations, leading to a higher apparent number of calls. Thisdistinction highlights the impact of training and profilingmethodologies on the interpretation of profiling data, so themain conclusions drawn here are based on a relative compar-ison of execution time for operations rather than the numberof calls. For interested readers; the full profiling output canbe found on GitHub4.4https://github.com/Brudalaxe/Comparative-Profiling-of-LDMsEuroMLSys ‚Äô24, Apr 22, 2024, Athens, Greece Bradley Aldous and Ahmed M. AbdelmoniemGPU Memory Allocated (%) GPU Memory Allocated (%)GPU Power Usage (%) GPU Power Usage (%)Figure 3. Plots showing the GPU memory allocated and theGPU power usage as percentages for the training of bothAudioLDM (the red plots to the left) and Stable Diffusion(the green plots to the right)4.2 Performance AnalysisFigure 3 shows the result of monitoring the training processof these two models. During the profiling of memory allo-cation for both AudioLDM and Stable Diffusion models, itwas observed that AudioLDM consistently required a higheramount of memory across all training epochs. This increasedmemory demand is attributed to its complex architecture,designed to handle audio-visual data, which inherently re-quires processing a larger volume of information. Notably,memory usage was consistent across the training of bothmodels.In terms of power usage, AudioLDMdemonstrated a higherand more uniform consumption compared to Stable Diffu-sion. This pattern indicates a steady and intensive utilisa-tion of GPU resources throughout training, reflecting themodel‚Äôs consistent computational demands. The uniformpower usage by AudioLDM contrasts with the more variableconsumption observed for Stable Diffusion, suggesting dif-ferences in their operational efficiencies and the nature oftasks being performed.Beyond memory allocation and power usage, the profilinghighlighted the impact of external factors and the choice ofprofiling tools on the accuracy and efficiency of performancemeasurement. The comparison between the PyTorch Profiler[19] and PyTorch Lightning‚Äôs built-in profiler revealed differ-ences in data collection and processing efficiency, suggestingthat the choice of tool can significantly affect the overheadand precision of performance profiling.4.3 Distributed PerformanceAn additional training run was performed on AudioLDM inwhich PyTorch‚Äôs Distributed Data Parallel Strategy [14] wasused to divide the training across two GPUs. The results arecompared to the single GPU training run and displayed inTraining Loss GPU Utilisation (%)Figure 4. Plots showing the convergence andGPU utilisationof two separate training runs of AudioLDM. the red plotsare for a single GPU training run, and the blue plots are fora data parallel training run on two GPUsFigure 4. The left plot shows the evolution of the loss duringtraining (here both examples were run for 10 epochs withidentical hyperparameters), and here you can see the timeefficiency of distributing the data over two GPUs. Duringthese two runs, the GPU utilisation was significantly higherin the distributed setting, showing a 20% increase. This, inturn, led to a significant speedup in terms of training time,with the single GPU run taking 13 hours and 47 minutes tocomplete 10 epochs, compared to 5 hours and 29 minutesfor the dual GPU run. In this experiment, distributing thetraining across two GPUs reduces the training time by afactor of 2.51, beyond linear scaling.4.4 DiscussionThe profiling results suggest that while both AudioLDMand Stable Diffusion models face challenges in optimisingperformance, the areas requiring improvement vary signifi-cantly. Stable Diffusion demonstrates significant CUDA timeconsumption in convolution operations and matrix multipli-cations, highlighting the computational intensity of theseprocesses. The study reveals that computational demandsand resource utilisation patterns differ significantly betweenthe two models. AudioLDM exhibits a higher memory re-quirement and more uniform power consumption through-out its training process, attributed to its complex architecturetailored to processing audio-visual data.Additionally, PyTorch‚Äôs Distributed Data Parallel Strategy[14], when applied to AudioLDM, resulted in a notable reduc-tion in training time. This suggests that distributed trainingis a viable strategy for mitigating computational demandsand accelerating the training process.5 Related WorkRecent studies have delved into performance analysis be-tween CPUs and GPUs for deep learning tasks, with findingsindicating GPUs‚Äô superior processing speeds and efficiency,particularly for complex models [6, 11]. Research on profil-ing and monitoring tools has shown that while monitoringComparative Profiling: Insights Into Latent Diffusion Model Training EuroMLSys ‚Äô24, Apr 22, 2024, Athens, Greecetools impose minimal overhead, enabling real-time decision-making, profiling tools, though resource-intensive, offer valu-able insights for optimising performance [28]. Furthermore,the development of SASSI, a precise, customisable profilingtool for GPUs, represents a significant advancement in ap-plication characterisation and architectural exploration [27].Efforts to optimise GPU memory usage in large dataset ap-plications have also shown substantial reductions in datatransfer volumes, enhancing efficiency [22].In the realm of distributed machine learning, innovationslike DC2 [3] and SIDCo [17] demonstrate significant advance-ments in minimising communication delays and speedingup training processes through delay-aware compression con-trol and statistical-based gradient compression techniques,respectively. Evaluations of distributed deep learning frame-works such as Horovod [24], DeepSpeed [20], and PyTorchDistributed Data Parallel [14] have highlighted their scala-bility and efficiency, showcasing their ability to effectivelyleverage cloud resources and improve training scalabilityacross various GPU configurations [1, 9]. Additionally, acomparative study of frameworks like Caffe-MPI [12], CNTK[23], MXNet [7], and TensorFlow [2] has identified key per-formance bottlenecks and optimisation opportunities fortraining convolutional neural networks with synchronousSGD [25].6 Conclusion & Future WorkThis study provides an in-depth analysis of the computa-tional efficiency and performance characteristics of Audi-oLDM and Stable Diffusion, two advanced machine learningmodels. The findings underscore the importance of tailoredoptimisation strategies for different model architectures.Future work should aim to further optimise the identi-fied areas of computational inefficiency. For AudioLDM, thiscould include efforts to reduce CPU-bound overhead and en-hance memory management efficiency. For Stable Diffusion,the focus could be on optimising convolution operations,possibly through the use of more efficient algorithms orhardware acceleration techniques. Additionally, exploringthe effects of different profiling tools and methodologiescould provide valuable insights into the accuracy and over-head of performance measurements, including the impact ofexternal system activities on training processes.The study also highlights the potential of distributed train-ing to significantly reduce training times, suggesting thatfurther research into these strategies is warranted. Certainly,here‚Äôs a paragraph you could use for the future work section:In future work, a comprehensive exploration into the scala-bility of distributed training across a broader spectrum andlarger quantity of GPU models is essential to validate theobserved superlinear speedup phenomena in greater depth.By extending the range of GPU architectures tested, fromconsumer-grade cards to high-end data center GPUs, thisinvestigation would offer more nuanced insights into howdifferent hardware configurations impact the efficiency ofgenerative diffusion model training.Lastly, future investigations into the trade-offs betweencomputational efficiency, model performance, and resourceutilisation are crucial for designing models that not onlyachieve high accuracy but also operate efficiently in resource-constrained environments.AcknowledgmentsThis work was supported by the UKRI and EPSRC undergrant EP/S022694/1.References[1] Marcel Aach, Eray Inanc, Rakesh Sarma, Morris Riedel, and AndreasLintermann. 2023. Large scale performance analysis of distributeddeep learning frameworks for convolutional neural networks. Journalof Big Data 10 (6 2023), 96. Issue 1. https://doi.org/10.1186/s40537-023-00765-w[2] Mart√≠n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, ZhifengChen, Craig Citro, Greg S. Corrado, AndyDavis, Jeffrey Dean,MatthieuDevin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, GeoffreyIrving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser,Manjunath Kudlur, Josh Levenberg, Dandelion Man√©, Rajat Monga,Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, JonathonShlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-cent Vanhoucke, Vijay Vasudevan, Fernanda Vi√©gas, Oriol Vinyals,Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi-aoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learningon Heterogeneous Systems. https://www.tensorflow.org/ Softwareavailable from tensorflow.org.[3] Ahmed M. Abdelmoniem and Marco Canini. 2021. DC2: Delay-awareCompression Control for Distributed Machine Learning. In IEEE IN-FOCOM 2021 - IEEE Conference on Computer Communications. 1‚Äì10.https://doi.org/10.1109/INFOCOM42981.2021.9488810[4] Lucas Bellaiche, Rohin Shahi, Martin Harry Turpin, Anya Ragnhild-stveit, Shawn Sprockett, Nathaniel Barr, Alexander Christensen, andPaul Seli. 2023. Humans versus AI: whether and why we prefer human-created compared to AI-created artwork. Cognitive Research: Principlesand Implications 8 (7 2023), 42. Issue 1. https://doi.org/10.1186/s41235-023-00499-6[5] Lukas Biewald. 2020. Experiment Tracking with Weights and Biases.https://www.wandb.com/ Software available from wandb.com.[6] Ebubekir BUBER and Banu DIRI. 2018. Performance Analysis andCPU vs GPU Comparison for Deep Learning. In 2018 6th InternationalConference on Control Engineering Information Technology (CEIT). 1‚Äì6.https://doi.org/10.1109/CEIT.2018.8751930[7] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang,Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015.MXNet: A Flexible and Efficient Machine Learning Library for Het-erogeneous Distributed Systems. (12 2015). http://arxiv.org/abs/1512.01274[8] William Falcon and The PyTorch Lightning team. 2019. PyTorch Light-ning. https://doi.org/10.5281/zenodo.3828935[9] Attila Farkas, Kriszti√°n P√≥ra, S√°ndor Sz√©n√°si, G√°bor Kert√©sz, andR√≥bert Lovas. 2022. Evaluation of a distributed deep learning frame-work as a reference architecture for a cloud environment. In 2022IEEE 10th Jubilee International Conference on Computational Cyber-netics and Cyber-Medical Systems (ICCC). 000083‚Äì000088. https://doi.org/10.1109/ICCC202255925.2022.9922765EuroMLSys ‚Äô24, Apr 22, 2024, Athens, Greece Bradley Aldous and Ahmed M. Abdelmoniem[10] Cong Fu, Keqiang Yan, Limei Wang, Wing Yee Au, Michael McThrow,Tao Komikado, Koji Maruhashi, Kanji Uchino, Xiaoning Qian, andShuiwang Ji. 2023. A Latent Diffusion Model for Protein StructureGeneration. arXiv:2305.04120 [q-bio.BM][11] Dipesh Gyawali. 2023. Comparative Analysis of CPU and GPU Pro-filing for Deep Learning Models. (9 2023). http://arxiv.org/abs/2309.02521[12] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, JonathanLong, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014.Caffe: Convolutional Architecture for Fast Feature Embedding. In Pro-ceedings of the 22nd ACM International Conference on Multimedia (Or-lando, Florida, USA) (MM ‚Äô14). Association for Computing Machinery,New York, NY, USA, 675‚Äì678. https://doi.org/10.1145/2647868.2654889[13] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and GunheeKim. 2019. AudioCaps: Generating Captions for Audios in TheWild. InProceedings of the 2019 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human Language Technolo-gies, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, andThamar Solorio (Eds.). Association for Computational Linguistics, Min-neapolis, Minnesota, 119‚Äì132. https://doi.org/10.18653/v1/N19-1011[14] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis,Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania,and Soumith Chintala. 2020. PyTorch Distributed: Experiences onAccelerating Data Parallel Training. (6 2020). http://arxiv.org/abs/2006.15704[15] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, DaniloMandic, Wenwu Wang, and Mark D. Plumbley. 2023. AudioLDM:Text-to-Audio Generation with Latent Diffusion Models. (1 2023).https://arxiv.org/abs/2301.12503[16] Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong,Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D. Plumbley.2023. AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining. (8 2023). http://arxiv.org/abs/2308.05734[17] Ahmed M. Abdelmoniem, Ahmed Elzanaty, Mohamed-Slim Alouini,and Marco Canini. 2021. An Efficient Statistical-based Gradient Com-pression Technique for Distributed Training Systems. In Proceedingsof Machine Learning and Systems, A. Smola, A. Dimakis, and I. Sto-ica (Eds.), Vol. 3. 297‚Äì322. https://proceedings.mlsys.org/paper_files/paper/2021/file/fea47a8aa372e42f3c84327aec9506cf-Paper.pdf[18] Sophie J Nightingale and Hany Farid. 2022. AI-synthesized faces areindistinguishable from real faces and more trustworthy. Proceedingsof the National Academy of Sciences of the United States of America 119(2 2022). Issue 8. https://doi.org/10.1073/pnas.2120481119[19] PyTorch. 2021. Introducing PyTorch Profiler - the new and improvedperformance tool. https://pytorch.org/docs/stable/profiler.html Soft-ware available from https://pytorch.org.[20] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and YuxiongHe. 2020. DeepSpeed: System Optimizations Enable Training DeepLearning Models with Over 100 Billion Parameters. In Proceedings ofthe 26th ACM SIGKDD International Conference on Knowledge Dis-covery & Data Mining (Virtual Event, CA, USA) (KDD ‚Äô20). Asso-ciation for Computing Machinery, New York, NY, USA, 3505‚Äì3506.https://doi.org/10.1145/3394486.3406703[21] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser,and Bj√∂rn Ommer. 2021. High-Resolution Image Synthesis with LatentDiffusion Models. (12 2021). http://arxiv.org/abs/2112.10752[22] Nadathur Satish, Narayanan Sundaram, and Kurt Keutzer. 2009. Op-timizing the use of GPU memory in applications with large datasets. 16th International Conference on High Performance Computing,HiPC 2009 - Proceedings, 408‚Äì418. https://doi.org/10.1109/HIPC.2009.5433185[23] Frank Seide and Amit Agarwal. 2016. CNTK: Microsoft‚Äôs Open-SourceDeep-Learning Toolkit. In Proceedings of the 22nd ACM SIGKDD In-ternational Conference on Knowledge Discovery and Data Mining (SanFrancisco, California, USA) (KDD ‚Äô16). Association for Computing Ma-chinery, New York, NY, USA, 2135. https://doi.org/10.1145/2939672.2945397[24] Alexander Sergeev and Mike Del Balso. 2018. Horovod: fast and easydistributed deep learning in TensorFlow. (2 2018). https://arxiv.org/abs/1802.05799[25] Shaohuai Shi, Qiang Wang, and Xiaowen Chu. 2017. PerformanceModeling and Evaluation of Distributed Deep Learning Frameworkson GPUs. (11 2017). http://arxiv.org/abs/1711.05979[26] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and SuryaGanguli. 2015. Deep Unsupervised Learning using NonequilibriumThermodynamics. Proceedings of the 32nd International Conference onMachine Learning, 2256‚Äì2265.[27] Mark Stephenson, Siva Kumar Sastry Hari, Yunsup Lee, EimanEbrahimi, Daniel R. Johnson, David Nellans, Mike O‚ÄôConnor, andStephen W. Keckler. 2015. Flexible software profiling of GPU archi-tectures. SIGARCH Comput. Archit. News 43, 3S (jun 2015), 185‚Äì197.https://doi.org/10.1145/2872887.2750375[28] Ehsan Yousefzadeh-Asl-Miandoab, Ties Robroek, and Pinar Tozun.2023. Profiling and Monitoring Deep Learning Training Tasks. InProceedings of the 3rd Workshop on Machine Learning and Systems(Rome, Italy) (EuroMLSys ‚Äô23). Association for Computing Machinery,New York, NY, USA, 18‚Äì25. https://doi.org/10.1145/3578356.3592589",
    "Link": "https://core.ac.uk/download/613176518.pdf"
}