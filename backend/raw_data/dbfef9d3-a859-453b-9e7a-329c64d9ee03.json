{
    "Title": "Paraphrasing with Large Language Models",
    "Authors": "Andrews, Martin, Witteveen, Sam",
    "Year": "No year available",
    "Abstract": "Recently, large language models such as GPT-2 have shown themselves to be\nextremely adept at text generation and have also been able to achieve\nhigh-quality results in many downstream NLP tasks such as text classification,\nsentiment analysis and question answering with the aid of fine-tuning. We\npresent a useful technique for using a large language model to perform the task\nof paraphrasing on a variety of texts and subjects. Our approach is\ndemonstrated to be capable of generating paraphrases not only at a sentence\nlevel but also for longer spans of text such as paragraphs without needing to\nbreak the text into smaller chunks.Comment: Accepted paper for WNGT workshop at EMNLP-IJCNLP 2019. (7 pages\n  including references and supplemental material",
    "Keywords": "No keywords available",
    "Publisher": "'Association for Computational Linguistics (ACL)'",
    "Publication Date": "No publication date available",
    "Journal": "No journal available",
    "Citation Count": 0,
    "Full Text": "ar\nX\niv\n:1\n91\n1.\n09\n66\n1v\n1 \n [c\ns.C\nL]\n  2\n1 N\nov\n 20\n19\nParaphrasing with Large Language Models\nSam Witteveen\nRed Dragon AI\nsam@reddragon.ai\nMartin Andrews\nRed Dragon AI\nmartin@reddragon.ai\nAbstract\nRecently, large language models such as GPT-\n2 have shown themselves to be extremely\nadept at text generation and have also been\nable to achieve high-quality results in many\ndownstream NLP tasks such as text classifica-\ntion, sentiment analysis and question answer-\ning with the aid of fine-tuning. We present\na useful technique for using a large language\nmodel to perform the task of paraphrasing on\na variety of texts and subjects. Our approach\nis demonstrated to be capable of generating\nparaphrases not only at a sentence level but\nalso for longer spans of text such as paragraphs\nwithout needing to break the text into smaller\nchunks.\n1 Introduction\nParaphrase generation is an NLP task that has mul-\ntiple uses in content creation, question answering,\ntranslation, and data augmentation. It is a task\nthat has been attempted for many decades using\nstatistical and rules-based approaches (McKeown,\n1979; Meteer and Shaked, 1988).\nWe propose a system that generates paraphrased\nexamples in an autoregressive fashion using a neu-\nral network, without the need for techniques such\nas top-k word selection or beam search.\nWe demonstrate that by using large language\nmodels we are able to produce not only para-\nphrases that are longer and of a higher quality than\nprevious work, but can also paraphrase text be-\nyond the individual sentence-level (i.e. full para-\ngraphs at a time).\nThe large language models we use implement\nthe encoder-decoder structure of the transformer\narchitecture (Vaswani et al., 2017) which has been\nshown to learn different representations of lan-\nguage at each level of its encoding (Devlin et al.,\n2019). The power of language models like GPT-2\n(Radford et al., 2019) and BERT allows them to\ndevelop useful representations of language which\ncan be used far beyond just generation of the next\nword (Rothe et al., 2019). In our experiments,\nwe have observed that the models have representa-\ntions of syntax and grammar, allowing them to be\nfine-tuned for the task of paraphrase generation.\n2 Related Work\nParaphrase generation has attracted a number of\ndifferent NLP approaches. These have included\nrule-based approaches (McKeown, 1979; Meteer\nand Shaked, 1988) and data-driven methods (Mad-\nnani and Dorr, 2010), with recently the most com-\nmon approach being that the task is treated as a\nlanguage translation task (Bannard and Callison-\nBurch, 2005; Barzilay and McKeown, 2001; Pang\net al., 2003) - often performed using a bilin-\ngual corpus pivoting back and forth (Madnani and\nDorr, 2010; Prakash et al., 2016; Mallinson et al.,\n2017). Other methods proposed include more\nrecently the use of Deep Reinforcement Learn-\ning (Li et al., 2018) , supervised learning using\nsequence-to-sequence models (Gupta et al., 2018;\nPrakash et al., 2016) and unsupervised approaches\n(Bowman et al., 2016; Roy and Grangier, 2019).\n3 Methodology\nIn this section, we define the task, present our im-\nplementation, describe our evaluation protocol and\nthe paraphrase datasets used.\nParaphrase generation can be described as gen-\nerating a phrase which is semantically as close as\npossible to the original while being rewritten in\nnew words and phrases.\n3.1 Large Language Model training\nTraining language models has become a popular\nway of creating models suited for Transfer Learn-\ning in the field of NLP. (Howard and Ruder, 2018;\nUSE ROUGE-L BLEU\nIn: A prisoner can asphyxiate himself in 90 seconds and, after\neight minutes or so, he will be brain dead.\nOut: In 90 seconds, a prisoner can asphyxiate himself and be brain\ndead after eight minutes or so.\n0.9326 0.4706 0.4730\nIn: The restaurant is a carved-off space up a couple of stairs to one\nside, dominated by faux bare-brick columns, faux-wood floors\nand an air of foetid despondency.\nOut: It is a carved-off space, up a couple of flights of stairs, to\nthe other side of the restaurant, dominated by fake bare-brick\ncolumns, fake wood floors and an air of foetid despondency.\n0.8954 0.5000 0.5348\nIn: I signed a bill that made the problem worse, and I want to admit\nit, he said.\nOut: He signed a bill that made the problem worse and he wants to\nadmit it.\n0.8363 0.4667 0.5299\nIn: It said the damage to the wing provided a pathway for hot\ngasses to penetrate the ship’s thermal armor during Columbia’s\nill-fated reentry.\nOut: The document says the damage to the wing provided a pathway\nfor hot gases to penetrate Columbia’s thermal armour during its\nfatal re-entry.\n0.9190 0.4545 0.5445\nTable 1: Examples of GPT-2 generated paraphrased sentences with scores for each pair\nPeters et al., 2018; Radford et al., 2018; Dong\net al., 2019). While these models are initially\ntrained in a semi-supervised manner to predict the\nnext word or words in a sequence, they can be fine-\ntuned and used for a variety of downstream NLP\ntasks such as text classification, sentiment analy-\nsis, tagging, and entity extraction.\nMore recently, large language models using\ntransformer architectures are achieving state of the\nart results for many of these tasks while using less\nsupervised data than previously needed.\nOne example of these large language models\nthat has proven to be very good at text generation\nis GPT-2. It makes use of a transformer architec-\nture and comes in various sizes up to 1.5 billion\nparameters. In these experiments, we have taken a\npre-trained version of the GPT-2 model trained in\na semi-supervised fashion on the WebText dataset\n(Radford et al., 2019) of over 8 million documents\nwith 40 GB of text in total.\n3.2 Fine-tuning for Task\nWe take the GPT-2 model and fine-tune it on a\nsupervised dataset of pre-made paraphrase exam-\nples. These examples are fed into the model as\noriginal phrase / paraphrase pairs, separated by a\nspecific identifying sequence (such as ”>>>>”).\nThis training is done for a small number of\nepochs to give the model just enough examples of\nwhat the task is asking from the model : The goal\nbeing to avoid overfitting the model on the new\ndata, while giving it sufficient exposure to the task\nto enable it to learn the general pattern expected.\nWhile we experimented with TPUs for the fine-\ntuning, in the end we were able to reproduce the\nsame results on a single K-80 GPU with around\n90 minutes of training.\nOnce the model is fine-tuned, we find that it can\nalso produce similar paraphrase training examples\nif sampled from with no conditional input. To give\nan indication of training progress, these ’naive’\nparaphrases are sampled on a periodic basis dur-\ning the training.\nAfter fine-tuning on this dataset, we are then\nable to feed in any original phrase followed by the\nunique token and have the model generate para-\nphrases on demand.\n3.3 Candidate Generation and Selection\nAfter the model is trained, we then sample from\nthe model using previously unseen sentences as\nconditional input. This conditional input allows\nus to generate multiple candidate sentences for the\nsingle original sentence.\nWhile the quality of the paraphrases is some-\nwhat variable, by generating multiple outputs and\nthen scoring them, we can select just the best qual-\nity paraphrases based on a number of criteria that\nserve to filter our output down to a set of satisfac-\ntory results.\nFirst, we obtain a similarity score between the\ngenerated paraphrase and the original sentence by\nusing the Universal Sentence Encoder (USE) (Cer\net al., 2018) to make a 512 dimensional sentence\nembedding for each output sentence and then com-\npare them to the embedding of the original sen-\ntence via the cosine similarity measure.\nAs a second step, we measure the ROUGE-\nL (Lin, 2004) score of the candidate paraphrases\nagainst the original sentence and eliminate candi-\ndates with a ROUGE-L score of above 0.7 . This\nprevents candidates that are too close to the orig-\ninal sentence being chosen. After testing both\ncutoff scores for ROUGE-L and BLEU (Papineni\net al., 2002), ROUGE-L has shown to be more use-\nful at finding candidates that are more unique in\ncomparison to the original sentence.\nBy choosing samples with sufficiently low\nROUGE-L scores but as high a similarity as pos-\nsible, we end up with an output that is semanti-\ncally similar to the original phrase but has a unique\nword order when compared to the original phrase.\n3.4 Datasets\nWe fine-tuned multiple versions of the model on\nseveral different datasets : 2 datasets of sentences\nand their matching paraphrases; and 1 dataset of\nparagraphs with matching paraphrases :\n1. The MSR Paraphrase Identification dataset\n(Dolan et al., 2004) which consists of just\nover 4,000 examples of original sentences\nwith a matching paraphrased sentence in its\ntrain set.\n2. An original dataset of 10,000 sentences from\nonline news articles along with matching\nparaphrases that were human-generated.\n3. A further original dataset of paragraphs with\ncorresponding paraphrased paragraphs from\nvarious entertainment, news, and food ar-\nticles found online, where the paraphrases\nwere human-generated.\nWe fine-tuned 3 versions of the GPT-2 model,\none corresponding to each dataset, and then made\npredictions using the same system outlined above.\nBy calculating USE, ROUGE-L and BLEU\nscores for each dataset we are able to quantify the\nquality of human-generated paraphrases and then\nuse that as a comparison for the models generated\nsentences (see Table 2).\nDataset USE R-L BLEU\nMSR train 0.8462 0.4315 0.4593\nMSR test 0.8415 0.4202 0.4966\nNews dataset 0.8948 0.4686 0.5648\nParagraphs dataset 0.9208 0.4966 0.5762\nTable 2: Average USE, ROUGE-L, BLEU Scores of\nthe datasets\n4 Experiments\nWe implemented the system described above us-\ning GPT-2 and trained it on the different datasets\nfor various lengths of training.\nTo evaluate the output of the model, we ran-\ndomly selected sentences from sources such as\nWikipedia, news sites and entertainment sites with\nno matching paraphrase to use as the conditional\ninput to the model.\n5 Results and Scoring\nWhen comparing our generated sentences with the\naverage scores of the original datasets, we can see\nthat that they compare favorably.\nAs discussed earlier, we assessed the semantic\nsimilarity of the sentence meanings using Univer-\nsal Sentence Encoder (Cer et al., 2018) and com-\npared them to the average USE score from the\ndatasets that were trained on. This showed that\nthe system can generate paraphrases which are se-\nmantically on par with the human-generated ones\nin each of the datasets.\nWe also compared the ROUGE-L (Lin, 2004)\nscores of the generated samples with the aver-\nage values for the datasets which were human-\ngenerated. This again shows that our phrases are\ncoherent and on par with human-generated para-\nphrases.\nWhen we further compared the results of unfil-\ntered examples generated by the model (Table 3)\nwe observe that when the USE score is below 0.85\nwe see clear deterioration in the semantic similar-\nity quality of the paraphrased versions.\nWe also observe that if the USE score is too\nclose to 1.0 then the ROUGE-L score also rises\nand the generated examples are too similar in word\nUSE R-L\nIn: A prisoner can asphyxiate himself in 90 seconds and, after eight minutes or\nso, he will be brain dead.\nOut 1: After 8 minutes, a brain fart will subdue the sufferer. 0.524 0.0\nOut 2: After 8 minutes, he will be brain-dead and his heart will stop. 0.565 0.138\nOut 3: A brain aneurysm can asphyxiate itself in 90 seconds and, after eight min-\nutes, it will be dead.\n0.721 0.412\nOut 4: After eight minutes, a brain anesthetist can asphyxiate a prisoner in 90 sec-\nonds and for several minutes after that.\n0.758 0.167\nOut 5: A brain-dead prisoner canasphyxiate himself in 90 seconds and then out\nloud after eight minutes.\n0.809 0.312\nOut 6: At asphyxiation, the prisoner canasphyxiate himself in 90 seconds and, after\n8 minutes, he will be brain dead.\n0.884 0.514\nOut 7: After eight minutes, a prisoner can asphyxiate himself in 90 seconds and,\nafter that, he will be brain dead.\n0.884 0.514\nOut 8*: In 90 seconds, a prisoner can asphyxiate himself and be brain dead\nafter eight minutes or so\n0.932 0.473\nOut 9: A prisoner can asphyxiate himself in 90 seconds and, after eight minutes,\nhe will be brain dead.\n0.972 0.824\nTable 3: Showing Candidates Selection and Scoring - *Selected Sentence\nand phrase selection to the original sentence to be\nuseful paraphrases.\nThis technique can be performed not only at\nsentence-level but also to generate paragraph-level\nparaphrases. Comparing USE and ROUGE-L\nscores of the generated paragraphs we see they are\nagain on par with the human generated examples\nfrom our paragraph dataset (samples are given in\nthe Supplemental Materials).\nDue to the pre-training of the Language Model,\nthe model is able to generalize to and generate\nparaphrases for types of content it has never seen\nduring the fine-tuning phase.\n6 Discussion\nThe technique outlined in this paper shows the ap-\nplicability of large language models to the para-\nphrasing task. It also highlights that there is still\nmuch to be learnt about further applications of\nlarge language models, and also the approaches\nused to fine-tune and use them for applications.\nMost of the results from models such as GPT-\n2 have focused on the quality of text genera-\ntion rather than quantitative methods for measur-\ning and improving the quality of text created, to\nmake it more consistent and usable. We pro-\npose the scoring and filtering of candidates using\ntechniques such as we have shown with USE and\nROUGE-L, may be a useful technique not just for\nparaphrasing but other text generation tasks.\nThe ability of our technique to work with long\nspans of text also gives it an advantage over prior\nwork which used rule-based and other statistical\napproaches which performed best on shorter spans\nof text.\nOur experiments show that pre-training of GPT-\n2 on such a large amount of data in the WebText\ndataset allows it to ’understand’ the syntax and to\na degree the grammar of English allowing it to\nbe able to quickly learn the task of paraphrasing\nthrough fine-tuning training on a small set of para-\nphrasing examples.\n7 Future Work\nExtrapolating from the paraphrasing results into\nmore generalizable ideas, we hope to investigate\nthe extent by which the representations learned in\nthe different layers of the transformer network cor-\nrespond to different parts of the linguistic hierar-\nchy. One possible approach to doing this would be\nto trace a set of ’markers’ through the transformer\nnetworks existing attention mechanism, in parallel\nto the text which gives rise to that structure.\nIn addition, the ability of the networks to learn\ntasks within the span of a single context frame in-\ndicates the possibility of an inherent bias towards\nmeta-( or one-shot) learning. These will be the\nsubject of further work.\nAcknowledgments\nWe would like to thank Google for access to the\nTFRC TPU program which was used in training\nand fine-tuning models for this paper.\nReferences\nColin J. Bannard and Chris Callison-Burch. 2005.\nParaphrasing with bilingual parallel corpora. In\nProceedings of the 43rd Annual Meeting of the As-\nsociation for Computational Linguistics (ACL05),\npages 597–604, Ann Arbor, Michigan. Association\nfor Computational Linguistics.\nRegina Barzilay and Kathleen McKeown. 2001. Ex-\ntracting paraphrases from a parallel corpus. In Pro-\nceedings of the 39th Annual Meeting on Association\nfor Computational Linguistics, ACL ’01, pages 50–\n57, Stroudsburg, PA, USA. Association for Compu-\ntational Linguistics.\nSamuel R. Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew M. Dai, Rafal Jo´zefowicz, and Samy Ben-\ngio. 2016. Generating sentences from a continuous\nspace. In Proceedings of The 20th SIGNLL Con-\nference on Computational Natural Language Learn-\ning, pages 10–21, Berlin, Germany. Association for\nComputational Linguistics.\nDaniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Con-\nstant, Mario Guajardo-Cespedes, Steve Yuan, Chris\nTar, Yun-Hsuan Sung, Brian Strope, and Ray\nKurzweil. 2018. Universal sentence encoder. ArXiv,\nabs/1803.11175.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nWilliam B. Dolan, Chris Quirk, and Chris Brockett.\n2004. Unsupervised construction of large para-\nphrase corpora: Exploiting massively parallel news\nsources. In COLING.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language\nmodel pre-training for natural language understand-\ning and generation. CoRR, abs/1905.03197.\nAnkush Gupta, Arvind Agarwal, Prawaan Singh, and\nPiyush Rai. 2018. A deep generative framework for\nparaphrase generation. In Thirty-Second AAAI Con-\nference on Artificial Intelligence.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 328–339, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nZichao Li, Xin Jiang, Lifeng Shang, and Hang Li.\n2018. Paraphrase generation with deep reinforce-\nment learning. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 3865–3878.\nChin-Yew Lin. 2004. Rouge: A package for auto-\nmatic evaluation of summaries. In ACL 2004, pages\n74–81, Barcelona, Spain. Association for Computa-\ntional Linguistics.\nNitin Madnani and Bonnie J. Dorr. 2010. Generat-\ning phrasal and sentential paraphrases: A survey\nof data-driven methods. Computational Linguistics,\n36:341–387.\nJonathan Mallinson, Rico Sennrich, and Mirella Lap-\nata. 2017. Paraphrasing revisited with neural ma-\nchine translation. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 1, Long Pa-\npers, pages 881–893.\nKathleen McKeown. 1979. Paraphrasing using given\nand new information in a question-answer system.\nIn 17th Annual Meeting of the Association for Com-\nputational Linguistics, pages 67–72.\nMarie Meteer and Varda Shaked. 1988. Strategies for\neffective paraphrasing. In Coling Budapest 1988\nVolume 2: International Conference on Computa-\ntional Linguistics.\nBo Pang, Kevin Knight, and Daniel Marcu. 2003. Syn-\ntax-based alignment of multiple translations: Ex-\ntracting paraphrases and generating new sentences.\nIn Proceedings of the 2003 Human Language Tech-\nnology Conference of the North American Chapter\nof the Association for Computational Linguistics,\npages 181–188.\nKishore Papineni, Salim Roukos, ToddWard, andWei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer,\nMatt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. 2018. Deep contextualized word\nrepresentations. pages 2227–2237, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nAaditya Prakash, Sadid A. Hasan, Kathy Lee, Vivek V.\nDatla, Ashequl Qadir, Joey Liu, and Oladimeji Farri.\n2016. Neural paraphrase generation with stacked\nresidual LSTM networks. CoRR, abs/1610.03098.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nSascha Rothe, Shashi Narayan, and Aliaksei Severyn.\n2019. Leveraging pre-trained checkpoints for se-\nquence generation tasks. CoRR, abs/1907.12461.\nAurko Roy and David Grangier. 2019. Unsuper-\nvised paraphrasing without translation. ArXiv,\nabs/1905.12752.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nSupplemental Materials\nIn: The first step in solving the problem is to prop the door open with tape. This means your cat\nwill now be able to see through to the other side; your cat will likely begin using the kitty\ndoor immediately. Once your cat has gotten used to using the kitty door, remove the tape.\nSometimes cats will continue to use the kitty door without any more prompting.\nOut: The first step in resolving the problem is to prop the gate with tape. This implies that your\ncat will shortly be enabled to see through to the other side; your cat will likely immediately\nbegin using the kitty door. Remove the tape when your cat has got used to using the kitty door.\nSometimes cats will continue to use the kitty gate without any more command.\nScores: USE: 0.9835 ROUGE-L: 0.6102 BLEU: 0.6858\nIn: I do well in school, and people think I am smart because of it. But its not true. In fact, three\nyears ago I struggled in school. However, two years ago I decided to get serious about school\nand made a few changes. First, I decided I would become interested in whatever was being\ntaught, regardless of what other people thought. I also decided I would work hard every day\nand never give up on any assignment.\nOut: I do well in school, and individuals believe that I am intelligent because of it. But it’s not true.\nIn fact, three years ago I struggled in school. I decided I would become interested in whatever\nwas taught, regardless of what other people thought. I also decided I would work every day\nand never give up on any task.\nScores: USE: 0.9380 ROUGE-L: 0.6929 BLEU: 0.6221\nIn: There are three reasons why Canada is one of the best countries in the world. First, Canada has\nan excellent health care system. All Canadians have access to medical services at a reasonable\nprice. Second, Canada has a high standard of education. Students are taught by well-trained\nteachers and are encouraged to continue studying at university. Finally, Canada’s cities are\nclean and efficiently managed. Canadian cities have many parks and lots of space for people to\nlive. As a result, Canada is a desirable place to live.\nOut: There are three main reasons why Canada is one of the finest locations in the globe. First,\nCanada has an excellent health care system. All Canadians have access to medical care at a\nreasonable cost. Second, Canada has a high standard of education. Students are taught by well-\ntrained teachers and are urged to continue studying at university. Finally, Canada’s cities are\nclean and efficiently managed with many parks and lots of room for living, as a consequence,\nCanada is a desirable location.\nScores: USE: 0.9775 ROUGE-L: 0.6832 BLEU: 0.7182\nTable 4: Sample paragraphs pairs\n",
    "Link": "http://arxiv.org/abs/1911.09661"
}