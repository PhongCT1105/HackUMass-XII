{
    "Title": "Large Language Models: A Survey",
    "Authors": "Amatriain, Xavier, Chenaghlu, Meysam, Gao, Jianfeng, Mikolov, Tomas, Minaee, Shervin, Nikzad, Narjes, Socher, Richard",
    "Year": "No year available",
    "Abstract": "Large Language Models (LLMs) have drawn a lot of attention due to their\nstrong performance on a wide range of natural language tasks, since the release\nof ChatGPT in November 2022. LLMs' ability of general-purpose language\nunderstanding and generation is acquired by training billions of model's\nparameters on massive amounts of text data, as predicted by scaling laws\n\\cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while\nvery recent, is evolving rapidly in many different ways. In this paper, we\nreview some of the most prominent LLMs, including three popular LLM families\n(GPT, LLaMA, PaLM), and discuss their characteristics, contributions and\nlimitations. We also give an overview of techniques developed to build, and\naugment LLMs. We then survey popular datasets prepared for LLM training,\nfine-tuning, and evaluation, review widely used LLM evaluation metrics, and\ncompare the performance of several popular LLMs on a set of representative\nbenchmarks. Finally, we conclude the paper by discussing open challenges and\nfuture research directions.Comment: arXiv admin note: substantial text overlap with arXiv:2401.1442",
    "Keywords": "No keywords available",
    "Publisher": "",
    "Publication Date": "No publication date available",
    "Journal": "No journal available",
    "Citation Count": 0,
    "Full Text": "Large Language Models: A SurveyShervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam ChenaghluRichard Socher, Xavier Amatriain, Jianfeng GaoAbstract—Large Language Models (LLMs) have drawn alot of attention due to their strong performance on a widerange of natural language tasks, since the release of ChatGPTin November 2022. LLMs’ ability of general-purpose languageunderstanding and generation is acquired by training billions ofmodel’s parameters on massive amounts of text data, as predictedby scaling laws [1], [2]. The research area of LLMs, while veryrecent, is evolving rapidly in many different ways. In this paper,we review some of the most prominent LLMs, including threepopular LLM families (GPT, LLaMA, PaLM), and discuss theircharacteristics, contributions and limitations. We also give anoverview of techniques developed to build, and augment LLMs.We then survey popular datasets prepared for LLM training,fine-tuning, and evaluation, review widely used LLM evaluationmetrics, and compare the performance of several popular LLMson a set of representative benchmarks. Finally, we concludethe paper by discussing open challenges and future researchdirections.I. INTRODUCTIONLanguage modeling is a long-standing research topic, dat-ing back to the 1950s with Shannon’s application of informa-tion theory to human language, where he measured how wellsimple n-gram language models predict or compress naturallanguage text [3]. Since then, statistical language modelingbecame fundamental to many natural language understandingand generation tasks, ranging from speech recognition, ma-chine translation, to information retrieval [4], [5], [6].The recent advances on transformer-based large languagemodels (LLMs), pretrained on Web-scale text corpora, signif-icantly extended the capabilities of language models (LLMs).For example, OpenAI’s ChatGPT and GPT-4 can be used notonly for natural language processing, but also as general tasksolvers to power Microsoft’s Co-Pilot systems, for instance,can follow human instructions of complex new tasks per-forming multi-step reasoning when needed. LLMs are thusbecoming the basic building block for the development ofgeneral-purpose AI agents or artificial general intelligence(AGI).As the field of LLMs is moving fast, with new findings,models and techniques being published in a matter of monthsor weeks [7], [8], [9], [10], [11], AI researchers and practi-tioners often find it challenging to figure out the best recipesto build LLM-powered AI systems for their tasks. This papergives a timely survey of the recent advances on LLMs. Wehope this survey will prove a valuable and accessible resourcefor students, researchers and developers.LLMs are large-scale, pre-trained, statistical language mod-els based on neural networks. The recent success of LLMs isan accumulation of decades of research and development oflanguage models, which can be categorized into four wavesthat have different starting points and velocity: statistical lan-guage models, neural language models, pre-trained languagemodels and LLMs.Statistical language models (SLMs) view text as a sequenceof words, and estimate the probability of text as the productof their word probabilities. The dominating form of SLMsare Markov chain models known as the n-gram models,which compute the probability of a word conditioned on itsimmediate proceeding n − 1 words. Since word probabilitiesare estimated using word and n-gram counts collected fromtext corpora, the model needs to deal with data sparsity (i.e.,assigning zero probabilities to unseen words or n-grams) byusing smoothing, where some probability mass of the modelis reserved for unseen n-grams [12]. N-gram models arewidely used in many NLP systems. However, these modelsare incomplete in that they cannot fully capture the diversityand variability of natural language due to data sparsity.Early neural language models (NLMs) [13], [14], [15], [16]deal with data sparsity by mapping words to low-dimensionalcontinuous vectors (embedding vectors) and predict the nextword based on the aggregation of the embedding vectors ofits proceeding words using neural networks. The embeddingvectors learned by NLMs define a hidden space where thesemantic similarity between vectors can be readily computedas their distance. This opens the door to computing semanticsimilarity of any two inputs regardless their forms (e.g., queriesvs. documents in Web search [17], [18], sentences in differentlanguages in machine translation [19], [20]) or modalities (e.g.,image and text in image captioning [21], [22]). Early NLMs aretask-specific models, in that they are trained on task-specificdata and their learned hidden space is task-specific.Pre-trained language models (PLMs), unlike early NLMs,are task-agnostic. This generality also extends to the learnedhidden embedding space. The training and inference of PLMsfollows the pre-training and fine-tuning paradigm, where lan-guage models with recurrent neural networks [23] or trans-formers [24], [25], [26] are pre-trained on Web-scale unlabeledtext corpora for general tasks such as word prediction, and thenfinetuned to specific tasks using small amounts of (labeled)task-specific data. Recent surveys on PLMs include [8], [27],[28].Large language models (LLMs) mainly refer totransformer-based neural language models 1 that containtens to hundreds of billions of parameters, which are pre-trained on massive text data, such as PaLM [31], LLaMA[32], and GPT-4 [33], as summarized in Table III. Compared1Recently, several very promising non-transformer LLMs have been pro-posed, such as the LLMs based on structured state space models [29], [30].See Section VII for more details.arXiv:2402.06196v2  [cs.CL]  20 Feb 2024EmergingBasic AugmentedLLM CapabilitiesReasoningCodingComprehensionMultilingualToolutilizationWorldknowledgeInstructionfollowing In-contextlearningInteractingwith usersSelf-improvementMulti choice QAWikipedia QAXNLICrosslingual QACrosslingual TasksTranslationReading ComprehensionMulti choice QABoolean QASimplificationSummarizationFunction CallingAPI callingLogicalSymbolicCommon SenseArithmeticTurn basedCompletionTask definitionFew-shotSymbolicreferencePos/Neg exampleStep by stepsolvingTool planningTaskdecompositionVirtual actingPhysical actingKnowledge baseutilizationAssignmentplanningSelf-cirtisimSelf-refinementFig. 1: LLM Capabilities.to PLMs, LLMs are not only much larger in model size, butalso exhibit stronger language understanding and generationabilities, and more importantly, emergent abilities that arenot present in smaller-scale language models. As illustratedin Fig. 1, these emergent abilities include (1) in-contextlearning, where LLMs learn a new task from a small setof examples presented in the prompt at inference time, (2)instruction following, where LLMs, after instruction tuning,can follow the instructions for new types of tasks withoutusing explicit examples, and (3) multi-step reasoning, whereLLMs can solve a complex task by breaking down that taskinto intermediate reasoning steps as demonstrated in thechain-of-thought prompt [34]. LLMs can also be augmentedby using external knowledge and tools [35], [36] so thatthey can effectively interact with users and environment [37],and continually improve itself using feedback data collectedthrough interactions (e.g. via reinforcement learning withhuman feedback (RLHF)).Through advanced usage and augmentation techniques,LLMs can be deployed as so-called AI agents: artificial entitiesthat sense their environment, make decisions, and take actions.Previous research has focused on developing agents for specifictasks and domains. The emergent abilities demonstrated byLLMs make it possible to build general-purpose AI agentsbased on LLMs. While LLMs are trained to produce responsesin static settings, AI agents need to take actions to interact withdynamic environment. Therefore, LLM-based agents oftenneed to augment LLMs to e.g., obtain updated informationfrom external knowledge bases, verify whether a system actionproduces the expected result, and cope with when things donot go as expected, etc. We will discuss in detail LLM-basedagents in Section IV.In the rest of this paper, Section II presents an overview ofstate of the art of LLMs, focusing on three LLM families (GPT,LLaMA and PaLM) and other representative models. SectionIII discusses how LLMs are built. Section IV discusses howLLMs are used, and augmented for real-world applicationsSections V and VI review popular datasets and benchmarks forevaluating LLMs, and summarize the reported LLM evaluationresults. Finally, Section VII concludes the paper by summa-rizing the challenges and future research directions.II. LARGE LANGUAGE MODELSIn this section we start with a review of early pre-trainedneural language models as they are the base of LLMs, andthen focus our discussion on three families of LLMs: GPT,LlaMA, and PaLM. Table I provides an overview of some ofthese models and their characteristics.A. Early Pre-trained Neural Language ModelsLanguage modeling using neural networks was pioneeredby [38], [39], [40]. Bengio et al. [13] developed one of the firstneural language models (NLMs) that are comparable to n-grammodels. Then, [14] successfully applied NLMs to machinetranslation. The release of RNNLM (an open source NLMtoolkit) by Mikolov [41], [42] helped significantly popularizeNLMs. Afterwards, NLMs based on recurrent neural networks(RNNs) and their variants, such as long short-term memory(LSTM) [19] and gated recurrent unit (GRU) [20], were widelyused for many natural language applications including machinetranslation, text generation and text classification [43].Then, the invention of the Transformer architecture [44]marks another milestone in the development of NLMs. Byapplying self-attention to compute in parallel for every wordin a sentence or document an “attention score” to model theinfluence each word has on another, Transformers allow formuch more parallelization than RNNs, which makes it possibleto efficiently pre-train very big language models on largeamounts of data on GPUs. These pre-trained language models(PLMs) can be fine-tuned for many downstream tasks.Paper StrcutureEarly Pre-trainedLanguage ModelsII Large Language ModelsAIII HOW LLMS ARE BUILTAData CleaningBLarge LanguageModel FamiliesBOther RepresentativeLLMsCDominant LLMArchitecturesTokenizationsCPositional EncodingDModel Pre-trainingEFine-tuning andInstruction TuningFAlignmentGDecoding StrategiesHI HOW LLMS ARE USED AND AUGMENTEDABLLM limitationsCost-Effective Training/Inference,Adaptation & CompressionIUsing LLMs: Prompt Designand EngineeringC Augmenting LLMs throughexternal knowledge - RAGD Using External ToolsE LLM AgentsV  POPULAR DATASETS FOR LLMSA Datasets for Basic Tasks: languagemodeling/understanding/generationB  Datasets for Emergent: ICL, reasoning,instruction followingC Datasets for Augmented: usingexternal knowledge/toolsVI  PROMINENT LLMS’ PERFORMANCEON BENCHMARKSABVII CHALLENGES AND FUTURE DIRECTIONSA Smaller and more efficientLanguage ModelsLLMs’ Performance on Different TasksPopular Metrics for Evaluating LLMsB New Post-attentionArchitectural ParadigmsC Multi-modal ModelsD Improved LLM Usage andAugmentation techniquesD Security andEthical/Responsible AIFig. 2: The paper structure.We group early popular Transformer-based PLMs, based ontheir neural architectures, into three main categories: encoder-only, decoder-only, and encoder-decoder models. Comprehen-sive surveys of early PLMs are provided in [43], [28].1) Encoder-only PLMs: As the name suggests, the encoder-only models only consist of an encoder network. These modelsare originally developed for language understanding tasks,such as text classification, where the models need to predict aclass label for an input text. Representative encoder-only mod-els include BERT and its variants, e.g., RoBERTa, ALBERT,DeBERTa, XLM, XLNet, UNILM, as to be described below.BERT (Birectional Encoder Representations from Trans-formers) [24] is one of the most widely used encoder-onlylanguage models. BERT consists of three modules: (1) anembedding module that converts input text into a sequenceof embedding vectors, (2) a stack of Transformer encodersthat converts embedding vectors into contextual representationvectors, and (3) a fully connected layer that converts therepresentation vectors (at the final layer) to one-hot vectors.BERT is pre-trained uses two objectives: masked languagemodeling (MLM) and next sentence prediction. The pre-trainedBERT model can be fine-tuned by adding a classifier layerfor many language understanding tasks, ranging from textTABLE I: High-level Overview of Popular Language ModelsType Model Name #Parameters Release Base Models OpenSource#Tokens Training datasetBERT 110M, 340M 2018 - ✓ 137B BooksCorpus, English WikipediaRoBERTa 355M 2019 - ✓ 2.2T BooksCorpus, English Wikipedia, CC-NEWS,STORIES (a subset of Common Crawl), RedditEncoder-Only ALBERT 12M, 18M, 60M,235M2019 - ✓ 137B BooksCorpus, English WikipediaDeBERTa - 2020 - ✓ - BooksCorpus, English Wikipedia, STORIES, Red-dit contentXLNet 110M, 340M 2019 - ✓ 32.89B BooksCorpus, English Wikipedia, Giga5, Com-mon Crawl, ClueWeb 2012-BDecoder-only GPT-1 120M 2018 - ✓ 1.3B BooksCorpusGPT-2 1.5B 2019 - ✓ 10B Reddit outboundT5 (Base) 223M 2019 - ✓ 156B Common CrawlEncoder-Decoder MT5 (Base) 300M 2020 - ✓ - New Common Crawl-based dataset in 101 lan-guages (m Common Crawl)BART (Base) 139M 2019 - ✓ - Corrupting textGPT-3 125M, 350M,760M, 1.3B, 2.7B,6.7B, 13B, 175B2020 × 300B Common Crawl (filtered), WebText2, Books1,Books2, WikipediaGPT Family CODEX 12B 2021 GPT ✓ - Public GitHub software repositoriesWebGPT 760M, 13B, 175B 2021 GPT-3 × - ELI5GPT-4 1.76T 2023 - × 13T -LLaMA1 7B, 13B, 33B, 65B 2023 - ✓ 1T, 1.4T Online sourcesLLaMA2 7B, 13B, 34B, 70B 2023 - ✓ 2T Online sourcesAlpaca 7B 2023 LLaMA1 ✓ - GPT-3.5Vicuna-13B 13B 2023 LLaMA1 ✓ - GPT-3.5LLaMA Family Koala 13B 2023 LLaMA ✓ - Dialogue dataMistral-7B 7.3B 2023 ✓ - -Code Llama 34 2023 LLaMA2 ✓ 500B Publicly available codeLongLLaMA 3B, 7B 2023 OpenLLaMA ✓ 1T -LLaMA-Pro-8B 8.3B 2024 LLaMA2-7B ✓ 80B Code and math corporaTinyLlama-1.1B 1.1B 2024 LLaMA1.1B ✓ 3T SlimPajama, StarcoderdataPaLM 8B, 62B, 540B 2022 - × 780B Web documents, books, Wikipedia, conversations,GitHub codeU-PaLM 8B, 62B, 540B 2022 - × 1.3B Web documents, books, Wikipedia, conversations,GitHub codePaLM Family PaLM-2 340B 2023 - ✓ 3.6T Web documents, books, code, mathematics, con-versational dataMed-PaLM 540B 2022 PaLM × 780B HealthSearchQA, MedicationQA, LiveQAMed-PaLM 2 - 2023 PaLM 2 × - MedQA, MedMCQA, HealthSearchQA, LiveQA,MedicationQAFLAN 137B 2021 LaMDA-PT ✓ - Web documents, code, dialog data, WikipediaGopher 280B 2021 - × 300B MassiveTextERNIE 4.0 10B 2023 - × 4TB Chinese textRetro 7.5B 2021 - × 600B MassiveTextLaMDA 137B 2022 - × 168B public dialog data and web documentsChinChilla 70B 2022 - × 1.4T MassiveTextGalactia-120B 120B 2022 - 450BOther Popular LLMs CodeGen 16.1B 2022 - ✓ - THE PILE, BIGQUERY, BIGPYTHONBLOOM 176B 2022 - ✓ 366B ROOTSZephyr 7.24B 2023 Mistral-7B ✓ 800B Synthetic dataGrok-0 33B 2023 - × - Online sourceORCA-2 13B 2023 LLaMA2 - 2001B -StartCoder 15.5B 2023 - ✓ 35B GitHubMPT 7B 2023 - ✓ 1T RedPajama, m Common Crawl, S2ORC, CommonCrawlMixtral-8x7B 46.7B 2023 - ✓ - Instruction datasetFalcon 180B 180B 2023 - ✓ 3.5T RefinedWebGemini 1.8B, 3.25B 2023 ✓ - Web documents, books, and code, image data,audio data, video dataDeepSeek-Coder 1.3B, 6.7B, 33B 2024 - ✓ 2T GitHub’s Markdown and StackExchangeDocLLM 1B,7B 2024 - × 2T IIT-CDIP Test Collection 1.0, DocBankclassification, question answering to language inference. Ahigh-level overview of BERT framework is shown in Fig 3. AsBERT significantly improved state of the art on a wide rangeof language understanding tasks when it was published, the AIcommunity was inspired to develop many similar encoder-onlylanguage models based on BERT.RoBERTa [25] significantly improves the robustness ofBERT using a set of model design choices and training strate-gies, such as modifying a few key hyperparameters, removingthe next-sentence pre-training objective and training with muchlarger mini-batches and learning rates. ALBERT [45] uses twoparameter-reduction techniques to lower memory consumptionand increase the training speed of BERT: (1) splitting theembedding matrix into two smaller matrices, and (2) usingrepeating layers split among groups. DeBERTa (Decoding-enhanced BERT with disentangled attention) [26] improves theBERT and RoBERTa models using two novel techniques. Thefirst is the disentangled attention mechanism, where each wordis represented using two vectors that encode its content andposition, respectively, and the attention weights among wordsFig. 3: Overall pre-training and fine-tuning procedures forBERT. Courtesy of [24]are computed using disentangled matrices on their contents andrelative positions, respectively. Second, an enhanced mask de-coder is used to incorporate absolute positions in the decodinglayer to predict the masked tokens in model pre-training. Inaddition, a novel virtual adversarial training method is used forfine-tuning to improve models’ generalization. ELECTRA [46]uses a new pre-training task, known as replaced token detection(RTD), which is empirically proven to be more sample-efficientthan MLM. Instead of masking the input, RTD corrupts it byreplacing some tokens with plausible alternatives sampled froma small generator network. Then, instead of training a modelthat predicts the original identities of the corrupted tokens, adiscriminative model is trained to predict whether a token inthe corrupted input was replaced by a generated sample or not.RTD is more sample-efficient than MLM because the formeris defined over all input tokens rather than just the small subsetbeing masked out, as illustrated in Fig 4.Fig. 4: A comparison between replaced token detection andmasked language modeling. Courtesy of [46].XLMs [47] extended BERT to cross-lingual languagemodels using two methods: (1) a unsupervised method thatonly relies on monolingual data, and (2) a supervised methodthat leverages parallel data with a new cross-lingual languagemodel objective, as illustrated in Fig 5. XLMs had obtainedstate-of-the-art results on cross-lingual classification, unsuper-vised and supervised machine translation, at the time they wereproposed.There are also encoder-only language models that leveragethe advantages of auto-regressive (decoder) models for modeltraining and inference. Two examples are XLNet and UNILM.XLNet [48] is based on Transformer-XL, pre-trained using ageneralized autoregressive method that enables learning bidi-rectional contexts by maximizing the expected likelihood overFig. 5: Cross-lingual language model pretraining. The MLMobjective is similar to BERT, but with continuous streamsof text as opposed to sentence pairs. The TLM objectiveextends MLM to pairs of parallel sentences. To predict amasked English word, the model can attend to both the Englishsentence and its French translation, and is encouraged to alignEnglish and French representations. Courtesy of [47].all permutations of the factorization order. UNILM (UNIfiedpre-trained Language Model) [49] is pre-trained using threetypes of language modeling tasks: unidirectional, bidirectional,and sequence-to-sequence prediction. This is achieved byemploying a shared Transformer network and utilizing specificself-attention masks to control what context the prediction isconditioned on, as illustrated in Fig 6. The pre-trained modelcan be fine-tuned for both natural language understanding andgeneration tasks.Fig. 6: Overview of unified LM pre-training. The modelparameters are shared across the LM objectives (i.e., bidirec-tional LM, unidirectional LM, and sequence-to-sequence LM).Courtesy of [49].2) Decoder-only PLMs: Two of the most widely useddecoder-only PLMs are GPT-1 and GPT-2, developed byOpenAI. These models lay the foundation to more powerfulLLMs subsequently, i.e., GPT-3 and GPT-4.GPT-1 [50] demonstrates for the first time that goodperformance over a wide range of natural language tasks can beobtained by Generative Pre-Training (GPT) of a decoder-onlyTransformer model on a diverse corpus of unlabeled text in aself-supervised learning fashion (i.e., next word/token predic-tion), followed by discriminative fine-tuning on each specificdownstream task (with much fewer samples), as illustrated inFig 7. GPT-1 paves the way for subsequent GPT models, witheach version improving upon the architecture and achievingbetter performance on various language tasks.Fig. 7: High-level overview of GPT pretraining, and fine-tuningsteps. Courtesy of OpenAI.GPT-2 [51] shows that language models are able to learnto perform specific natural language tasks without any explicitsupervision when trained on a large WebText dataset consistingof millions of webpages. The GPT-2 model follows the modeldesigns of GPT-1 with a few modifications: Layer normal-ization is moved to the input of each sub-block, additionallayer normalization is added after the final self-attention block,initialization is modified to account for the accumulation onthe residual path and scaling the weights of residual layers,vocabulary size is expanded to 50,25, and context size isincreased from 512 to 1024 tokens.3) Encoder-Decoder PLMs: In [52], Raffle et al. shows thatalmost all NLP tasks can be cast as a sequence-to-sequencegeneration task. Thus, an encoder-decoder language model, bydesign, is a unified model in that it can perform all naturallanguage understanding and generation tasks. Representativeencoder-decoder PLMs we will review below are T5, mT5,MASS, and BART.T5 [52] is a Text-to-Text Transfer Transformer (T5) model,where transfer learning is effectively exploited for NLP via anintroduction of a unified framework in which all NLP tasks arecast as a text-to-text generation task. mT5 [53] is a multilingualvariant of T5, which is pre-trained on a new Common Crawl-based dataset consisting of texts in 101 languages.MASS (MAsked Sequence to Sequence pre-training) [54]adopts the encoder-decoder framework to reconstruct a sen-tence fragment given the remaining part of the sentence. Theencoder takes a sentence with randomly masked fragment(several consecutive tokens) as input, and the decoder predictsthe masked fragment. In this way, MASS jointly trains theencoder and decoder for language embedding and generation,respectively.BART [55] uses a standard sequence-to-sequence transla-tion model architecture. It is pre-trained by corrupting text withan arbitrary noising function, and then learning to reconstructthe original text.B. Large Language Model FamiliesLarge language models (LLMs) mainly refer totransformer-based PLMs that contain tens to hundredsof billions of parameters. Compared to PLMs reviewed above,LLMs are not only much larger in model size, but also exhibitstronger language understanding and generation and emergentabilities that are not present in smaller-scale models. In whatfollows, we review three LLM families: GPT, LLaMA, andPaLM, as illustrated in Fig 8.1) The GPT Family: Generative Pre-trained Transform-ers (GPT) are a family of decoder-only Transformer-basedlanguage models, developed by OpenAI. This family con-sists of GPT-1, GPT-2, GPT-3, InstrucGPT, ChatGPT, GPT-4,CODEX, and WebGPT. Although early GPT models, such asGPT-1 and GPT-2, are open-source, recent models, such asGPT-3 and GPT-4, are close-source and can only be accessedvia APIs. GPT-1 and GPT-2 models have been discussed inthe early PLM subsection. We start with GPT-3 below.GPT-3 [56] is a pre-trained autoregressive language modelwith 175 billion parameters. GPT-3 is widely considered asthe first LLM in that it not only is much larger than previousPLMs, but also for the first time demonstrates emergentabilities that are not observed in previous smaller PLMs. GPT-3 shows the emergent ability of in-context learning, whichmeans GPT-3 can be applied to any downstream tasks withoutany gradient updates or fine-tuning, with tasks and few-shotdemonstrations specified purely via text interaction with themodel. GPT-3 achieved strong performance on many NLPtasks, including translation, question-answering, and the clozetasks, as well as several ones that require on-the-fly reasoningor domain adaptation, such as unscrambling words, using anovel word in a sentence, 3-digit arithmetic. Fig 9 plots theperformance of GPT-3 as a function of the number of examplesin in-context prompts.CODEX [57], released by OpenAI in March 2023, is ageneral-purpose programming model that can parse naturallanguage and generate code in response. CODEX is a de-scendant of GPT-3, fine-tuned for programming applicationson code corpora collected from GitHub. CODEX powersMicrosoft’s GitHub Copilot.WebGPT [58] is another descendant of GPT-3, fine-tuned toanswer open-ended questions using a text-based web browser,facilitating users to search and navigate the web. Specifically,WebGPT is trained in three steps. The first is for WebGPTto learn to mimic human browsing behaviors using humandemonstration data. Then, a reward function is learned topredict human preferences. Finally, WebGPT is refined tooptimize the reward function via reinforcement learning andrejection sampling.To enable LLMs to follow expected human instructions,InstructGPT [59] is proposed to align language models withuser intent on a wide range of tasks by fine-tuning withhuman feedback. Starting with a set of labeler-written promptsand prompts submitted through the OpenAI API, a datasetof labeler demonstrations of the desired model behavior iscollected. Then GPT-3 is fine-tuned on this dataset. Then, adataset of human-ranked model outputs is collected to furtherfine-tune the model using reinforcement learning. The methodis known Reinforcement Learning from Human FeedbackGPT Family PaLM Family   LLaMA 1/2 FamilyGPTGPT1GPT2GPT3GPT4GPT3.5 Turbotext-davincicode-davinciCODEXInstructGPTWebGPTGPT4 VisionGPT4 TurboGorillaMistralVigogneStable Beluga2KoalaCode LLaMAVicuna AlpacaBaizeLong LLaMAGiraffeGuanacoTuluWizardLMMed-PaLMPaLM-EMed-PaLM2FLAN-PaLMU-PaLMPaLM2PaLMFig. 8: Popular LLM Families.Fig. 9: GPT-3 shows that larger models make increasinglyefficient use of in-context information. It shows in-contextlearning performance on a simple task requiring the model toremove random symbols from a word, both with and withouta natural language task description. Courtesy of [56].(RLHF), as shown in 10. The resultant InstructGPT modelshave shown improvements in truthfulness and reductions intoxic output generation while having minimal performanceregressions on public NLP datasets.Fig. 10: The high-level overview of RLHF. Courtesy of [59].The most important milestone of LLM development is thelaunch of ChatGPT (Chat Generative Pre-trained Transformer)[60] on November 30, 2022. ChatGPT is chatbot that enablesusers to steer a conversation to complete a wide range oftasks such as question answering, information seeking, textsummarization, and more. ChatGPT is powered by GPT-3.5(and later by GPT-4), a sibling model to InstructGPT, whichis trained to follow an instruction in a prompt and provide adetailed response.GPT-4 [33] is the latest and most powerful LLM in theGPT family. Launched in March, 2023, GPT-4 is a multi-modal LLM in that it can take image and text as inputs andproduce text outputs. While still less capable than humansin some of the most challenging real-world scenarios, GPT-4exhibits human-level performance on various professional andacademic benchmarks, including passing a simulated bar examwith a score around the top 10% of test takers, as shown inFig 11. Like early GPT models, GPT-4 was first pre-trained topredict next tokens on large text corpora, and then fine-tunedwith RLHF to align model behaviors with human-desired ones.2) The LLaMA Family: LLaMA is a collection of founda-tion language models, released by Meta. Unlike GPT models,LLaMA models are open-source, i.e., model weights arereleased to the research community under a noncommerciallicense. Thus, the LLaMA family grows rapidly as thesemodels are widely used by many research groups to developbetter open-source LLMs to compete the closed-source ones orto develop task-specific LLMs for mission-critical applications.The first set of LLaMA models [32] was released in Febru-ary 2023, ranging from 7B to 65B parameters. These modelsare pre-trained on trillions of tokens, collected from publiclyavailable datasets. LLaMA uses the transformer architecture ofGPT-3, with a few minor architectural modifications, including(1) using a SwiGLU activation function instead of ReLU,(2) using rotary positional embeddings instead of absolutepositional embedding, and (3) using root-mean-squared layer-normalization instead of standard layer-normalization. Theopen-source LLaMA-13B model outperforms the proprietaryGPT-3 (175B) model on most benchmarks, making it a goodbaseline for LLM research.Fig. 11: GPT-4 performance on academic and professionalexams, compared with GPT 3.5. Courtesy of [33].In July 2023, Meta, in partnership with Microsoft, releasedthe LLaMA-2 collection [61], which include both foundationlanguage models and Chat models finetuned for dialog, knownas LLaMA-2 Chat. The LLaMA-2 Chat models were reportedto outperform other open-source models on many publicbenchmarks. Fig 12 shows the training process of LLaMA-2Chat. The process begins with pre-training LLaMA-2 usingpublicly available online data. Then, an initial version ofLLaMA-2 Chat is built via supervised fine-tuning. Subse-quently, the model is iteratively refined using RLHF, rejectionsampling and proximal policy optimization. In the RLHF stage,the accumulation of human feedback for revising the rewardmodel is crucial to prevent the reward model from beingchanged too much, which could hurt the stability of LLaMAmodel training.Fig. 12: Training of LLaMA-2 Chat. Courtesy of [61].Alpaca [62] is fine-tuned from the LLaMA-7B model using52K instruction-following demonstrations generated in thestyle of self-instruct using GPT-3.5 (text-davinci-003). Alpacais very cost-effective for training, especially for academicresearch. On the self-instruct evaluation set, Alpaca performssimilarly to GPT-3.5, despite that Alpaca is much smaller.The Vicuna team has developed a 13B chat model, Vicuna-13B, by fine-tuning LLaMA on user-shared conversationscollected from ShareGPT. Preliminary evaluation using GPT-4 as a evaluator shows that Vicuna-13B achieves more than90% quality of OpenAI’s ChatGPT, and Google’s Bard whileoutperforming other models like LLaMA and Stanford Alpacain more than 90% of cases. 13 shows the relative responsequality of Vicuna and a few other well-known models byGPT-4. Another advantage of Vicuna-13B is its relative limitedcomputational demand for model training. The training cost ofVicuna-13B is merely $300.Fig. 13: Relative Response Quality of Vicuna and a few otherwell-known models by GPT-4. Courtesy of Vicuna Team.Like Alpaca and Vicuna, the Guanaco models [63] are alsofinetuned LLaMA models using instruction-following data. Butthe finetuning is done very efficiently using QLoRA suchthat finetuning a 65B parameter model can be done on asingle 48GB GPU. QLoRA back-propagates gradients througha frozen, 4-bit quantized pre-trained language model into LowRank Adapters (LoRA). The best Guanaco model outperformsall previously released models on the Vicuna benchmark,reaching 99.3% of the performance level of ChatGPT whileonly requiring 24 hours of fine-tuning on a single GPU.Koala [64] is yet another instruction-following languagemodel built on LLaMA, but with a specific focus on interactiondata that include user inputs and responses generated by highlycapable closed-source chat models such as ChatGPT. TheKoala-13B model performs competitively with state-of-the-artchat models according to human evaluation based on real-world user prompts.Mistral-7B [65] is a 7B-parameter language model engi-neered for superior performance and efficiency. Mistral-7Boutperforms the best open-source 13B model (LLaMA-2-13B)across all evaluated benchmarks, and the best open-source34B model (LLaMA-34B) in reasoning, mathematics, and codegeneration. This model leverages grouped-query attention forfaster inference, coupled with sliding window attention toeffectively handle sequences of arbitrary length with a reducedinference cost.The LLaMA family is growing rapidly, as more instruction-following models have been built on LLaMA or LLaMA-2, including Code LLaMA [66], Gorilla [67], Giraffe [68],Vigogne [69], Tulu 65B [70], Long LLaMA [71], and StableBeluga2 [72], just to name a few.3) The PaLM Family: The PaLM (Pathways LanguageModel) family are developed by Google. The first PaLMmodel [31] was announced in April 2022 and remained privateuntil March 2023. It is a 540B parameter transformer-basedLLM. The model is pre-trained on a high-quality text corpusconsisting of 780 billion tokens that comprise a wide rangeof natural language tasks and use cases. PaLM is pre-trainedon 6144 TPU v4 chips using the Pathways system, whichenables highly efficient training across multiple TPU Pods.PaLM demonstrates continued benefits of scaling by achiev-ing state-of-the-art few-shot learning results on hundreds oflanguage understanding and generation benchmarks. PaLM-540B outperforms not only state-of-the-art fine-tuned modelson a suite of multi-step reasoning tasks, but also on par withhumans on the recently released BIG-bench benchmark.The U-PaLM models of 8B, 62B, and 540B scales arecontinually trained on PaLM with UL2R, a method of continuetraining LLMs on a few steps with UL2’s mixture-of-denoiserobjective [73]. An approximately 2x computational savingsrate is reported.U-PaLM is later instruction-finetuned as Flan-PaLM [74].Compared to other instruction finetuning work mentionedabove, Flan-PaLM’s finetuning is performed using a muchlarger number of tasks, larger model sizes, and chain-of-thought data. As a result, Flan-PaLM substantially outperformsprevious instruction-following models. For instance, Flan-PaLM-540B, which is instruction-finetuned on 1.8K tasks,outperforms PaLM-540B by a large margin (+9.4% on av-erage). The finetuning data comprises 473 datasets, 146 taskcategories, and 1,836 total tasks, as illustrated in Fig 14.Fig. 14: Flan-PaLM finetuning consist of 473 datasets in abovetask categories. Courtesy of [74].PaLM-2 [75] is a more compute-efficient LLM with bet-ter multilingual and reasoning capabilities, compared to itspredecessor PaLM. PaLM-2 is trained using a mixture ofobjectives. Through extensive evaluations on English, multi-lingual, and reasoning tasks, PaLM-2 significantly improvesthe model performance on downstream tasks across differentmodel sizes, while simultaneously exhibiting faster and moreefficient inference than PaLM.Med-PaLM [76] is a domain-specific PaLM, and is de-signed to provide high-quality answers to medical questions.Med-PaLM is finetuned on PaLM using instruction prompttuning, a parameter-efficient method for aligning LLMs tonew domains using a few exemplars. Med-PaLM obtains veryencouraging results on many healthcare tasks, although it isstill inferior to human clinicians. Med-PaLM 2 improves Med-PaLM via med-domain finetuning and ensemble prompting[77]. Med-PaLM 2 scored up to 86.5% on the MedQAdataset (i.e., a benchmark combining six existing open ques-tion answering datasets spanning professional medical exams,research, and consumer queries), improving upon Med-PaLMby over 19% and setting a new state-of-the-art.C. Other Representative LLMsIn addition to the models discussed in the previous sub-sections, there are other popular LLMs which do not belongto those three model families, yet they have achieved greatperformance and have pushed the LLMs field forward. Webriefly describe these LLMs in this subsection.FLAN: In [78], Wei et al. explored a simple method forimproving the zero-shot learning abilities of language models.They showed that instruction tuning language models on acollection of datasets described via instructions substantiallyimproves zero-shot performance on unseen tasks. They takea 137B parameter pretrained language model and instructiontune it on over 60 NLP datasets verbalized via natural languageinstruction templates. They call this instruction-tuned modelFLAN. Fig 15 provides a comparison of instruction tuningwith pretrain–finetune and prompting.Fig. 15: comparison of instruction tuning with pre-train–finetune and prompting. Courtesy of [78].Gopher: In [79], Rae et al. presented an analysis ofTransformer-based language model performance across a widerange of model scales — from models with tens of millions ofparameters up to a 280 billion parameter model called Gopher.These models were evaluated on 152 diverse tasks, achievingstate-of-the-art performance across the majority. The numberof layers, the key/value size, and other hyper-parameters ofdifferent model sizes are shown in Fig 16.Fig. 16: Model architecture details of Gopher with differentnumber of parameters. Courtesy of [78].T0: In [80], Sanh et al. developed T0, a system for easilymapping any natural language tasks into a human-readableprompted form. They converted a large set of superviseddatasets, each with multiple prompts with diverse wording.These prompted datasets allow for benchmarking the abilityof a model to perform completely held-out tasks. Then, aT0 encoder-decoder model is developed to consume textualinputs and produces target responses. The model is trained ona multitask mixture of NLP datasets partitioned into differenttasks.ERNIE 3.0: In [81], Sun et al. proposed a unified frame-work named ERNIE 3.0 for pre-training large-scale knowledgeenhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tai-lored for both natural language understanding and generationtasks using zero-shot learning, few-shot learning or fine-tuning.They have trained ERNIE 3.0 with 10 billion parameterson a 4TB corpus consisting of plain texts and a large-scaleknowledge graph. Fig 17 illustrates the model architecture ofErnie 3.0.Fig. 17: High-level model architecture of ERNIE 3.0. Courtesyof [81].RETRO: In [82], Borgeaud et al. enhanced auto-regressivelanguage models by conditioning on document chunks re-trieved from a large corpus, based on local similarity with pre-ceding tokens. Using a 2-trillion-token database, the Retrieval-Enhanced Transformer (Retro) obtains comparable perfor-mance to GPT-3 and Jurassic-1 [83] on the Pile, despite using25% fewer parameters. As shown in Fig 18, Retro combinesa frozen Bert retriever, a differentiable encoder and a chunkedcross-attention mechanism to predict tokens based on an orderof magnitude more data than what is typically consumedduring training.GLaM: In [84], Du et al. proposed a family of LLMsnamed GLaM (Generalist Language Model), which use asparsely activated mixture-of-experts architecture to scale themodel capacity while also incurring substantially less trainingcost compared to dense variants. The largest GLaM has 1.2trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 andrequires half of the computation flops for inference, while stillachieving better overall zero, one and few-shot performanceacross 29 NLP tasks. Fig 19 shows the high-level architectureof GLAM.LaMDA: In [85], Thoppilan et al. presented LaMDA, afamily of Transformer-based neural language models special-ized for dialog, which have up to 137B parameters and arepre-trained on 1.56T words of public dialog data and web text.Fig. 18: Retro architecture. Left: simplified version where asequence of length n = 12 is split into l = 3 chunks of sizem = 4. For each chunk, we retrieve k = 2 neighbours of r =5 tokens each. The retrieval pathway is shown on top. Right:Details of the interactions in the CCA operator. Causality ismaintained as neighbours of the first chunk only affect the lasttoken of the first chunk and tokens from the second chunk.Courtesy of [82].Fig. 19: GLaM model architecture. Each MoE layer (thebottom block) is interleaved with a Transformer layer (theupper block). Courtesy of [84].They showed that fine-tuning with annotated data and enablingthe model to consult external knowledge sources can lead tosignificant improvements towards the two key challenges ofsafety and factual grounding.OPT: In [86], Zhang et al. presented Open Pre-trainedTransformers (OPT), a suite of decoder-only pre-trained trans-formers ranging from 125M to 175B parameters, which theyshare with researchers. The OPT models’ parameters areshown in 20Fig. 20: Different OPT Models’ architecture details. Courtesyof [86].Chinchilla: In [2], Hoffmann et al. investigated the optimalmodel size and number of tokens for training a transformerlanguage model under a given compute budget. By trainingover 400 language models ranging from 70 million to over16 billion parameters on 5 to 500 billion tokens, they foundthat for compute-optimal training, the model size and thenumber of training tokens should be scaled equally: for everydoubling of model size the number of training tokens shouldalso be doubled. They tested this hypothesis by training apredicted compute-optimal model, Chinchilla, that uses thesame compute budget as Gopher but with 70B parameters and4% more more data.Galactica: In [87], Taylor et al. introduced Galactica, alarge language model that can store, combine and reason aboutscientific knowledge. They trained on a large scientific corpusof papers, reference material, knowledge bases and many othersources. Galactica performed well on reasoning, outperformingChinchilla on mathematical MMLU by 41.3% to 35.7%, andPaLM 540B on MATH with a score of 20.4% versus 8.8%.CodeGen: In [88], Nijkamp et al. trained and releaseda family of large language models up to 16.1B parameters,called CODEGEN, on natural language and programminglanguage data, and open sourced the training library JAX-FORMER. They showed the utility of the trained model bydemonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval.They further investigated the multi-step paradigm for programsynthesis, where a single program is factorized into multi-ple prompts specifying sub-problems. They also constructedan open benchmark, Multi-Turn Programming Benchmark(MTPB), consisting of 115 diverse problem sets that arefactorized into multi-turn prompts.AlexaTM: In [89], Soltan et al. demonstrated that mul-tilingual large-scale sequence-to-sequence (seq2seq) models,pre-trained on a mixture of denoising and Causal LanguageModeling (CLM) tasks, are more efficient few-shot learnersthan decoder-only models on various task. They trained a20 billion parameter multilingual seq2seq model called AlexaTeacher Model (AlexaTM 20B) and showed that it achievesstate-of-the-art (SOTA) performance on 1-shot summarizationtasks, outperforming a much larger 540B PaLM decodermodel. AlexaTM consist of 46 encoder layers, 32 decoderlayers, 32 attention heads, and dmodel = 4096.Sparrow: In [90], Glaese et al. presented Sparrow, aninformation-seeking dialogue agent trained to be more helpful,correct, and harmless compared to prompted language modelbaselines. They used reinforcement learning from human feed-back to train their models with two new additions to helphuman raters judge agent behaviour. The high-level pipelineof Sparrow model is shown in Fig 21.Minerva: In [91], Lewkowycz et al. introduced Minerva,a large language model pretrained on general natural languagedata and further trained on technical content, to tackle previousLLM struggle with quantitative reasoning (such as solvingmathematics, science, and engineering problems).MoD: In [92], Tay et al. presented a generalized andunified perspective for self-supervision in NLP and show howdifferent pre-training objectives can be cast as one anotherand how interpolating between different objectives can beFig. 21: Sparrow pipeline relies on human participation tocontinually expand a training set. Courtesy of [90].effective. They proposed Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigmstogether. This framework is known as Unifying LanguageLearning (UL2). An overview of UL2 pretraining paradigmis shown in Fig 21.Fig. 22: An overview of UL2 pretraining paradigm. Courtesyof [92].BLOOM: In [93], Scao et al. presented BLOOM, a 176B-parameter open-access language model designed and builtthanks to a collaboration of hundreds of researchers. BLOOMis a decoder-only Transformer language model trained on theROOTS corpus, a dataset comprising hundreds of sources in46 natural and 13 programming languages (59 in total). Anoverview of BLOOM architecture is shown in Fig 23.Fig. 23: An overview of BLOOM architecture. Courtesy of[93].GLM: In [94], Zeng et al. introduced GLM-130B, abilingual (English and Chinese) pre-trained language modelwith 130 billion parameters. It was an attempt to open-sourcea 100B-scale model at least as good as GPT-3 (davinci) andunveil how models of such a scale can be successfully pre-trained.Pythia: In [95], Biderman et al. introduced Pythia, a suiteof 16 LLMs all trained on public data seen in the exact sameorder and ranging in size from 70M to 12B parameters. Weprovide public access to 154 checkpoints for each one of the16 models, alongside tools to download and reconstruct theirexact training dataloaders for further study.Orca: In [96], Mukherjee et al. develop Orca, a 13-billionparameter model that learns to imitate the reasoning processof large foundation models. Orca learns from rich signalsfrom GPT-4 including explanation traces; step-by-step thoughtprocesses; and other complex instructions, guided by teacherassistance from ChatGPT.StarCoder: In [97], Li et al. introduced StarCoder andStarCoderBase. They are 15.5B parameter models with 8Kcontext length, infilling capabilities and fast large-batch in-ference enabled by multi-query attention. StarCoderBase istrained on one trillion tokens sourced from The Stack, alarge collection of permissively licensed GitHub repositorieswith inspection tools and an opt-out process. They fine-tunedStarCoderBase on 35B Python tokens, resulting in the creationof StarCoder. They performed the most comprehensive evalu-ation of Code LLMs to date and showed that StarCoderBaseoutperforms every open Code LLM that supports multiple pro-gramming languages and matches or outperforms the OpenAIcode-cushman-001 model.KOSMOS: In [98], Huang et al. introduced KOSMOS-1,a Multimodal Large Language Model (MLLM) that can per-ceive general modalities, learn in context (i.e., few-shot), andfollow instructions (i.e. zero-shot). Specifically, they trainedKOSMOS-1 from scratch on web-scale multi-modal corpora,including arbitrarily interleaved text and images, image-captionpairs, and text data. Experimental results show that KOSMOS-1 achieves impressive performance on (i) language understand-ing, generation, and even OCR-free NLP (directly fed withdocument images), (ii) perception-language tasks, includingmultimodal dialogue, image captioning, visual question an-swering, and (iii) vision tasks, such as image recognition withdescriptions (specifying classification via text instructions).Gemini: In [99], Gemini team introduced a new family ofmultimodal models, that exhibit promising capabilities acrossimage, audio, video, and text understanding. Gemini familyincludes three versions: Ultra for highly-complex tasks, Profor enhanced performance and deployability at scale, and Nanofor on-device applications. Gemini architecture is built on topof Transformer decoders, and is trained to support 32k contextlength (via using efficient attention mechanisms).Some of the other popular LLM frameworks (or techniquesused for efficient developments of LLMs) includes Inner-Monologue [100], Megatron-Turing NLG [101], LongFormer[102], OPT-IML [103], MeTaLM [104], Dromedary [105],Palmyra [106], Camel [107], Yalm [108], MPT [109], ORCA-2 [110], Gorilla [67], PAL [111], Claude [112], CodeGen 2[113], Zephyr [114], Grok [115], Qwen [116], Mamba [30],Mixtral-8x7B [117], DocLLM [118], DeepSeek-Coder [119],FuseLLM-7B [120], TinyLlama-1.1B [121], LLaMA-Pro-8B[122].Fig 24 provides an overview of some of the most repre-sentative LLM frameworks, and the relevant works that havecontributed to the success of LLMs and helped to push thelimits of LLMs.III. HOW LLMS ARE BUILTIn this section, we first review the popular architecturesused for LLMs, and then discuss data and modeling techniquesranging from data preparation, tokenization, to pre-training,instruction tuning, and alignment.Once the model architecture is chosen, the major stepsinvolved in training an LLM includes: data preparation (col-lection, cleaning, deduping, etc.), tokenization, model pre-training (in a self-supervised learning fashion), instructiontuning, and alignment. We will explain each of them in aseparate subsection below. These steps are also illustrated inFig 25.A. Dominant LLM ArchitecturesThe most widely used LLM architectures are encoder-only,decoder-only, and encoder-decoder. Most of them are based onTransformer (as the building block). Therefore we also reviewthe Transformer architecture here.1) Transformer: in a ground-breaking work [44], Vaswaniet al. proposed the Transformer framework, which was orig-inally designed for effective parallel computing using GPUs.The heart of Transformer is the (self-)attention mechanism,which can capture long-term contextual information muchmore effectively using GPUs than the recurrence and convo-lution mechanisms. Fig 26 provides a high-level overview oftransformer work. In this section we provide an overview of themain elements and variants, see [44], [123] for more details.The Transformer language model architecture, originallyproposed for machine translation, consists of an encoder anda decoder. The encoder is composed of a stack of N = 6identical Transformer layers. Each layer has two sub-layers.The first one is a multi-head self-attention layer, and the otherone is a simple position-wise fully connected feed-forwardnetwork. The decoder is composed of a stack of 6 identicallayers. In addition to the two sub-layers in each encoder layer,the decoder has a third sub-layer, which performs multi-headattention over the output of the encoder stack. The attentionfunction can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, andoutput are all vectors. The output is computed as a weightedsum of the values, where the weight assigned to each valueis computed by a compatibility function of the query with thecorresponding key. Instead of performing a single attentionfunction with dmodel dimensional keys, values and queries,it is found to be beneficial to linearly project the queries,keys and values h with different, learned linear projections todk, dk and dv dimensions, respectively. Positional encoding isincorporated to fuse information about the relative or absoluteposition of the tokens in the sequence.Fig. 24: Timeline of some of the most representative LLM frameworks (so far). In addition to large language models with our#parameters threshold, we included a few representative works, which pushed the limits of language models, and paved the wayfor their success (e.g. vanilla Transformer, BERT, GPT-1), as well as some small language models. ♣ shows entities that servenot only as models but also as approaches. ♦ shows only approaches.2) Encoder-Only: For this family, at each stage, the atten-tion layers can access all the words in the initial sentence.The pre-training of these models usually consist of some-how corrupting a given sentence (for instance, by maskingrandom words in it) and tasking the model with finding orreconstructing the initial sentence. Encoder models are greatfor tasks requiring an understanding of the full sequence,such as sentence classification, named entity recognition, andextractive question answering. One prominent encoder onlymodel is BERT (Bidirectional Encoder Representations fromTransformers), proposed in [24].3) Decoder-Only: For these models, at each stage, for anyword, the attention layers can only access the words positionedbefore that in the sentence. These models are also sometimescalled auto-regressive models. The pretraining of these modelsis usually formulated as predicting the next word (or token)in the sequence. The decoder-only models are best suited fortasks involving text generation. GPT models are prominentexample of this model category.4) Encoder-Decoder: These models use both encoder anddecoder, and are sometimes called sequence-to-sequence mod-els. At each stage, the attention layers of the encoder can accessall the words in the initial sentence, whereas the attentionlayers of the decoder only accesses the words positioned beforea given word in the input. These models are usually pre-trained using the objectives of encoder or decoder models, butusually involve something a bit more complex. For instance,some models are pretrained by replacing random spans of text(that can contain several words) with a single mask specialword, and the objective is then to predict the text that thismask word replaces. Encoder-decoder models are best suitedfor tasks about generating new sentences conditioned on agiven input, such as summarization, translation, or generativequestion answering.B. Data CleaningData quality is crucial to the performance of languagemodels trained on them. Data cleaning techniques such asfiltering, deduplication, are shown to have a big impact onthe model performance.As an example, in Falcon40B [124], Penedo et al. showedthat properly filtered and deduplicated web data alone can leadto powerful models; even significantly outperforming modelsfrom the state-of-the-art trained on The Pile. Despite extensivefiltering, they were able to obtain five trillion tokens fromCommonCrawl. They also released an extract of 600 billiontokens from our REFINEDWEB dataset, and 1.3/7.5B param-eters language models trained on it. 27 shows the Refinementprocess of CommonCrawl data by this work.1) Data Filtering: Data filtering aims to enhance the qual-ity of training data and the effectiveness of the trained LLMs.Common data filtering techniques include:Removing Noise: refers to eliminating irrelevant or noisydata that might impact the model’s ability to generalize well.As an example, one can think of removing false informationfrom the training data, to lower the chance of model generatingfalse responses. Two mainstream approaches for quality filter-ing includes: classifier-based, and heuristic-based frameworks.How LLMs Are Built?Data CleaningTokenizationsBytePairEncodingWordPieceEncodingSentencePieceEncodingPositional EncodingAbsolute Positional EmbeddingsRelative Positional EmbeddingsRotary Position EmbeddingsRelative Positional BiasModel Pre-trainingMasked Language ModelingCausal Language ModelingNext Sentence PredictionMixture of ExpertsFine-tuning and Instruction TuningAlignmentSupervised learningReinforcement Learning from Human FeedbackDirect Preference OptimizationKahneman-Tversky OptimizationDecoding StrategiesGreedy SearchBeam SearchTop-k SamplingTop-p SamplingCost-Effective Training/Inference,Adaptation & CompressionOptimized TrainingZero Redundancy OptimizerReceptance Weighted Key ValueLow-Rank AdaptionKnowledge DistillationQuantizationData FilteringRemoving NoiseHandling OutliersAddressing ImbalancesText PreprocessingDeduplicationLLM ArchitecturesEncoder-OnlyDecoder-OnlyEncoder-Decoder...Supervised Fine-tuningGeneral Fine-tuningMulti-turn InstructionsInstruction FollowingFig. 25: This figure shows different components of LLMs.Fig. 26: High-level overview of transformer work. Courtesy of[44].Fig. 27: Subsequent stages of Macrodata Refinement removenearly 90% of the documents originally in CommonCrawl.Courtesy of [124].Handling Outliers: Identifying and handling outliers oranomalies in the data to prevent them from disproportionatelyinfluencing the model.Addressing Imbalances: Balancing the distribution ofclasses or categories in the dataset to avoid biases and ensurefair representation. This is specially useful for responsiblemodel training and evaluation.Text Preprocessing: Cleaning and standardizing text databy removing stop words, punctuation, or other elements thatmay not contribute significantly to the model’s learning.Dealing with Ambiguities: Resolving or excluding am-biguous or contradictory data that might confuse the modelduring training. This can help the model to provide moredefinite and reliable answers.2) Deduplication: De-duplication refers to the process ofremoving duplicate instances or repeated occurrences of thesame data in a dataset. Duplicate data points can introducebiases in the model training process and reduce the diversity, asthe model may learn from the same examples multiple times,potentially leading to overfitting on those particular instances.Some works [125] have shown that de-duplication improvesmodels’ ability to generalize to new, unseen data.The de-duplication process is particularly important whendealing with large datasets, as duplicates can unintentionallyinflate the importance of certain patterns or characteristics.This is especially relevant in NLP tasks, where diverse andrepresentative training data is crucial for building robust lan-guage models.The specific de-duplication method can vary based onthe nature of the data and the requirements of the particularlanguage model being trained. It may involve comparing entiredata points or specific features to identify and eliminate du-plicates. At the document level, existing works mainly rely onthe overlap ratio of high-level features (e.g. n-grams overlap)between documents to detect duplicate samples.C. TokenizationsTokenization referes to the process of converting a se-quence of text into smaller parts, known as tokens. Whilethe simplest tokenization tool simply chops text into tokensbased on white space, most tokenization tools rely on a worddictionary. However, out-of-vocabulary (OOV) is a problemin this case because the tokenizer only knows words in itsdictionary. To increase the coverage of dictionaries, populartokenizers used for LLMs are based on sub-words, which canbe combined to form a large number of words, including thewords unseen in training data or words in different languages.In what follows, we describe three popular tokenizers.1) BytePairEncoding: BytePairEncoding is originally atype of data compression algorithm that uses frequent patternsat byte level to compress the data. By definition, this algorithmmainly tries to keep the frequent words in their original formand break down ones that are not common. This simpleparadigm keeps the vocabulary not very large, but also goodenough to represent common words at the same time. Alsomorphological forms of the frequent words can be representedvery well if suffix or prefix is also commonly presented in thetraining data of the algorithm.2) WordPieceEncoding: This algorithm is mainly used forvery well-known models such as BERT and Electra. At thebeginning of training, the algorithm takes all the alphabet fromthe training data to make sure that nothing will be left as UNKor unknown from the training dataset. This case happens whenthe model is given an input that can not be tokenized by thetokenizer. It mostly happens in cases where some characters arenot tokenizable by it. Similar to BytePairEncoding, it tries tomaximize the likelihood of putting all the tokens in vocabularybased on their frequency.3) SentencePieceEncoding: Although both tokenizers de-scribed before are strong and have many advantages comparedto white-space tokenization, they still take assumption ofwords being always separated by white-space as granted. Thisassumption is not always true, in fact in some languages, wordscan be corrupted by many noisy elements such as unwantedspaces or even invented words. SentencePieceEncoding triesto address this issue.D. Positional Encoding1) Absolute Positional Embeddings: (APE) [44] has beenused in the original Transformer model to preserve the infor-mation of sequence order. Therefore, the positional informationof words is added to the input embeddings at the bottom ofboth the encoder and decoder stacks. There are various optionsfor positional encodings, either learned or fixed. In the vanillaTransformer, sine and cosine functions are employed for thispurpose. The main drawback of using APE in Transformersis the restriction to a certain number of tokens. Additionally,APE fails to account for the relative distances between tokens.2) Relative Positional Embeddings: (RPE) [126] involvesextending self-attention to take into account the pairwise linksbetween input elements. RPE is added to the model at twolevels: first as an additional component to the keys, andsubsequently as a sub-component of the values matrix. Thisapproach looks at the input as a fully-connected graph withlabels and directed edges. In the case of linear sequences, edgescan capture information about the relative position differencesbetween input elements. A clipping distance, represented as k2 ≤ k ≤ n − 4, specifies the maximum limit on relative lo-cations. This allows the model to make reasonable predictionsfor sequence lengths that are not part of the training data.3) Rotary Position Embeddings: Rotary Positional Em-bedding (RoPE) [127] tackles problems with existing ap-proaches. Learned absolute positional encodings can lack gen-eralizability and meaningfulness, particularly when sentencesare short. Moreover, current methods like T5’s positionalembedding face challenges with constructing a full attentionmatrix between positions. RoPE uses a rotation matrix toencode the absolute position of words and simultaneously in-cludes explicit relative position details in self-attention. RoPEbrings useful features like flexibility with sentence lengths, adecrease in word dependency as relative distances increase,and the ability to improve linear self-attention with relativeposition encoding. GPT-NeoX-20B, PaLM, CODEGEN, andLLaMA are among models that take advantage of RoPE intheir architectures.4) Relative Positional Bias: The concept behind this typeof positional embedding is to facilitate extrapolation duringinference for sequences longer than those encountered in train-ing. In [128] Press et al. proposed Attention with Linear Biases(ALiBi). Instead of simply adding positional embeddings toword embeddings, they introduced a bias to the attention scoresof query-key pairs, imposing a penalty proportional to theirdistance. In the BLOOM model, ALiBi is leveraged.E. Model Pre-trainingPre-training is the very first step in large language modeltraining pipeline, and it helps LLMs to acquire fundamentallanguage understanding capabilities, which can be useful in awide range of language related tasks. During pre-training, theLLM is trained on a massive amount of (usually) unlabeledtexts, usually in a self-supervised manner. There are differentapproaches used for pre-training like next sentence prediction[24], two most common ones include, next token prediction(autoregressive language modeling), and masked languagemodeling.In Autoregressive Language Modeling framework, givena sequence of n tokens x1, ..., xn, the model tries to predictnext token xn+1 (and sometimes next sequence of tokens) inan auto-regressive fashion. One popular loss function in thiscase is the log-likelihood of predicted tokens as shown in Eq2LALM (x) =N∑i=1p(xi+n|xi, ..., xi+n−1) (1)Given the auto-regressive nature of this framework, thedecoder-only models are naturally better suited to learn howto accomplish these task.In Masked Language Modeling, some words are maskedin a sequence and the model is trained to predict the maskedwords based on the surrounding context. Sometimes peoplerefer to this approach as denoising autoencoding, too. If wedenote the masked/corrupted samples in the sequence x, as x̃,then the training objective of this approach can be written as:LMLM (x) =N∑i=1p(x̃|x\\x̃) (2)And more recently, Mixture of Experts (MoE) [130],[131] have become very popular in LLM space too. MoEsenable models to be pre-trained with much less compute,which means one can dramatically scale up the model ordataset size with the same compute budget as a dense model.MoE consists of two main elements: Sparse MoE layers,which are used instead of dense feed-forward network (FFN)layers, and have a certain number of “experts” (e.g. 8), inwhich each expert is a neural network. In practice, the expertsare FFNs, but they can also be more complex networks. A gatenetwork or router, that determines which tokens are sent towhich expert. It is worth noting that, one can send a tokento more than one expert. How to route a token to an expertis one of the big decisions when working with MoEs - therouter is composed of learned parameters and is pretrained atthe same time as the rest of the network. Fig 29 provides anillustration of a Switch Transformer encoder block, which areused in MoE.F. Fine-tuning and Instruction TuningEarly language models such as BERT trained using self-supervision as explained in section III-E were not able toperform specific tasks. In order for the foundation model to beuseful it needed to be fine-tuned to a specific task with labeleddata (so-called supervised fine-tuning or SFT for short). Forexample, in the original BERT paper [24], the model was fine-tuned to 11 different tasks. While more recent LLMs no longerrequire fine-tuning to be used, they can still benefit from taskor data-specific fine-tuning. For example, OpenAI reports thatthe much smaller GPT-3.5 Turbo model can outperform GPT-4when fine-tuned with task specific data 2.Fine-tuning does not need to be performed to a singletask though, and there are different approaches to multi-taskfine-tuning (see e.g. Mahabi et al. [132]). Fine-tuning to oneor more tasks is known to improve results and reduce thecomplexity of prompt engineering, and it can serve as an2https://platform.openai.com/docs/guides/fine-tuning(a) Absolute Positional Embeddings [129] (b) Relative Positional Embeddings(c) Rotary Positional Embedding [127] (d) Relative Positional Bias [128]Fig. 28: Various positional encodings are employed in LLMs.Fig. 29: : Illustration of a Switch Transformer encoder block.They replaced the dense feed forward network (FFN) layerpresent in the Transformer with a sparse Switch FFN layer(light blue). . Courtesy of [131].alternative to retrieval augmented generation. Furthermore,there are other reasons why it might be advisable to fine-tune.For example, one might want to fine-tune to expose the modelto new or proprietary data that it has not been exposed toduring pre-training.An important reason to fine-tune LLMs is to align theresponses to the expectations humans will have when providinginstructions through prompts. This is the so-called instructiontuning [133]. We dive into the details of how to designand engineer prompts in section IV-B, but in the contextof instruction tuning, it is important to understand that theinstruction is a prompt that specifies the task that the LLMshould accomplish. Instruction tuning datasets such as NaturalInstructions [134] include not only the task definition but othercomponents such as positive/negative examples or things toavoid.The specific approach and instruction datasets used toinstruction-tune an LLM varies, but, generally speaking, in-struction tuned models outperform their original foundationmodels they are based on. For example, InstructGPT [59]outperforms GPT-3 on most benchmarks. The same is truefor Alpaca [62] when compared to LLaMA.Self-Instruct [135], proposed by Wang et al. is also apopular approach along this line, in which they introduced aframework for improving the instruction-following capabilitiesof pre-trained language models by bootstrapping their owngenerations. Their pipeline generates instructions, input, andoutput samples from a language model, then filters invalid orsimilar ones before using them to fine tune the original model.G. AlignmentAI Alignment is the process of steering AI systems towardshuman goals, preferences, and principles. LLMs, pre-trainedfor word prediction, often exhibit unintended behaviors. Forexample, they might generate contents that are toxic, harmful,misleading and biased.Instruction tuning, discussed above, gets LLMs a stepcloser to being aligned. However, in many cases, it is importantto include further steps to improve the alignment of the modeland avoid unintended behaviors 3. We review the most popular3According to very recent research by Ethayarajh et al. [136], furtheralignment besides SFT mainly improves models of at least 7B parameters.For smaller models, SFT is sufficient.approaches to alignment in this subsection.RLHF (reinforcement learning from human feedback) andRLAIF (reinforcement learning from AI feedback) are twopopular approaches. RLHF uses a reward model to learnalignment from human feedback. This reward model, afterbeing tuned, is able to rate different outputs and score themaccording to their alignment preferences given by humans. Thereward model gives feedback to the original LLM and thisfeedback is used to tune the LLM further [137]. Reinforcementlearning from AI feedback on the other hand, directly connectsa pretrained and well-aligned model to the LLM and helps itto learn from larger and more aligned models [138].In another recent work (known as DPO) [139], Rafailovet al. discussed that RLHF is a complex and often unstableprocedure, and tried to address this with a new approach. Theyleveraged a mapping between reward functions and optimalpolicies to show that this constrained reward maximizationproblem can be optimized exactly with a single stage of policytraining, essentially solving a classification problem on thehuman preference data. The resulting algorithm, which theycalled Direct Preference Optimization (DPO), is stable, per-formant, and computationally lightweight, eliminating the needfor fitting a reward model, sampling from the LM during fine-tuning, or performing significant hyperparameter tuning. Theyobserved that fine-tuning with DPO exceeds RLHF’s ability tocontrol sentiment of generations and improves response qualityin summarization. Fig 30 shows the high-level comparisonbetween DPO vs RLHF.Fig. 30: DPO optimizes for human preferences while avoidingreinforcement learning. Existing methods for fine-tuning lan-guage models with human feedback first fit a reward modelto a dataset of prompts and human preferences over pairs ofresponses, and then use RL to find a policy that maximizesthe learned reward. In contrast, DPO directly optimizes forthe policy best satisfying the preferences with a simple classi-fication objective, without an explicit reward function or RL.Courtesy of [139].Even more recently Ethayarajh et al. proposed a new align-ment approach called the Kahneman-Tversky Optimization(KTO) [136]. Unlike existing state-of-the-art approaches, KTOdoes not require paired preference data (x, yw, yl), and itonly needs (x,y) and knowledge of whether y is desirable orundesirable. KTO-aligned models are shown to be good orbetter than DPO-aligned models at scales from 1B to 30B,despite not using paired preferences. KTO is also far easier touse in the real world than preference optimization methods, asthe kind of data it needs is far more abundant. As an example,every retail company has a lot of customer interaction data andwhether that interaction was successful (e.g., purchase made)or unsuccessful (e.g., no purchase made). However, They havelittle to no counterfactual data (i.e., what would have madean unsuccessful customer interaction yl into a successful oneyw). Fig 31 shows a high-level comparison between KTO andother alignment approaches discussed above.Fig. 31: LLM alignment involves supervised finetuning fol-lowed by optimizing a human-centered loss (HALO). How-ever, the paired preferences that existing approaches need arehard-to-obtain. In contrast, KTO uses a far more abundantkind of data, making it much easier to use in the real world.Courtesy of [136].H. Decoding StrategiesDecoding refers to the process of text generation using pre-trained LLMs. Given an input prompt, the tokenizer translateseach token in the input text into a corresponding token ID.Then, the language model uses these token IDs as input andpredicts the next most likely token (or a sequence of tokens).Finally, the model generates logits, which are converted toprobabilities using a softmax function. Different decodingstrategies have been proposed. Some of the most popular onesare greedy search, beam search, as well as different sampletechniques such as top-K, top-P (Nucleus sampling).1) Greedy Search: Greedy search takes the most probabletoken at each step as the next token in the sequence, discardingall other potential options. As you can imagine, this is a simpleapproach and can loose a lot of temporal consistency andcoherency. It only considers the most probable token at eachstep, without considering the overall effect on the sequence.This property makes it fast, but it also means that it can missout on better sequences that might have appeared with slightlyless probable next tokens.2) Beam Search: Unlike greedy search that only considersthe next most probable token, beam search takes into accountthe N most likely tokens, where N denotes the number ofbeams. This procedure is repeated until a predefined maxi-mum sequence length is reached or an end-of-sequence tokenappears. At this point, the sequence of tokens (AKA “beam”)with the highest overall score is chosen as the output. Forexample for beam size of 2 and maximum length of 5,the beam search needs to keep track of 25 = 32 possiblesequences. So it is more computationally intensive than greedysearch.3) Top-k Sampling: Top-k sampling is a technique thatuses the probability distribution generated by the languagemodel to select a token randomly from the k most likelyoptions.Suppose we have 6 tokens (A, B, C, D, E, F) and k=2,and P(A)= 30%, and P(B)= 20%, P(C)= P(D)= P(E)= P(F)=12.5%. In top-k sampling, tokens C, D, E, F are disregarded,and the model outputs A 60% of the time, and B, 40% ofthe time. This approach ensures that we prioritize the mostprobable tokens while introducing an element of randomnessin the selection process.The randomness is usually introduced via the concept oftemperature. The temperature T is a parameter that ranges from0 to 1, which affects the probabilities generated by the softmaxfunction, making the most likely tokens more influential. Inpractice, it simply consists of dividing the input logits bytemperature value:softmax(xi) =exi/T∑j exj/T(3)A low temperature setting significantly alters the proba-bility distribution (and is commonly used in text generationto control the level of “creativity” in the generated output),while a large temperature prioritizes the tokens with higherprobabilities. Top-k is a creative way of sampling, and can beused along with beam search. The sequence chosen by top-k sampling may not be the sequence with highest probabilityin beam search. But it’s important to remember that highestscores do not always lead to more realistic or meaningfulsequences.4) Top-p Sampling: Top-p sampling, also known as Nu-cleus sampling, takes a slightly different approach from top-ksampling. Instead of selecting the top k most probable tokens,nucleus sampling chooses a cutoff value p such that the sum ofthe probabilities of the selected tokens exceeds p. This formsa “nucleus” of tokens from which to randomly choose the nexttoken. In other words, in top-p sampling the language modelexamines the most probable tokens in descending order andkeeps adding them to the list until the sum of probabilitiessurpasses the threshold p. As you can imagine, this could bebetter specially for scenarios in which top-k tokens do not havea large probability mass. Unlike top-k sampling, the numberof tokens included in the nucleus sampling is not fixed. Thisvariability often results in a more diverse and creative output,making nucleus sampling popular for text generation relatedtasks.I. Cost-Effective Training/Inference/Adaptation/CompressionIn this part, we review some of the popular approachesused for more cost-friendly (and compute-friendly) trainingand usage of LLMs.1) Optimized Training: There are many frameworks de-veloped for optimized training of LLMs, here we introducesome of the prominent ones.ZeRO: In [140], Rajbhandari et al. developed a novelsolution, Zero Redundancy Optimizer (ZeRO), to optimizememory, vastly improving training speed of LLMs whileincreasing the model size that can be efficiently trained. ZeROeliminates memory redundancies in data- and model-paralleltraining while retaining low communication volume and highcomputational granularity, allowing one to scale the modelsize proportional to the number of devices with sustained highefficiency.RWKV: In [141], Peng et al. proposed a novel modelarchitecture, Receptance Weighted Key Value (RWKV), thatcombines the efficient parallelizable training of Transformerswith the efficient inference of RNNs. Their approach leveragesa linear attention mechanism and allows them to formulate themodel as either a Transformer or an RNN, which parallelizescomputations during training and maintains constant compu-tational and memory complexity during inference, leading tothe first non-transformer architecture to be scaled to tens ofbillions of parameters. RWKV architecture is shown in Fig32. The Time Complexity comparison of RWKV with differentFig. 32: RWKV architecture. Courtesy of [141].Transformers are provided in Fig 33.Fig. 33: Time Complexity comparison of RWKV with differentTransformers. Here T denotes the sequence length, d thefeature dimension, and c is MEGA’s chunk size of quadraticattention. Courtesy of [141].2) Low-Rank Adaption (LoRA): Low-Rank Adaptation isa popular and lightweight training technique that significantlyreduces the number of trainable parameters, and is basedon a crucial insight that the difference between the fine-tuned weights for a specialized task and the initial pre-trainedweights often exhibits “low intrinsic rank” - meaning thatit can be approximated well by a low rank matrix [142].Fig. 34: An illustration of LoRA reparametrizan. Only A andB trained during this process. Courtesy of [142].Training with LoRA is much faster, memory-efficient, andproduces smaller model weights (a few hundred MBs), that areeasier to store and share. One property of low-rank matricesis that they can be represented as the product of two smallermatrices. This realization leads to the hypothesis that this deltabetween fine-tuned weights and initial pre-trained weights canbe represented as the matrix product of two much smallermatrices. By focusing on updating these two smaller matricesrather than the entire original weight matrix, computationalefficiency can be substantially improved.Specifically, for a pre-trained weight matrix W0 ∈ Rd×k,LoRA constrains its update by representing the latter witha low-rank decomposition W0 + ∆W = W0 + BA, whereB ∈ Rd×r , A ∈ Rr×k, and the rank r ≪ min(d, k). Duringtraining, W0 is frozen and does not receive gradient updates,while A and B contain trainable parameters. It is worthmentioning that both W0 and ∆W = BA are multiplied withthe same input, and their respective output vectors are summedcoordinate-wise. For h = W0x, their modified forward passyields: h = W0x+∆Wx = W0x+ BAx. Usually a randomGaussian initialization is used for A, and zero initializationfor B, so ∆W = BA is zero at the beginning of training.They then scale ∆Wx by αr, where α is a constant in r. Thisreparametrization is illustrated in Figure 34It is worth mentioning that LoRA can be applied to any asubset of weight matrices in a neural network to reduce thenumber of trainable parameters. In the Transformer architec-ture, there are four weight matrices in the self-attention module(Wq , Wk, Wv , Wo), and two in the MLP module. Most ofthe time, LoRA is focused on adapting the attention weightsonly for downstream tasks, and freezes the MLP modules, sothey are not trained in downstream tasks both for simplicityand parameter-efficiency.3) Knowledge Distillation: Knowledge distillation is theprocess of learning from a larger model [143]. Earlier days ofbest-performing models release have proven that this approachis very useful even if it is used in an API distillation approach.It is also referred to as an approach to distill the knowledge ofnot a single model but in fact multiple models into a smallerone. Creating smaller models by this approach yields smallermodel sizes that can be used even on edge devices. Knowledgedistillation as shown in Fig 35, illustrates a general setup ofthis training scheme.Fig. 35: A generic knowledge distillation framework withstudent and teacher (Courtesy of [144]).Knowledge can be transferred by different forms of learn-ing: response distillation, feature distillation, and API distilla-tion. Response distillation is concerned only with the outputsof the teacher model and tries to teach the student modelhow to exactly or at least similarly perform (in the sense ofprediction) as the teacher. Feature distillation not only usesthe last layer but also intermediate layers as well to create abetter inner representation for the student model. This helps thesmaller model to have a similar representation as the teachermodel.API distillation is the process of using an API (typicallyfrom an LLM provider such as OpenAI) to train smallermodels. In the case of LLMs, it is used to train the modelfrom the direct output of the larger model which makes it verysimilar to response distillation. Many concerns are raised bythis type of distillation because in cases where the model itselfis not openly available, a (usually) paid API is exposed for endusers. On the other hand, while users pay for each call, how touse the predictions is limited, for example, OpenAI prohibitsusage of its API to create LLMs that later will be used tocompete with it. The main value in such case is training data.4) Quantization: deep learning in its core, is a set ofmathematical functions applied to matrices, with a specificprecision for model weights. Reducing the precision of theweights can be used to reduce the size of the model and alsomake it faster. As an example, Float-32 operations comparedto Int-8 operations are slower. This process, which is calledquantization, can be applied in different phases. Main ap-proaches for model quantization can be categorized as: posttraining quantization and quantization-aware training. Post-training quantization is concerned with quantized trained mod-els in two well-known methods: dynamic and static. Dynamicpost-training quantization computes the range of quantizationon the runtime and is slower compared to static. Quantization-aware training adds quantization criteria into training, anda quantized model is trained and optimized during trainingprocess. This approach ensures that the end model will havegood performance and also does not need to be quantized aftertraining.IV. HOW LLMS ARE USED AND AUGMENTEDOnce the LLMs are trained, we can use them to generatedesired outputs for a variety of tasks. LLMs can be useddirectly through basic prompting. However, in order to exploittheir full potential or to address some of the shortcomings,we need to augment the models through some external means.In this section we first provide a brief overview of the mainshortcoming of LLMs, with a deeper look at the issue ofhallucination. We then describe how prompting and some aug-mentation approaches can not only address those limitationsbut also be used to augment the capabilities of LLMs goingas far as turning an LLM into a full-blown AI agent with theability to interface with the external world.A. LLM limitationsIt is important to remember that LLMs are trained to predicta token. While fine-tuning and alignment improves their per-formance and adds different dimensions to their abilities, thereare still some important limitations that come up, particularlyif they are used naively. Some of them include the following:• They don’t have state/memory. LLMs on their owncannot remember even what was sent to them in theprevious prompt. That is an important limitation formany of the uses cases that require some form of state.• They are stochastic/probabilistic. If you send the sameprompt to an LLM several times, you are likely to getdifferent responses. While there are parameters, andin particular the temperature, to limit the variabilityin the response, this is an inherent property of theirtraining that can create issues.• They have stale information and, on their own, don’thave access to external data. An LLM on its own doesnot even know about the current time or day and doesnot have access to any information that was not presentin its training set.• They are generally very large. This means that manycostly GPU machines are needed for training andserving. In some cases, largest models have poorSLAs, particularly in terms of latency.• They hallucinate. LLMs do not have a notion of”truth” and they have usually been trained on a mixof good and bad content. They can produce veryplausible but untruthful answers.While the previous limitations can all become importantfor some applications, it is worth for us to dive a bit into thelast one, hallucinations, since it has gathered a lot of interestover the past few months and it has also sparked many of theprompt approaches and LLM augmentation methods we willlater describe.Hallucination: In the realm of Large Language Models(LLMs), the phenomenon of ”hallucinations” has garneredsignificant attention. Defined in the literature, notably in the”Survey of Hallucination in Natural Language Generation”paper [145], hallucination in an LLM is characterized as”the generation of content that is nonsensical or unfaithfulto the provided source.” This terminology, although rooted inpsychological parlance, has been appropriated within the fieldof artificial intelligence.Hallucinations in LLMs can be broadly categorized intotwo types:1) Intrinsic Hallucinations: These directly conflict withthe source material, introducing factual inaccuraciesor logical inconsistencies.2) Extrinsic Hallucinations: These, while not contra-dicting, are unverifiable against the source, encom-passing speculative or unconfirmable elements.The definition of ’source’ in LLM contexts varies with thetask. In dialogue-based tasks, it refers to ’world knowledge’,whereas in text summarization, it pertains to the input textitself. This distinction plays a crucial role in evaluating andinterpreting hallucinations. The impact of hallucinations is alsohighly context-dependent. For instance, in creative endeavorslike poem writing, hallucinations might be deemed acceptableor even beneficial.LLMs, trained on diverse datasets including the internet,books, and Wikipedia, generate text based on probabilisticmodels without an inherent understanding of truth or falsity.Recent advancements like instruct tuning and ReinforcementLearning from Human Feedback (RLHF) have attempted tosteer LLMs towards more factual outputs, but the fundamentalprobabilistic nature and its inherent limitations remain. Arecent study, “Sources of Hallucination by Large LanguageModels on Inference Tasks” [146], highlights two key aspectscontributing to hallucinations in LLMs: the veracity prior andthe relative frequency heuristic, underscoring the complexitiesinherent in LLM training and output generation.Effective automated measurement of hallucinations inLLMs requires a combination of statistical and model-basedmetrics.Statistical Metrics:• Metrics like ROUGE [147] and BLEU [148] are com-mon for assessing text similarity, focusing on intrinsichallucinations.• Advanced metrics such as PARENT [149], PARENT-T [150], and Knowledge F1 [151] are utilized whenstructured knowledge sources are available. Thesemetrics, while effective, have limitations in capturingsyntactic and semantic nuances.Model-Based Metrics:• IE-Based Metrics: Utilize Information Extractionmodels to simplify knowledge into relational tuples,then compare these with the source.• QA-Based Metrics: Assess the overlap between gen-erated content and the source through a question-answering framework (see [152]).• NLI-Based Metrics: Use Natural Language Inferencedatasets to evaluate the truthfulness of a generatedhypothesis based on a given premise (see [153]).• Faithfulness Classification Metrics: Offer a refinedassessment by creating task-specific datasets for anuanced evaluation (see [154]).Despite advances in automated metrics, human judgmentremains a vital piece. It typically involves two methodologies:B) Augmenting LLMs throughexternal knowledge - RAGHow LLMs Are Used and AugmentedC) Using External ToolsD) LLM AgentsFunctionality of an LLM-based agentTool Access and UtilizationDecision MakingPrompt engineering techniques for agentsReasoning without ObservationReason and ActDialog-Enabled Resolving Agentsa) RAG-aware prompting techniquesa) Tool-aware prompting techniquesA) LLM limitationsHallucinationHallucination QuantificationAutomated metricsHuman judgmentStatistical MetricsModel-Based MetricsScoringComparative AnalysisIE-Based MetricsQA-Based MetricsNLI-Based MetricsB) Using LLMs Prompt Design and Engineering1) Chain of ThoughtZero-Shot CoTManual CoT5) Expert Prompting6) Chains2) Tree of Thought 7) RailsTopical RailsFact-Checking RailsJailbreaking Rails8) Automatic Prompt EngineeringPrompt GenerationPrompt ScoringRefinement and Iteration3) Self-Consistency4) ReflectionComponents of a RAGRetrieval Generation AugmentationRAG ToolsLangChain LlamaIndexHayStackMeltanoCohere CoralFlowise AIFig. 36: How LLMs Are Used and Augmented.1) Scoring: Human evaluators rate the level of halluci-nation within a predefined scale.2) Comparative Analysis: Evaluators compare gener-ated content against baseline or ground-truth refer-ences, adding an essential layer of subjective assess-ment.FactScore [155] is a recent example of a metric that can beused both for human and model-based evaluation. The metricbreaks an LLM generation into “atomic facts”. The final scoreis computed as the sum of the accuracy of each atomic fact,giving each of them equal weight. Accuracy is a binary numberthat simply states whether the atomic fact is supported by thesource. The authors implement different automation strategiesthat use LLMs to estimate this metric.Finally, mitigating hallucinations in LLMs is a multifacetedchallenge, requiring tailored strategies to suit various applica-tions. Those include:• Product Design and User Interaction Strategies suchas use case design, structuring the input/output, orproviding mechanisms for user feedback.• Data Management and Continuous Improvement.Maintaining and analyzing a tracking set of hallucina-tions is essential for ongoing model improvement.• Prompt Engineering and Metaprompt Design. Manyof the advanced prompt techniques described in IV-Bsuch as Retrieval Augmented Generation directly ad-dress hallucination risks.• Model Selection and Configuration for HallucinationMitigation. For exemple, larger models with lowertemperature settings usually perform better. Also,techniques such as RLHF or domain-sepcific fine-tuning can mitigate hallucination risks.B. Using LLMs: Prompt Design and EngineeringA prompt in generative AI models is the textual inputprovided by users to guide the model’s output. This couldrange from simple questions to detailed descriptions or specifictasks. Prompts generally consist of instructions, questions,input data, and examples. In practice, to elicit a desiredresponse from an AI model, a prompt must contain eitherinstructions or questions, with other elements being optional.Advanced prompts involve more complex structures, such as”chain of thought” prompting, where the model is guided tofollow a logical reasoning process to arrive at an answer.Prompt engineering is a rapidly evolving discipline thatshapes the interactions and outputs of LLMs and other gen-erative AI models. The essence of prompt engineering lies incrafting the optimal prompt to achieve a specific goal witha generative model. This process is not only about instructingthe model but also involves some understanding of the model’scapabilities and limitations, and the context within which itoperates.Prompt engineering transcends the mere construction ofprompts; it requires a blend of domain knowledge, understand-ing of the AI model, and a methodical approach to tailorprompts for different contexts. This might involve creatingtemplates that can be programmatically modified based on agiven dataset or context. For example, generating personalizedresponses based on user data might use a template that isdynamically filled with relevant user information.Furthermore, prompt engineering is an iterative and ex-ploratory process, akin to traditional machine learning prac-tices such as model evaluation or hyperparameter tuning. Therapid growth of this field suggests its potential to revolutionizecertain aspects of machine learning, moving beyond traditionalmethods like feature or architecture engineering. On the otherhand, traditional engineering practices such as version con-trol and regression testing need to be adapted to this newparadigm just like they were adapted to other machine learningapproaches [156].In the following paragraphs we detail some of the mostinteresting and popular prompt engineering approaches.1) Chain of Thought (CoT): The Chain of Thought (CoT)technique, initially described in the paper “Chain-of-ThoughtPrompting Elicits Reasoning in Large Language Models”[34]by Google researchers, represents a pivotal advancement inprompt engineering for Large Language Models (LLMs).This approach hinges on the understanding that LLMs, whileproficient in token prediction, are not inherently designed forexplicit reasoning. CoT addresses this by guiding the modelthrough essential reasoning steps.CoT is based on making the implicit reasoning process ofLLMs explicit. By outlining the steps required for reasoning,the model is directed closer to a logical and reasoned output,especially in scenarios demanding more than simple informa-tion retrieval or pattern recognition.CoT prompting manifests in two primary forms:1) Zero-Shot CoT: This form involves instructing theLLM to “think step by step”, prompting it to de-construct the problem and articulate each stage ofreasoning.2) Manual CoT: A more complex variant, it requiresproviding step-by-step reasoning examples as tem-plates for the model. While yielding more effectiveresults, it poses challenges in scalability and mainte-nance.Manual CoT is more effective than zero-shot. However,the effectiveness of this example-based CoT depends on thechoice of diverse examples, and constructing prompts withsuch examples of step by step reasoning by hand is hard anderror prone. That is where automatic CoT [157] comes intoplay.2) Tree of Thought (ToT): The Tree of Thought (ToT)[158] prompting technique is inspired by the concept ofconsidering various alternative solutions or thought processesbefore converging on the most plausible one. ToT is basedon the idea of branching out into multiple ”thought trees”where each branch represents a different line of reasoning.This method allows the LLM to explore various possibilitiesand hypotheses, much like human cognitive processes wheremultiple scenarios are considered before determining the mostlikely one.A critical aspect of ToT is the evaluation of these reasoningpaths. As the LLM generates different branches of thought,each is assessed for its validity and relevance to the query.This process involves real-time analysis and comparison ofthe branches, leading to a selection of the most coherent andlogical outcome.ToT is particularly useful in complex problem-solvingscenarios where a single line of reasoning might not suffice.It allows LLMs to mimic a more human-like problem-solvingapproach, considering a range of possibilities before arrivingat a conclusion. This technique enhances the model’s abilityto handle ambiguity, complexity, and nuanced tasks, making ita valuable tool in advanced AI applications.3) Self-Consistency: Self-Consistency [159] utilizes anensemble-based method, where the LLM is prompted to gen-erate multiple responses to the same query. The consistencyamong these responses serves as an indicator of their accuracyand reliability.The Self-Consistency approach is grounded in the principlethat if an LLM generates multiple, similar responses to thesame prompt, it is more likely that the response is accurate.This method involves asking the LLM to tackle a query mul-tiple times, each time analyzing the response for consistency.This technique is especially useful in scenarios where factualaccuracy and precision are paramount.The consistency of responses can be measured using vari-ous methods. One common approach is to analyze the overlapin the content of the responses. Other methods may includecomparing the semantic similarity of responses or employingmore sophisticated techniques like BERT-scores or n-gramoverlaps. These measures help in quantifying the level ofagreement among the responses generated by the LLM.Self-Consistency has significant applications in fieldswhere the veracity of information is critical. It is particularlyrelevant in scenarios like fact-checking, where ensuring theaccuracy of information provided by AI models is essential.By employing this technique, prompt engineers can enhancethe trustworthiness of LLMs, making them more reliable fortasks that require high levels of factual accuracy.4) Reflection: Reflection [160] involves prompting LLMsto assess and potentially revise their own outputs based onreasoning about the correctness and coherence of their re-sponses. The concept of Reflection centers on the ability ofLLMs to engage in a form of self-evaluation. After generatingan initial response, the model is prompted to reflect on itsown output, considering factors like factual accuracy, logicalconsistency, and relevance. This introspective process can leadto the generation of revised or improved responses.A key aspect of Reflection is the LLM’s capacity forself-editing. By evaluating its initial response, the model canidentify potential errors or areas of improvement. This iterativeprocess of generation, reflection, and revision enables the LLMto refine its output, enhancing the overall quality and reliabilityof its responses.5) Expert Prompting: Expert Prompting [161] enhances thecapabilities of Large Language Models (LLMs) by simulatingthe responses of experts in various fields. This method involvesprompting the LLMs to assume the role of an expert and re-spond accordingly, providing high-quality, informed answers.A key strategy within Expert Prompting is the multi-expertapproach. The LLM is prompted to consider responses frommultiple expert perspectives, which are then synthesized toform a comprehensive and well-rounded answer. This tech-nique not only enhances the depth of the response but alsoincorporates a range of viewpoints, reflecting a more holisticunderstanding of the subject matter.6) Chains: Chains refer to the method of linking multiplecomponents in a sequence to handle complex tasks with LargeLanguage Models (LLMs). This approach involves creating aseries of interconnected steps or processes, each contributingto the final outcome. The concept of Chains is based onthe idea of constructing a workflow where different stagesor components are sequentially arranged. Each component ina Chain performs a specific function, and the output of oneserves as the input for the next. This end-to-end arrangementallows for more complex and nuanced processing, as eachstage can be tailored to handle a specific aspect of the task.Chains can vary in complexity and structure, depending onthe requirements. In “PromptChainer: Chaining Large Lan-guage Model Prompts through Visual Programming” [162],the authors not only describe the main challenges in designingchains, but also describe a visual tool to support those tasks.7) Rails: Rails in advanced prompt engineering refer toa method of guiding and controlling the output of LargeLanguage Models (LLMs) through predefined rules or tem-plates. This approach is designed to ensure that the model’sresponses adhere to certain standards or criteria, enhancing therelevance, safety, and accuracy of the output. The concept ofRails involves setting up a framework or a set of guidelinesthat the LLM must follow while generating responses. Theseguidelines are typically defined using a modeling language ortemplates known as Canonical Forms, which standardize theway natural language sentences are structured and delivered.Rails can be designed for various purposes, depending onthe specific needs of the application:• Topical Rails: Ensure that the LLM sticks to aparticular topic or domain.• Fact-Checking Rails: Aimed at minimizing the gen-eration of false or misleading information.• Jailbreaking Rails: Prevent the LLM from generatingresponses that attempt to bypass its own operationalconstraints or guidelines.8) Automatic Prompt Engineering (APE): AutomaticPrompt Engineering (APE) [163] focuses on automating theprocess of prompt creation for Large Language Models(LLMs). APE seeks to streamline and optimize the promptdesign process, leveraging the capabilities of LLMs themselvesto generate and evaluate prompts. APE involves using LLMsin a self-referential manner where the model is employedto generate, score, and refine prompts. This recursive use ofLLMs enables the creation of high-quality prompts that aremore likely to elicit the desired response or outcome.The methodology of APE can be broken down into severalkey steps:• Prompt Generation: The LLM generates a range ofpotential prompts based on a given task or objective.• Prompt Scoring: Each generated prompt is thenevaluated for its effectiveness, often using criterialike clarity, specificity, and likelihood of eliciting thedesired response.• Refinement and Iteration: Based on these evalua-tions, prompts can be refined and iterated upon, furtherenhancing their quality and effectiveness.C. Augmenting LLMs through external knowledge - RAGOne of the main limitations of pre-trained LLMs is theirlack of up-to-date knowledge or access to private or use-case-specific information. This is where retrieval augmentedgeneration (RAG) comes into the picture [164]. RAG, illus-trated in figure 37, involves extracting a query from the inputprompt and using that query to retrieve relevant informationfrom an external knowledge source (e.g. a search engine or aknowledge graph, see figure 38 ). The relevant information isthen added to the original prompt and fed to the LLM in orderfor the model to generate the final response. A RAG systemincludes three important components: Retrieval, Generation,Augmentation [165].a) RAG-aware prompting techniques: Because of theimportance of RAG to build advanced LLM systems, severalRAG-aware prompting techniques have been developed re-cently. One such technique is Forward-looking Active RetrievalAugmented Generation (FLARE)Forward-looking Active Retrieval Augmented Generation(FLARE) [168] enhances the capabilities of Large LanguageModels (LLMs) by iteratively combining prediction and in-formation retrieval. FLARE represents an evolution in theuse of retrieval-augmented generation, aimed at improving theaccuracy and relevance of LLM responses.FLARE involves an iterative process where the LLMactively predicts upcoming content and uses these predictionsas queries to retrieve relevant information. This method con-trasts with traditional retrieval-augmented models that typicallyretrieve information once and then proceed with generation. InFLARE, this process is dynamic and ongoing throughout thegeneration phase. In FLARE, each sentence or segment gener-ated by the LLM is evaluated for confidence. If the confidencelevel is below a certain threshold, the model uses the generatedcontent as a query to retrieve relevant information, which isthen used to regenerate or refine the sentence. This iterativeFig. 37: An example of synthesizing RAG with LLMs for question answering application [166].Fig. 38: This is one example of synthesizing the KG as aretriever with LLMs [167].process ensures that each part of the response is informed bythe most relevant and current information available.For more details on RAG framework and its relevant works,we refer the readers to this survey of retrieval augmentedgenerations [165].D. Using External ToolsRetrieving information from an external knowledge sourceas described above is only one of the potential ways to augmentan LLM. More generally, an LLM can access any numberof external tools (e.g. an API to a service) to augment itsfunctionality. In that regards, RAG can be seen as a specificinstance of the broader category of the so called ”tools”.Tools in this context are external functions or services thatLLMs can utilize. These tools extend the range of tasks anLLM can perform, from basic information retrieval to complexinteractions with external databases or APIs.In the paper ”Toolformer: Language Models Can TeachThemselves to Use Tools” [169], the authors go beyond simpletool usage by training an LLM to decide what tool to usewhen, and even what parameters the API needs. Tools includetwo different search engines, or a calculator. In the followingexamples, the LLM decides to call an external Q&A tool,a calculator, and a Wikipedia Search Engine More recently,researchers at Berkeley have trained a new LLM called Gorilla[67] that beats GPT-4 at the use of APIs, a specific but quitegeneral tool.a) Tool-aware prompting techniques: Similarly to whatwas described with RAG, several tool-aware prompting ap-proaches have been developed to make usage of tools morescalable. A popular technique is the so called Automatic Multi-step Reasoning and Tool-use (ART).Automatic Multi-step Reasoning and Tool-use (ART) [170]is a prompt engineering technique that combines automatedchain of thought prompting with the use of external tools.ART represents a convergence of multiple prompt engineeringstrategies, enhancing the ability of Large Language Models(LLMs) to handle complex tasks that require both reasoningand interaction with external data sources or tools.ART involves a systematic approach where, given a taskand input, the system first identifies similar tasks from a tasklibrary. These tasks are then used as examples in the prompt,guiding the LLM on how to approach and execute the currenttask. This method is particularly effective when tasks require acombination of internal reasoning and external data processingor retrieval.E. LLM AgentsThe idea of AI agents has been well-explored in the historyof AI. An agent is typically an autonomous entity that canperceive the environment using its sensors, make a judgmentbased on the state it currently is, and accordingly act based onthe actions that are available to it.In the context of LLMs, an agent refers to a system basedon a specialized instantiation of an (augmented) LLM thatis capable of performing specific tasks autonomously. Theseagents are designed to interact with users and environment tomake decisions based on the input and the intended goal ofthe interaction. Agents are based on LLMs equipped with theability to access and use tools, and to make decisions based onthe given input. They are designed to handle tasks that requirea degree of autonomy and decision-making, typically beyondsimple response generation.The functionalities of a generic LLM-based agent include:• Tool Access and Utilization: Agents have the capabil-ity to access external tools and services, and to utilizethese resources effectively to accomplish tasks.• Decision Making: They can make decisions based onthe input, context, and the tools available to them,often employing complex reasoning processes.As an example, an LLM that has access to a function (oran API) such as weather API, can answer any question relatedto the weather of the specific place. In other words, it can useAPIs to solve problems. Furthermore, if that LLM has accessto an API that allows to make purchases, a purchasing agentcan be built to not only have capabilities to read informationfrom the external world, but also act on it [171].Fig. 40 shows another example of LLM-based agents forconversational information seeking [36], where an LLM isaugmented with a set of plug-and-play modules, includinga working memory that tracks the dialog state, a policy thatmakes an execution plan for the task and selects next systemaction, an action executor that performs an action selected bythe policy (consolidating evidence from external knowledge,or prompting the LLM to generate responses), and a utilitythat accesses the alignment of the LLM’s responses with userexpectations or specific business requirements, and generatefeedback to improve agent performance.For more details on LLM-based AI agents see recent survey[172], [173], [174].a) Prompt engineering techniques for agents: LikeRAG and Tools, prompt engineering techniques that specif-ically address the needs of LLM-based agents have beendeveloped. Three such examples are Reasoning without Ob-servation (ReWOO), Reason and Act (ReAct), and Dialog-Enabled Resolving Agents (DERA).Reasoning without Observation (ReWOO) [175] aims todecouple reasoning from direct observations. ReWOO operatesby enabling LLMs to formulate comprehensive reasoning plansor meta-plans without immediate reliance on external dataor tools. This approach allows the agent to create a struc-tured framework for reasoning that can be executed once thenecessary data or observations are available. In ReWOO, theLLM initially develops a plan (a series of steps) that outlineshow to approach and solve a given problem. This meta-planning phase is crucial as it sets the stage for the agent toprocess information once it becomes available. The executionphase then involves integrating actual data or observations intothe pre-specified plan, leading to coherent and contextuallyrelevant responses. ReWOO offers significant advantages interms of token efficiency and robustness to tool failure. Itenables LLMs to handle tasks where immediate access toexternal data is not available, relying instead on a well-structured reasoning framework. This method is particularlyadvantageous in scenarios where data retrieval is costly, slow,or uncertain, allowing the LLM-based agent to maintain a highlevel of performance and reliability.Reason and Act (ReAct)[176] prompts LLMs to generatenot only verbal reasoning but also actionable steps, thusenhancing the model’s dynamic problem-solving capabilities.ReAct is grounded in the principle of integrating reasoningwith action. In this approach, the LLM is prompted to alternatebetween generating reasoning traces (explanations) and takingactions (steps or commands) in an interleaved manner. Thisapproach allows the model to dynamically reason about a prob-lem, and propose and take concrete actions simultaneously.Dialog-Enabled Resolving Agents (DERA) [177] are spe-cialized AI agents that can engage in dialogue, resolve queries,and make decisions based on interactive exchanges. DERAis developed based on the idea of utilizing multiple agentswithin a dialog context, each with specific roles and functions.These agents can include Researchers, who gather and analyzeinformation, and Deciders, who make final judgments basedon the information provided. This division of roles allows fora well-organized and efficient approach to problem-solvingand decision-making. DERA is particularly advantageous inscenarios requiring complex decision-making and problem-solving, such as those in medical diagnostics or customer ser-vice. The collaborative and interactive nature of DERA agentsallows them to handle intricate queries with a level of depthand nuance that single-agent systems might struggle with.Moreover, this approach aligns well with human decision-making processes, making AI reasoning more relatable andtrustworthy.V. POPULAR DATASETS FOR LLMSLarge language models exhibit promising accomplish-ments, but the main question that arises is how effectivelythey function and how their performance can be assessed inspecific tasks or applications.The evaluation of LLMs poses particular challenges dueto the evolving landscape of their applications. The originalintent behind developing LLMs was to boost the performanceof NLP tasks such as translation, summarization, question-answering, and so on [178]. However, it is evident todaythat these models are finding utility across diverse domainsincluding code generation and finance. Moreover, the eval-uation of LLMs encompasses several critical considerationssuch as fairness and bias, fact-checking, and reasoning. Inthis section, we outline the commonly used benchmarks forassessing LLMs. These benchmarks are categorized based ontraining or evaluating the LLM Capabilities.A. Datasets for Basic Tasks: language model-ing/understanding/generationThis section provides an overview of the benchmarks anddatasets suited to evaluate the basic abilities of LLMs.• Natural Questions [179] is a QA dataset that consistsof real anonymized, aggregated queries submitted tothe Google search engine as questions. An annotatoris presented with a question along with a Wikipediapage from the top 5 search results, and annotates along answer (typically a paragraph) and a short answerFig. 39: HuggingGPT: An agent-based approach to use tools and planning [image courtesy of [171]]Fig. 40: A LLM-based agent for conversational informationseeking. Courtesy of [36].(one or more entities) if present on the page, or marksnull if no long/short answer is present.• MMLU [180] is intended to evaluate the knowl-edge gained in zero-shot and few-shot scenarios. Thatmeans that MMLU assesses both the general knowl-edge and problem-solving ability of a model. It covers57 subjects in STEM, humanities, social sciences,and other areas. The benchmark varies in complexity,ranging from elementary to advanced professional.It is worth mentioning that the main contribution ofthis dataset is for multi-task language understanding,question answering, and arithmetic reasoning.• MBPP [181] stands for “Mostly Basic Python Prob-lems” and provides a benchmark for evaluating theperformance of models designed for code generation.The benchmark encompasses 974 short Python pro-grams including a wide range of topics, includingfundamental programming concepts and standard li-brary usage, and more. Each challenge comprises atask description, a code solution, and three automatedtest cases.• HumanEval [182] is a dataset for code generationtask. This dataset consists of 164 hand-crafted pro-gramming challenges. Each challenge is accompaniedby a function signature, docstring, code body, and mul-tiple unit tests. The main intuition behind developingthis dataset is to guarantee the exclusion of its contentsfrom training datasets for code generation models.• APPS [183] is designed for code generation taskfocusing on the Python programming language. TheAPPS dataset contains a collection of 232, 444 Pythonprograms. Each program in the dataset has an averageof 18 lines of Python code. Additionally, APPS offersaccess to a repository of 10, 000 unique programmingexercises, each with text-based problem descriptions.The final aspect to highlight is that the it includes testcases.• WikiSQL [184] is crafted for code generation task andit has 87,726 carefully labeled pairs of SQL queriesand corresponding natural language questions fromWikipedia tables. The SQL queries comprise threesubsets: test sets (17, 284 examples), development(9, 145 examples), and training (61, 297 examples).• TriviaQA [185] is designed for QA task. Thisdataset comprises more than 650, 000 question-answer-evidence triples. There are 95, 000 question-answer pairs in this dataset, each authored by trivia en-thusiasts and supported by an average of six indepen-dently sourced evidence documents. These documentsare automatically acquired from Wikipedia or broaderweb search results. The dataset is categorized intotwo segments, including those with authentic answersfrom Wikipedia and web domains, and verified setsembody the accurately answered questions along withtheir associated documents from both Wikipedia andonline.Fig. 41: Dataset applications.• RACE [186] suits for reading comprehension task.This dataset is based on English tests completed byChinese students from middle school and high school,aged 12 to 18, and it contains roughly 28, 000 textsand 100, 000 questions rigorously prepared by humanspecialists, primarily English instructors. This datasetcontains a wide range of subjects that were purpose-fully chosen to assess students’ comprehension andreasoning abilities. This dataset is available in threesubgroups: RACE-M, RACE-H, and RACE. RACE-M refers to the middle school examinations, whereasRACE-H denotes the high school tests. Finally, RACEis the synthesis of RACE-M and RACE-H.• SQuAD [187] stands for “Stanford Question Answer-ing Dataset” and is a crowdsourced reading compre-hension dataset based on Wikipedia articles. It hasapproximately 100, 000 question-answer pairs con-nected to more than 500 articles. The answers tothese questions are typically text fragments or spanstaken from the corresponding reading passages. Thequestions may be unanswerable in some cases. Thedataset is divided into three sets: an 80% training set,a 10% development set, and a 10% hidden test set.Fig. 42: Datasets licensed under different licenses.• BoolQ [188] is a yes/no question-answering datasetwhere the goal is reading comprehension task. BoolQincludes 15, 942 examples. Each example is a tripletthat includes a question, a relevant paragraph, andthe solution. Although the main intuition behindthis dataset is for reading comprehension, it can beused for reasoning, natural language inference, andquestion-answering tasks.• MultiRC [189] is another dataset that fits readingcomprehension task. MultiRC contains brief para-graphs as well as multi-sentence questions that canbe answered using the information in the paragraph.The paragraphs in this dataset come from a varietyof sources, including news, fiction, historical texts,Wikipedia articles, discussions on society and law,elementary school science textbooks, and 9/11 re-ports. Each question has many response choices, withone or more of them being correct. Answering thequestions requires reasoning across several sentences.MultiRC dataset encompasses around 6, 000 multi-sentence questions gathered from over 800 paragraphs.On average, each question offers about two validanswer alternatives out of a total of five.B. Datasets for Emergent: ICL, reasoning (CoT), instructionfollowingThis section centers on the benchmarks and datasets em-ployed to evaluate the emergent abilities of LLMs.• GSM8K [190] is designed to evaluate the model’sability for multi-step mathematical reasoning. GSM8Kincludes 8.5K linguistically diverse grade school mathword problems written by humans. The dataset is splitinto two sets: a training set with 7.5K problems,and a test set with 1K problems. These problemsneed 2 to 8 steps to be solved. Solutions mainlyare a series of elementary calculations using basicarithmetic operations.• MATH [191] enables to assess how well models cansolve math problems. MATH dataset hast 12, 500problems from high school math competitions. Eachproblem in the dataset has a step-by-step solution anda final answer enclosed in a box. The problems covera wide range of topics and have different levels ofcomplexity. There are seven subjects in total. Further-more, the difficulty of each problem is rated basedon the AoPS standards on a scale from ′1′ to ′5′. A′1′ shows the easiest problems in a subject, while ′5′represents the most difficult. In terms of formatting,all problems and solutions are presented using LATEXand the Asymptote vector graphics language.• HellaSwag [192] is designed to assess commonsensereasoning in LLMs. This benchmark includes 70, 000multiple-choice questions. Each question is derivedfrom one of two domains: ActivityNet or WikiHow,and presents four answer choices regarding whatmight happen in the following situation. The correctanswer provides an actual statement describing theupcoming event, but the three wrong answers arecreated to confuse machines.• AI2 Reasoning Challenge (ARC) [193] is usedfor commonsense reasoning. This benchmark encom-passes 7, 787 science examination questions. Thesequestions are in English, and most of them are setup in a multiple-choice format. The questions havebeen divided into two groups: a Challenge Set with2, 590 difficult questions and an Easy Set with 5,197questions. Each collection has also been pre-dividedinto Train, Development, and Test subsets.• PIQA [194] is intended to evaluate the languagerepresentations on their knowledge of physical com-monsense. In this dataset, the focus is on everydaysituations with a preference for uncommon solutions.The central task is a multiple-choice question answer-ing, where a question (q) is provided along with twopotential solutions (s1, s2). Then, the best solution ischosen by whether a model or a human. For eachquestion, only one of the solutions is the correctanswer.• SIQA [195] provides a framework for evaluating mod-els’ ability for commonsense reasoning about socialsituations. SIQA dataset has 38, 000 multiple-choicequestions designed to assess emotional and socialintelligence in everyday circumstances. This datasetcovers a wide variety of social scenarios. In SIQA,the potential answers is a mixture of human-selectedresponses and machine-generated ones that have beenfiltered through adversarial processes.• OpenBookQA (OBQA) [196] is a new kind ofquestion-answering dataset where answering its ques-tions requires additional common and commonsenseknowledge not contained in the book and rich textcomprehension. This dataset includes around 6,000multiple-choice questions. Each question is linked toone core fact, as well as an additional collectionof over 6000 facts. The questions were developedusing a multi-stage crowdsourcing and expert filter-ing procedure. OpenBookQA questions are difficultbecause they need multi-hop reasoning with limitedbackground.• TruthfulQA [197] is designed specifically to eval-uate the truthfulness of language models in gen-erating answers to questions. This dataset includes817 questions, written by authors, from 38 differentcategories, including health, law, finance, and politics.These questions are purposefully designed to chal-lenge human responders, as they may contain commonmisunderstandings that lead to incorrect answers.• OPT-IML Bench [103] is a comprehensive bench-mark for Instruction Meta-Learning. It covers 2000NLP tasks from 8 existing benchmarks. The OPT-IMLBench consists of a training set with 17.9 M examples,a dev set with 145K samples, and a test set with 321Ksamples.C. Datasets for Augmented: using external knowledge/toolsThis section focuses on datasets designed for the aug-mented abilities of LLMs.• HotpotQA [198] is designed to cover a diverse andexplainable question-answering dataset that necessi-tates multi-hop reasoning. This dataset is derived fromthe English Wikipedia. It consists of roughly 113, 000questions. Each question in the dataset comes withtwo paragraphs, called gold paragraphs, from twoWikipedia articles. Also, there is a list of sentencesin those paragraphs that crowdworkers have picked asimportant for answering the question.• ToolQA [199] is a question answering benchmarkto evaluate LLMs’ ability to use external tools foranswering questions.• GPT4Tools serves as an instructional dataset, gener-ated by instructing advanced teachers (such as Chat-GPT), with instructions conditioned on visual contentand tool descriptions. This process results in thegeneration of instructions related to the use of tools.There are three versions of this dataset. The firstversion comprises 71,000 instruction-following datapoints utilized to fine-tune the GPT4Tools model. Thenext version consists of manually cleaned instructiondata used for validation, covering instructions relatedto the tools from the first version. The last version iscleaned instruction data used for testing and includesinstructions related to some tools that are not presentin the first version.VI. PROMINENT LLMS’ PERFORMANCE ONBENCHMARKSIn this section we first provide an overview of some ofpopular metrics used for evaluating the performance of LLMsunder different scenarios. We then look at the performanceof prominent large language models on some of the populardatasets and benchmarks.A. Popular Metrics for Evaluating LLMsEvaluating the performance of generative language modelsdepends on the underlying task they are going to be used for.Tasks that are mostly about selecting a choice out of givenones (such as sentiment analysis), can be seen as simple asclassification and their performance can be evaluated usingclassification metrics. Metrics such as accuracy, precision,recall, F1, etc are applicable in this case. It is also important tonote that the answers generated by the model for specific taskssuch as multi-choice question answering are always either Trueor False. If the answer is not in a set of options, it can be seenas False as well.However, some tasks that are purely open-ended text gener-ation cannot be evaluated in the same way as for categorization.Different metrics are required for the specific purpose of theevaluation. Code generation is a very different case in open-ended generative evaluations. The generated code must passthe test suite but on the other hand, it is also importantto understand if a model is capable of generating differentTABLE II: LLM Datasets Overview.Benchmark Name Evaluation Metric Leaderboard Source paperswithcodeHumanEval PASS@k Link Link LinkMBPP PASS@k, Accuracy - Link LinkAPPS PASS@k, Accuracy - Link LinkWikiSQL Accuracy - Link LinkCoNaLa BLEU Link LinkCodeParrot PASS@k - Link -HellaSwag Accuracy Link Link LinkAI2 ReasoningChallenge (ARC) Accuracy Link Link LinkBoolQ Accuracy - Link LinkMultiRC F1-score, Accuracy - Link LinkCNN/Daily Mail [200] Accuracy - Link -SQuAD F1-score, EM Link Link LinkRACE Accuracy - Link LinkCNN/Daily Mail [201] ROUGE - Link LinkDrop F1-score, EM Link Link LinkQuAC F1-score, HEQ-Q, HEQ-D Link Link LinkTriviaQA EM, F1-score, Accuracy Link Link LinkNatural Questions EM, F1-score, Accuracy Link Link LinkStrategyQA Accuracy, Recall@10, SARI Link Link LinkCoQA F1-score Link Link LinkXSum ROUGE - Link LinkSAMSum ROUGE - - LinkWikiSum ROUGE - Link -DialogSum ROUGE - Link LinkTruthfulQA MC1 , MC2, % true, % info, BLEURT Link Link LinkMMLU Accuracy Link Link LinkGSM8K Accuracy Link Link LinkPIQA Accuracy Link Link LinkSIQA Accuracy Link Link LinkOpenBookQA (OBQA) Accuracy Link Link LinkHotpotQA EM, F1-score, Joint EM, Joint F1-score, Link Link LinkMATH Accuracy - Link LinkCommonsenseQA Accuracy Link Link LinkNatural Instructions ROUGE-L, Human Link Link LinkBIG-bench Accuracy, Average - Link LinkToolTalk Success rate, Precision, Recall, Incorrectaction rate, Percent of failing error types - Link LinkMetaTool Accuracy, Precision, Recall, F1-score - Link LinkGPT4ToolsSuccessful Rate of Thought, SuccessfulRate of Action, Successful Rate of Ar-guments, Success Rate- Link LinkAPI-BankCorrectness, ROUGE, Error(API Hallu-cination, Has Exception, Invalid InputParameters, False API Call Format, APICall, Miss Input Parameters)- Link LinkAlpaca-CoT - - Link Linksolutions as a code, what is the probability of selecting thecorrect one among them. Pass@k is a very good metric in thiscase. It works in this manner that given a problem, differentsolutions as code are generated. They are tested for correctnessusing different functionality tests. Afterward, from generatedn solutions, and the respective c number of them being correctequation 4 provides the final value.pass@k := EProblems[1−(n−ck)(nk) ] (4)Exact match (EM) is another metric that is mostly con-cerned with exact matches from (pre-defined) answers. Itcounts a prediction as correct if it exactly matches one ofmore than one desired reference text token by token. In somecases, it can be the same as accuracy and the equation 5 showsthe mathematical definition. Here M is total number of correctanswers and N is the total number of questions [202].EM =MN(5)Human equivalence score (HEQ) on the other hand, is analternative to F1 score [203]. HEQ-Q represents the precisionof individual questions, wherein an answer is deemed correctif the model’s F1 score surpasses the average human F1 score.Likewise, HEQ-D denotes the precision of each dialogue; it isdeemed accurate when all questions within the dialogue meetthe criteria of HEQ [182].Evaluation of other generative tasks such as machine trans-lation are based on metrics such as Rouge and BLEU. Thesescores work well when there is a reference text as groundtruth (such as translation) and a hypothesis that is generatedby the generative model, in our case the LLM. These scoresare mostly used for cases where the goal is to detect thesimilarity of the answer and ground truth in a computationmanner. In a computation manner, it meant that nothing morethan N-Grams would be used. However, metrics such as BERT-Score are also good for these cases but they are also heavilyTABLE III: LLM categories and respective definitions.Classification Category DescriptionSizeSmall Number of parameters ≤ 1BMedium 1B < Number of parameters ≤ 10BLarge 10B < Number of parameters ≤ 100BVery Large 100B < Number of parametersTypeFoundation model Pretrained language modelInstruction model Pretrained and instruction fine-tuned language modelChat model Pretrained, instruction fine-tuned, and chat fine-tuned language modelOrigin Original model An original model released with either Foundation, Instruction, or Chat modelTuned model Fine-tuned version of an original modelAvailability Publicly available Model and weights are available due to request to without requestPublicly unavailable Model and weights are not publicly availableTABLE IV: Different LLM categorization.Model Size #Params (B) Type Availability OriginDavinci-002 Very Large 175 Instruction Unavailable TunedDavinci-003 Very Large 175 Instruction Unavailable TunedGPT 3.5-turbo Large 20 Chat Unavailable TunedFalcon 7B Medium 7 Foundation Public OriginalAlpaca Large 13 Chat Public TunedPythia 7B Medium 7 Foundation Public OriginalPythia 12B Large 12 Foundation Public OriginalLLAMA 7B Medium 7 Chat Public OriginalLLAMA 2 7B Medium 7 Chat Public TunedLLAMA 2 7B Medium 7 Foundation Public OriginalVicuna 13B Large 13 Foundation Public TunedVicuna 7B Medium 7 Foundation Public TunedClaude Large 93 Chat Unavailable OriginalClaude 2 Very Large 137 Chat Unavailable Originalerroneous because another model is used to judge. Still, eventoday, evaluating purely generated content is very hard andno completely fitting metric is not found, metrics are eitherlooking for simplistic features such as N-Gram, SkipGram,etc, or they are models with unknown accuracy and preciseness[204].Generative evaluation metrics are also another type of eval-uation metric for LLMs that use another LLM for evaluatingthe answer. However, depending on the task itself, evaluationcan be possible in this way or not. Another dependencythat makes generative evaluation error-prone is reliance onthe prompt itself. RAGAS is one of the good examples thatincorporate the usage of generative evaluation.Various benchmarks and leaderboards have been proposedto address the most challenging question in the world oflarge language models: Which one is better? However nota simple answer can address this question. The answer de-pends on various aspects of large language models. Section Vshows the categorical presentation of different tasks and themost important datasets in each category. We will follow thesame categorization and provide a comparison based on eachcategory. After providing comparison for each category, wewill provide a broad overview of aggregated performance byaveraging the reported performance metric on different tasks.Evaluating different LLMs can be seen also from differentperspectives. For example, a LLM with a drastically fewernumber of parameters is not completely comparable to onewith a larger number of parameters. From this perspective, wewill categorize LLMs in four categories as well: small (lessthan or equal to 1 billion parameters), medium (between 1 and10 billion), large (between 10 and 100 billion), and very large(more than 100 billion). Another classification for the LLMswe use is their primary use case. We consider each LLM tobe either: Foundation model (pretrained language model withno instruction fine-tuning and chat fine-tuning), Instructionmodel (pretrained language model with only instruction fine-tuning), and Chat model (pretrained language model withinstruction and chat fine-tuning). Apart from all the catego-rization described, another category is required to distinguishbetween original models and tuned ones. Original models arethose that have been released as a foundation model or a fine-tuned one. Tuned models are those that grasped the originalmodel and tuned it with different datasets or even differenttraining approaches. It is also good to note that original modelsare usually foundation models that have been fine-tuned onspecific datasets or even different approaches. Availability ofthe model weights regardless of the license is another categoryin our classification. Models that have their weights publiclyavailable (even through request) are noted as Public modelswhile others are noted as Private. Table III shows all of thesedefinitions and abbreviations used in the rest of the article.Figure 43 illustrate these visually.According to the provided categorizations, we can catego-rize and label each notable LLM as shown in table IV. As canbe seen from this table, models categorized as very large arealso unavailable as well.B. LLMs’ Performance on Different TasksCommonsense reasoning is one of the important capabili-ties each model can obtain. This capability denotes the abilityof the model to use prior knowledge in combination withreasoning skills. In the case of HellaSwag for example, findingthe continuation of text is challenging because the given textcontains a partial part of the story while the given choicesas continuation are tricky to select, and without having priorLargeLanguageModelsParametersAvailabilityOriginality TypeSmall LM# of params <1BMedium LM1B < # of params <10BLarge LM10B < # of params <100BVery Large LM100B < # of paramsTunedFine tuningOriginalPublicPrivateFoundationInstructionChatFine tuned models that are originallybased on original models.Example: Alpaca (based on LLaMA)Original models that are not finetuned or based on any otherpretrained model.Example: LLaMAModel weights are publicly releasedand is available.Example: LLaMAModel weights are NOT publiclyreleased and is NOT available.Example: GPT-4Pretrained model with no instructionor chat fine-tuning.Example: MPT-7BPretrained model that isalso fine-tuned oninstruction following.Example: MPT-7B-instructPretrained model that isalso fine-tuned on chat.Example: MPT-7B-chatFig. 43: LLM categorizations.knowledge about the world it is not possible. This specific kindof reasoning deserves high attention because it is related toutilizing previous knowledge with open text-described scenesor facts. As can be seen from table V not just Unavailablemodels but also Public ones can achieve good results onvarious tests.TABLE V: Commonsense reasoning comparison.Model OBQA HellaSwagDavinci-003 51 83.4Falcon 7B 44.4 76.3Alpaca 43.4 73.9Pythia 7B 37.2 64Pythia 12B 43.2 68.1LLAMA 7B 42.4 73Dolly 6B 41.2 67.6Dolly 12B 40.4 71Alpaca 7B 43.4 73.9Alpaca Lora 7B 42.6 74GPT-J 6.7B 38.2 66.2LLama 7B 42.4 73LLama 13B 42.2 76.2Pythia 6.7B 37.2 64Pythia 12B 38 67.3StableLM Tuned 33.4 53.6Koala 13B 42.8 72.6Mosaic mpt-7B 42.6 76.3LLAMA 2 70B - 87.33LLAMA 65B - 86.09Falcon 40B - 85.3Falcon 180B - 88.86MPT Instruct 30B - 84.31MPT Instruct 7B - 77.91Yi 6B - 76.42Yi 34B - 85.69GPT-4 - 95.3Gemini Ultra - 87.8From the results presented in Table V it is clear that GPT-4achieves best results for HellaSwag while Davinci-003 is bestmodel for OBQA. It is also good to note that results for OBQAare not reported for all of the models and possibly davinci-003is not the best model achieving highest results on OBQA.Not all models report their performance on all datasets, andbecause of that, the number of models for which performanceis reported in different tables varies.TABLE VI: Symbolic reasoning comparison.Model Cobjects PenguinsGPT-NeoX 26 33.56OPT 66B 31.2 28.08Bloomberg GPT 34.8 37.67BLOOM 176B 36.8 40.41PaLM 540B 38 44.5Gopher-280B 49.2 40.6Chinchilla-70B 59.7 48.7PaLM 2 61.2 65.8World knowledge is mostly about general knowledge ques-tions, for example, in Wikifact dataset questions such as ”Whois the author of a specific well-known book” can be found andreferences are also provided. Table VII shows the results.TABLE VII: World knowledge comparison.Model TriviaQA NaturalQ WebQ ARCBLOOM - - - 32.9BLOOM 176B - - - 50.85Bloomberg GPT - - - 48.63Chinchilla - 35.5 - -Codex + REPLUG 76.8 44.7 - -GAL 120B - - - 67.9GLaM 62B/64E 75.8 32.5 15.5 50.3Gopher - 28.2 - -GPT-3 175B 71.2 29.9 41.5 85.2GPT-4 - - - 96.4GPT-NeoX - - - 45.39LLaMA 13B - - - 52.7LLaMA 2 70B 85 33 - -LLaMA 33B - 24.9 - 57.8LLaMA 65B 72.6 39.9 - -LLaMA 7B - - - 47.6Mistral 7B 69.9 28.8 - 55.5Neo-6B - 13.7 - -OPT - - - 31.1OPT 66B - - - 44.54OPT-175B - - - 43.94OPT-175B - - - 25.6PaLM 2-L 86.1 37.5 28.2 95.1PaLM 2-M 81.7 32 26.9 64.9PaLM 2-S 75.2 25.3 21.8 59.6PaLM-540B 81.4 39.6 43.5 87.1phi-1.5-web 1.3B - - - 44.9SparseGPT - - - 38.99SparseGPT - - - 39.85SparseGPT - - - 41.3For some specific use-case models, it is highly demanded tohave coding and code-generation capability. Table VIII showsthe results of different models on coding capability.TABLE VIII: Coding capability comparison.Model HumanEvalGemini Ultra 74.4Gemini Pro 67.7GPT-4 67WizardCoder 15B 57.3phi-1 1.3B 50.6Code Llama 48.8GPT-3.5 48.1OctoCoder 46.2phi-1-small 45PaLM 2-S 37.6InstructCodeT5+ 16B 35Mistral 7B 30.5LLaMA 2 29.9phi-1-base 29Codex-12B 28.81PaLM 540B 26.2CodeT5+ 2B 24.2LLaMA 65B 23.7LLaMA 33B 21.7PaLM 62B 15.9LLaMA 13B 15.8LaMDA 137B 14MIM-350M 13.7LLaMA 7B 10.5PaLM 8B 3.6Arithmetic reasoning is another challenging reasoning ca-pability to achieve. GSM8K for example contains grade schoolmathematical questions with respect to their answers. Table IXprovides an insight for different model comparisons.TABLE IX: Arithmetic reasoning comparison.Model GSM8k MATHGemini Ultra 94.4 53.2GPT-4 87.1 42.5Gemini Pro 86.5 32.6ToRA 70B 84.3 49.7MathCoder-L-70B 83.9 -MetaMath 70B 82.3 26MuggleMATH 70B 82.3 -MathCoder-CL-34B 81.7 45.2ToRA-Code 34B 80.7 50.8MetaMath-Mistral-7B 77.7 -Arithmo2-Mistral-7B 76.4 -ToRA-Code 13B 75.8 48.1Arithmo-Mistral-7B 74.7 -MathCoder-CL-13B 74.1 35.9MuggleMATH 13B 74 -CodeT5+ 73.8 -KwaiYiiMath 13B 73.3 -ToRA-Code 7B 72.6 44.6MathCoder-L-13B 72.6 29.9MetaMath 13B 71 22.5LLaMA 65B 69.7 10.6MuggleMATH 7B 68.4 -MathCoder-CL-7B 67.8 23.3MetaMath 7B 66.4 19.4RFT 70B 64.8 -MathCoder-L-7B 64.2 -Orca 2-13B 59.14 -U-PaLM 58.5 -PaLM-540B 58.1 8.8LLaMA 2 70B 56.8 -RFT 13B 55.3 -LLaMA 33B 53.1 7.1Mistral 7B 52.2 13.1RFT 7B 51.2 -LLaMA 65B 50.9 20.5Orca 2-7B 47.23 -Text-davinci-002 40.7 19.1LLaMA 33B 35.6 3.9GPT-Neo-2.7B 19.5 -LLaMA 7B 18.1 2.9PaLM 540B 17.9 8.8LLaMA 13B 17.8 3.9LLaMA 7B 11 2.9GPT-Neo-125M 7.5 -PaLM 8B 4.1 1.5GPT-2 - 5.4GPT-3 175B - 5.2PaLM 62B - 4.4GPT-3-13B - 3LLaMA 7B 11 2.9PaLM 8B - 1.5Large language models in some cases are hallucinating an-swers simply because they are next-token prediction machines.Hallucination is one of the important factors in measuringhow much a large language model is trustworthy and reliable.Measuring hallucination on the other hand is also not easy as itseems because each fact can be written in different styles andeven the smallest changes in writing make it hard to detect.It is fair to assume if any particular LLM is more capableto detect hallucination of false information in text, it is alsomore trustworthy. HaluEval is one of the datasets that aims tomeasure hallucination in this field [205]. Evaluation can also beperformed by another model judging the response with regardto the actual answer [206]. Table X shows the evaluation ofdifferent models based on these datasets.VII. CHALLENGES AND FUTURE DIRECTIONSAs we have seen in the previous sections, large languagemodels have achieved impressive results in the past 1-2 years.TABLE X: Hallucination evaluationModel HHEM HaluEval QA HaluEval Dialogue HaluEval Sum. HaluEval GeneralGPT 4 97 - - - -GPT 4 Turbo 97 - - - -GPT 3.5 Turbo 96.5 62.59 72.4 58.53 79.44Davinci002 - 60.05 60.81 47.77 80.42Davinci003 - 49.65 68.37 48.07 80.4GPT-3 - 49.21 50.02 51.23 72.72Google Gemini Pro 95.2 - - - -Llama 2 70B 94.9 - - - -Llama 2 7B 94.4 49.6 43.99 49.55 20.46Llama 2 13B 94.1 - - - -Cohere-Chat 92.5 - - - -Cohere 91.5 - - - -Claude 2 91.5 69.78 64.73 57.75 75Claude 1 67.6 64.83 53.76 73.88Microsoft Phi 2 91.5 - - - -Google Palm 2 (beta) 91.4 - - - -Mixtral 8x7B 90.7 - - - -Amazon Titan Express 90.6 - - - -Mistral 7B 90.6 - - - -Google Palm 2 Chat (beta) 90 - - - -Google Palm 2 87.9 - - - -Google Palm 2 Chat 72.8 - - - -ChatGLM - 47.93 44.41 48.57 30.92Falcon - 39.66 29.08 42.71 18.98Vicuna - 60.34 46.35 45.62 19.48Alpaca - 6.68 17.55 20.63 9.54At the same time this is still a new and extremely activeresearch area where the pace of innovation is increasing ratherthan slowing down. As in any other evolving area though, thereare still numerous challenges ahead. Here we briefly mentionsome of the challenges and main active areas which are knownso far. It is worth noting that LLM challenges are discussedin details in a work by Kaddour et al. [207].A. Smaller and more efficient Language ModelsThis is a survey on large language models, and therehas been an initial push towards ”larger is better” that hasclearly been rewarded with ever larger models like GPT-4 getting better accuracy and performance in benchmarks.However, those large models are costly and inefficient inseveral dimensions (e.g. high latency). In response to all ofthis, there is a current research trend to come up with SmallLanguage Models (SLMs) as a cost-effective alternative toLLMs, particularly when used on specific tasks that might notrequire the full generality of larger models. Prominent worksin this direction include Phi-1 [208], Phi-1.5 [209], and Phi-2from Microsoft.More generally, we should expect many research efforts inthis area of how to train smaller and more efficient models.Techniques such as parameter-efficient fine-tuning (PEFT),teacher/student, and other forms of distillation – see sectionIII-I – will continue to be used to build a smaller model outof larger ones.B. New Post-attention Architectural ParadigmsTransformer blocks have been a crucial and constant part ofmost of current LLM frameworks, and it’s a big question markhow much longer this architecture will be in vogue, and whatwill be the next big architectural break-through in the field ofdeep learning (and NLP). Since AlexNet in 2012, we have seenmany architectures go in and out of fashion, including LSTM,GRU, seq2seq, but Transformers have been the dominantapproach since its inception. As described earlier, attention isthe main mechanism driving transformers. More recently, therehas been promising research in alternative approaches that arebeing labelled as post-attention.An important class of such class of post-attention modelsare the so called State Space Models (SSMs). While the notionof State Space Models has a long history in machine learning,it should be noted that in the context of language models, SSMis usually used in reference to the newer Structure State SpaceModel architecture or S4 for short (see Gu et al. [29]). Somerecent models in this category are Mamba [30], Hyena [210],and Striped Hyena [211].While all of those models are very competitive in terms ofperformance in leaderboards and efficiency, they also addressan important challenge in more traditional attention-basedarchitectures: the lack of support for larger context windows.Having a good answer to many prompts requires context.For example, the response to ”Recommend some good moviesfor me” requires a lot of context about ”me” as well as whatmovies are available and which ones I have not watched.Context length is especially important for RAG, where largeportions of text might be retrieved and injected into the promptfor generation (see section IV-C.The longer the context length, the more tokens we cansqueeze into the context. The more information the model hasaccess to, the better its response will be. But on the otherhand, with very long context, it would be hard for the modelto remember everything and efficiently process all the informa-tion. Attention-based models are highly inefficient for longercontexts and that is why we should expect more research indifferent mechanisms that enable processing longer contextsand generally come up with more efficient architectures.That being said, new architectures might not only proposealternatives for the attention mechanism but rather rethink thewhole Transformer architecture. As an early example of this,Monarch Mixer [212] proposes a new architecture that usesthe same sub-quadratic primitive that achieves high hardwareefficiency on GPUs – Monarch matrices – along both sequencelength and model dimension.On the other end of the spectrum, it is worth mentioningthat there are some attention-compatible architectural mecha-nisms that have been recently gaining steam and proving theirvalue in creating better and more powerful LLMs. Probablythe best example of such mechanism is Mixture of Experts(MoE). MoEs have been around in machine learning for years,even before the Deep Learning Era [213], but they have beengaining popularity since then, and particularly in the contextof Transformer models and LLMs.In LLMs, MoEs allow to train an extremely large modelthan is then only partially instantiated during inferencewhen some of the experts are turned off wherever the gat-ing/weighting function has a low weight assigned to them. Asan example, the GLaM model has 1.2 trillion parameters, butduring inference only 2 out of the 64 experts are used [84].MoEs are nowadays an important component of the so-called frontier LLMs (i.e. the most advanced and capablemodels). GPT-4 itself is rumored to be based on a MoEarchitecture, and some of the best performing LLMs such asMixtral [117], are basically an MoE version of pre-existingLLMs.Finally, it is important to note that MoEs can be used as acomponent of any architecture regardless of whether it is basedon attention or not. In fact, MoEs have also been applied toSSM-based LLMs like Mamba citepioro2024moemamba. Weshould continue to see MoE-driven improvements in the futureregardless of the underlying architecture.C. Multi-modal ModelsFuture LLMs are expected to be multi-modal and handlea variety of data types, such as text, images, and videos,audio, in a unified manner. This opens up possibilities formore diverse applications in fields like question answering,content generation, creative arts, and healthcare, robotics, andbeyond. There are already several prominent multi-modalLLMs out there, including: LLAVA [214], LLAVA-Plus [215],GPT-4 [33], Qwen-vl [116], Next-GPT [216], but the trend isexpected to be continued. Evaluation of these models also is anew research topic, especially conversational generative visionmodels [217]. Multi-modal LLMs can unlock huge potentialsin a variety of tasks, and there has already been a descentprogress in this direction, which needs a dedicated paper todiscuss all its details.D. Improved LLM Usage and Augmentation techniquesAs we described in sectionIV, many of the shortcomingsand limitations of LLMs such as hallucination can be ad-dressed through advanced prompt engineering, use of tools,or other augmentation techniques. We should expect not onlycontinued, but accelerated research in this area. It is worthmentioning that, in the specific case of software engineering,some works ([218]) tried to automatically eliminate this issuefrom the overall software engineering workflowLLM-based systems are already starting to replace ma-chine learning systems that were until recently using otherapproaches. As a clear example of this, LLMs are now beingdeployed to better understand people preference and interests,and provide more personalized interactions, whether in cus-tomer service, content recommendation, or other applications.This involves better understanding of user preferences, andanalyzing their past interactions and using them as the context.We will continue to see research in the application and usageof LLMs for not only personalization and recommendations,but many other application areas using other machine learningtechniques.Finally, another important area of research we expect togather increased attention is that of LLM-based agents andmulti-agent systems [172], [173], [174]. The development ofLLM systems with access to external tools and decision-making capabilities is both exciting and challenging. We willsee continued research and progress in this important area thatsome argue could lead to Artificial General Intelligence (AGI).E. Security and Ethical/Responsible AIEnsuring the robustness and security of LLMs againstadversarial attacks and other vulnerabilities is a critical areaof research [219]. As LLMs are increasingly deployed in real-world applications, they need to be protected from potentialthreats, to prevent them being used to manipulate people orspread mis-information.Addressing ethical concerns and biases in LLMs is anotheractive area of research. Efforts are being made to ensure thatLLMs are fair, unbiased, and capable of handling sensitiveinformation responsibly. As LLMs are being used more andmore by a large number of people on a daily basis, makingsure they are unbiased and behave responsibly is crucial.VIII. CONCLUSIONThis paper present a survey of LLMs developed in thepast few years. We first provide an overview of early pre-trained language models (e.g., as BERT), then review threepopular LLM families (GPT, LLaMA, PaLM), and otherrepresentative LLMs. We then survey methods and techniquesof building, augmenting, and using LLMs. We review popularLLM datasets and benchmarks, and compare performance ofa set of prominent models on public benchmarks. Finally, wepresent open challenges and future research directions.REFERENCES[1] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling lawsfor neural language models,” arXiv preprint arXiv:2001.08361, 2020.[2] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clarket al., “Training compute-optimal large language models,” arXivpreprint arXiv:2203.15556, 2022.[3] C. E. Shannon, “Prediction and entropy of printed english,” Bell systemtechnical journal, vol. 30, no. 1, pp. 50–64, 1951.[4] F. Jelinek, Statistical methods for speech recognition. MIT press,1998.[5] C. Manning and H. Schutze, Foundations of statistical natural lan-guage processing. MIT press, 1999.[6] C. D. Manning, An introduction to information retrieval. Cambridgeuniversity press, 2009.[7] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,B. Zhang, J. Zhang, Z. Dong et al., “A survey of large languagemodels,” arXiv preprint arXiv:2303.18223, 2023.[8] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,L. He et al., “A comprehensive survey on pretrained foundation mod-els: A history from bert to chatgpt,” arXiv preprint arXiv:2302.09419,2023.[9] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-train, prompt, and predict: A systematic survey of prompting methodsin natural language processing,” ACM Computing Surveys, vol. 55,no. 9, pp. 1–35, 2023.[10] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,J. Xu, and Z. Sui, “A survey for in-context learning,” arXiv preprintarXiv:2301.00234, 2022.[11] J. Huang and K. C.-C. Chang, “Towards reasoning in large languagemodels: A survey,” arXiv preprint arXiv:2212.10403, 2022.[12] S. F. Chen and J. Goodman, “An empirical study of smoothingtechniques for language modeling,” Computer Speech & Language,vol. 13, no. 4, pp. 359–394, 1999.[13] Y. Bengio, R. Ducharme, and P. Vincent, “A neural probabilisticlanguage model,” Advances in neural information processing systems,vol. 13, 2000.[14] H. Schwenk, D. Déchelotte, and J.-L. Gauvain, “Continuous spacelanguage models for statistical machine translation,” in Proceedingsof the COLING/ACL 2006 Main Conference Poster Sessions, 2006,pp. 723–730.[15] T. Mikolov, M. Karafiát, L. Burget, J. Cernockỳ, and S. Khudanpur,“Recurrent neural network based language model.” in Interspeech,vol. 2, no. 3. Makuhari, 2010, pp. 1045–1048.[16] A. Graves, “Generating sequences with recurrent neural networks,”arXiv preprint arXiv:1308.0850, 2013.[17] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck, “Learningdeep structured semantic models for web search using clickthroughdata,” in Proceedings of the 22nd ACM international conference onInformation & Knowledge Management, 2013, pp. 2333–2338.[18] J. Gao, C. Xiong, P. Bennett, and N. Craswell, Neural Approaches toConversational Information Retrieval. Springer Nature, 2023, vol. 44.[19] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learningwith neural networks,” Advances in neural information processingsystems, vol. 27, 2014.[20] K. Cho, B. Van Merriënboer, D. Bahdanau, and Y. Bengio, “Onthe properties of neural machine translation: Encoder-decoder ap-proaches,” arXiv preprint arXiv:1409.1259, 2014.[21] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dollár,J. Gao, X. He, M. Mitchell, J. C. Platt et al., “From captions tovisual concepts and back,” in Proceedings of the IEEE conferenceon computer vision and pattern recognition, 2015, pp. 1473–1482.[22] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell:A neural image caption generator,” in Proceedings of the IEEEconference on computer vision and pattern recognition, 2015, pp.3156–3164.[23] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,and L. Zettlemoyer, “Deep contextualized word representations. corrabs/1802.05365 (2018),” arXiv preprint arXiv:1802.05365, 2018.[24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-trainingof deep bidirectional transformers for language understanding,” arXivpreprint arXiv:1810.04805, 2018.[25] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bertpretraining approach,” arXiv preprint arXiv:1907.11692, 2019.[26] P. He, X. Liu, J. Gao, and W. Chen, “Deberta: Decoding-enhanced bertwith disentangled attention,” arXiv preprint arXiv:2006.03654, 2020.[27] X. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu, Y. Yao,A. Zhang, L. Zhang et al., “Pre-trained models: Past, present andfuture,” AI Open, vol. 2, pp. 225–250, 2021.[28] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, “Pre-trainedmodels for natural language processing: A survey,” Science ChinaTechnological Sciences, vol. 63, no. 10, pp. 1872–1897, 2020.[29] A. Gu, K. Goel, and C. Ré, “Efficiently modeling long sequences withstructured state spaces,” 2022.[30] A. Gu and T. Dao, “Mamba: Linear-time sequence modeling withselective state spaces,” arXiv preprint arXiv:2312.00752, 2023.[31] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al.,“Palm: Scaling language modeling with pathways,” arXiv preprintarXiv:2204.02311, 2022.[32] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar et al., “Llama:Open and efficient foundation language models,” arXiv preprintarXiv:2302.13971, 2023.[33] OpenAI, “GPT-4 Technical Report,” https://arxiv.org/pdf/2303.08774v3.pdf, 2023.[34] J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter,F. Xia, E. Chi, Q. V. Le, and D. Zhou, “Chain-of-thoughtprompting elicits reasoning in large language models,” inAdvances in Neural Information Processing Systems, S. Koyejo,S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,Eds., vol. 35. Curran Associates, Inc., 2022, pp. 24 824–24 837.[Online]. Available: https://proceedings.neurips.cc/paper files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf[35] G. Mialon, R. Dessı̀, M. Lomeli, C. Nalmpantis, R. Pasunuru,R. Raileanu, B. Rozière, T. Schick, J. Dwivedi-Yu, A. Celikyil-maz et al., “Augmented language models: a survey,” arXiv preprintarXiv:2302.07842, 2023.[36] B. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang,L. Liden, Z. Yu, W. Chen, and J. Gao, “Check your facts and tryagain: Improving large language models with external knowledge andautomated feedback,” arXiv preprint arXiv:2302.12813, 2023.[37] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,“React: Synergizing reasoning and acting in language models,” arXivpreprint arXiv:2210.03629, 2022.[38] D. E. Rumelhart, G. E. Hinton, R. J. Williams et al., “Learning internalrepresentations by error propagation,” 1985.[39] J. L. Elman, “Finding structure in time,” Cognitive science, vol. 14,no. 2, pp. 179–211, 1990.[40] M. V. Mahoney, “Fast text compression with neural networks.” inFLAIRS conference, 2000, pp. 230–234.[41] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. Černockỳ, “Strate-gies for training large scale neural network language models,” in 2011IEEE Workshop on Automatic Speech Recognition & Understanding.IEEE, 2011, pp. 196–201.[42] tmikolov. rnnlm. [Online]. Available: https://www.fit.vutbr.cz/∼imikolov/rnnlm/[43] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu,and J. Gao, “Deep learning–based text classification: a comprehensivereview,” ACM computing surveys (CSUR), vol. 54, no. 3, pp. 1–40,2021.[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”Advances in neural information processing systems, vol. 30, 2017.[45] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,“Albert: A lite bert for self-supervised learning of language represen-tations,” arXiv preprint arXiv:1909.11942, 2019.[46] K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, “Electra: Pre-training text encoders as discriminators rather than generators,” arXivpreprint arXiv:2003.10555, 2020.[47] G. Lample and A. Conneau, “Cross-lingual language model pretrain-ing,” arXiv preprint arXiv:1901.07291, 2019.[48] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, andQ. V. Le, “Xlnet: Generalized autoregressive pretraining for languageunderstanding,” Advances in neural information processing systems,vol. 32, 2019.[49] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao,M. Zhou, and H.-W. Hon, “Unified language model pre-training fornatural language understanding and generation,” Advances in neuralinformation processing systems, vol. 32, 2019.[50] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., “Improv-ing language understanding by generative pre-training,” 2018.[51] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,“Language models are unsupervised multitask learners,” OpenAI blog,vol. 1, no. 8, p. 9, 2019.[52] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learningwith a unified text-to-text transformer,” The Journal of MachineLearning Research, vol. 21, no. 1, pp. 5485–5551, 2020.[53] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,A. Barua, and C. Raffel, “mt5: A massively multilingual pre-trainedtext-to-text transformer,” arXiv preprint arXiv:2010.11934, 2020.[54] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu, “Mass: Maskedsequence to sequence pre-training for language generation,” arXivpreprint arXiv:1905.02450, 2019.[55] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,V. Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, andcomprehension,” arXiv preprint arXiv:1910.13461, 2019.[56] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod-els are few-shot learners,” Advances in neural information processingsystems, vol. 33, pp. 1877–1901, 2020.[57] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Ka-plan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al.,“Evaluating large language models trained on code,” arXiv preprintarXiv:2107.03374, 2021.[58] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,C. Hesse, S. Jain, V. Kosaraju, W. Saunders et al., “Webgpt: Browser-assisted question-answering with human feedback,” arXiv preprintarXiv:2112.09332, 2021.[59] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,C. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training languagemodels to follow instructions with human feedback,” Advances inNeural Information Processing Systems, vol. 35, pp. 27 730–27 744,2022.[60] OpenAI. (2022) Introducing chatgpt. [Online]. Available: https://openai.com/blog/chatgpt[61] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., “Llama2: Open foundation and fine-tuned chat models,” arXiv preprintarXiv:2307.09288, 2023.[62] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang,and T. B. Hashimoto, “Alpaca: A strong, replicable instruction-following model,” Stanford Center for Research on Foundation Mod-els. https://crfm. stanford. edu/2023/03/13/alpaca. html, vol. 3, no. 6,p. 7, 2023.[63] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Ef-ficient finetuning of quantized llms,” arXiv preprint arXiv:2305.14314,2023.[64] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,and D. Song, “Koala: A dialogue model for academic research,” Blogpost, April, vol. 1, 2023.[65] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al.,“Mistral 7b,” arXiv preprint arXiv:2310.06825, 2023.[66] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi,J. Liu, T. Remez, J. Rapin et al., “Code llama: Open foundation modelsfor code,” arXiv preprint arXiv:2308.12950, 2023.[67] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, “Gorilla: Largelanguage model connected with massive apis,” 2023.[68] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, andS. Naidu, “Giraffe: Adventures in expanding context lengths in llms,”arXiv preprint arXiv:2308.10882, 2023.[69] B. Huang, “Vigogne: French instruction-following and chat models,”https://github.com/bofenghuang/vigogne, 2023.[70] Y. Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. R. Chandu,D. Wadden, K. MacMillan, N. A. Smith, I. Beltagy et al., “How far cancamels go? exploring the state of instruction tuning on open resources,”arXiv preprint arXiv:2306.04751, 2023.[71] S. Tworkowski, K. Staniszewski, M. Pacek, Y. Wu, H. Michalewski,and P. Miłoś, “Focused transformer: Contrastive training for contextscaling,” arXiv preprint arXiv:2307.03170, 2023.[72] D. Mahan, R. Carlow, L. Castricato, N. Cooper,and C. Laforte, “Stable beluga models.” [Online].Available: [https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)[73] Y. Tay, J. Wei, H. W. Chung, V. Q. Tran, D. R. So, S. Shakeri, X. Gar-cia, H. S. Zheng, J. Rao, A. Chowdhery et al., “Transcending scalinglaws with 0.1% extra compute,” arXiv preprint arXiv:2210.11399,2022.[74] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus,Y. Li, X. Wang, M. Dehghani, S. Brahma et al., “Scaling instruction-finetuned language models,” arXiv preprint arXiv:2210.11416, 2022.[75] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,S. Shakeri, E. Taropa, P. Bailey, Z. Chen et al., “Palm 2 technicalreport,” arXiv preprint arXiv:2305.10403, 2023.[76] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung,N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al., “Large languagemodels encode clinical knowledge,” arXiv preprint arXiv:2212.13138,2022.[77] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou,K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal et al., “Towards expert-level medical question answering with large language models,” arXivpreprint arXiv:2305.09617, 2023.[78] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,A. M. Dai, and Q. V. Le, “Finetuned language models are zero-shotlearners,” arXiv preprint arXiv:2109.01652, 2021.[79] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,J. Aslanides, S. Henderson, R. Ring, S. Young et al., “Scaling languagemodels: Methods, analysis & insights from training gopher,” arXivpreprint arXiv:2112.11446, 2021.[80] V. Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,A. Chaffin, A. Stiegler, T. L. Scao, A. Raja et al., “Multi-task prompted training enables zero-shot task generalization,” arXivpreprint arXiv:2110.08207, 2021.[81] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,Y. Zhao, Y. Lu et al., “Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation,” arXiv preprintarXiv:2107.02137, 2021.[82] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Mil-lican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clarket al., “Improving language models by retrieving from trillions oftokens,” in International conference on machine learning. PMLR,2022, pp. 2206–2240.[83] O. Lieber, O. Sharir, B. Lenz, and Y. Shoham, “Jurassic-1: Technicaldetails and evaluation,” White Paper. AI21 Labs, vol. 1, p. 9, 2021.[84] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun,Y. Zhou, A. W. Yu, O. Firat et al., “Glam: Efficient scaling oflanguage models with mixture-of-experts,” in International Conferenceon Machine Learning. PMLR, 2022, pp. 5547–5569.[85] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du et al., “Lamda: Languagemodels for dialog applications,” arXiv preprint arXiv:2201.08239,2022.[86] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen,C. Dewan, M. Diab, X. Li, X. V. Lin et al., “Opt: Open pre-trainedtransformer language models,” arXiv preprint arXiv:2205.01068, 2022.[87] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Sar-avia, A. Poulton, V. Kerkez, and R. Stojnic, “Galactica: A largelanguage model for science,” arXiv preprint arXiv:2211.09085, 2022.[88] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou,S. Savarese, and C. Xiong, “Codegen: An open large languagemodel for code with multi-turn program synthesis,” arXiv preprintarXiv:2203.13474, 2022.[89] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza,H. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky et al.,“Alexatm 20b: Few-shot learning using a large-scale multilingualseq2seq model,” arXiv preprint arXiv:2208.01448, 2022.[90] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu,T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker et al.,“Improving alignment of dialogue agents via targeted human judge-ments,” arXiv preprint arXiv:2209.14375, 2022.[91] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski,V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo et al.,“Solving quantitative reasoning problems with language models,”Advances in Neural Information Processing Systems, vol. 35, pp.3843–3857, 2022.[92] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, D. Bahri, T. Schuster,H. S. Zheng, N. Houlsby, and D. Metzler, “Unifying language learningparadigms,” arXiv preprint arXiv:2205.05131, 2022.[93] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow,R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé et al., “Bloom: A 176b-parameter open-access multilingual language model,” arXiv preprintarXiv:2211.05100, 2022.[94] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y. Xu,W. Zheng, X. Xia et al., “Glm-130b: An open bilingual pre-trainedmodel,” arXiv preprint arXiv:2210.02414, 2022.[95] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O’Brien,E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff et al.,“Pythia: A suite for analyzing large language models across train-ing and scaling,” in International Conference on Machine Learning.PMLR, 2023, pp. 2397–2430.[96] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, andA. Awadallah, “Orca: Progressive learning from complex explanationtraces of gpt-4,” arXiv preprint arXiv:2306.02707, 2023.[97] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou,M. Marone, C. Akiki, J. Li, J. Chim et al., “Starcoder: may the sourcebe with you!” arXiv preprint arXiv:2305.06161, 2023.[98] S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv,L. Cui, O. K. Mohammed, Q. Liu et al., “Language is not all youneed: Aligning perception with language models,” arXiv preprintarXiv:2302.14045, 2023.[99] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,J. Schalkwyk, A. M. Dai, A. Hauth et al., “Gemini: a family of highlycapable multimodal models,” arXiv preprint arXiv:2312.11805, 2023.[100] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,J. Tompson, I. Mordatch, Y. Chebotar et al., “Inner monologue:Embodied reasoning through planning with language models,” arXivpreprint arXiv:2207.05608, 2022.[101] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikantiet al., “Using deepspeed and megatron to train megatron-turingnlg 530b, a large-scale generative language model,” arXiv preprintarXiv:2201.11990, 2022.[102] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-document transformer,” arXiv preprint arXiv:2004.05150, 2020.[103] S. Iyer, X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shus-ter, T. Wang, Q. Liu, P. S. Koura et al., “Opt-iml: Scaling languagemodel instruction meta learning through the lens of generalization,”arXiv preprint arXiv:2212.12017, 2022.[104] Y. Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma,and F. Wei, “Language models are general-purpose interfaces,” arXivpreprint arXiv:2206.06336, 2022.[105] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang,and C. Gan, “Principle-driven self-alignment of language mod-els from scratch with minimal human supervision,” arXiv preprintarXiv:2305.03047, 2023.[106] W. E. team, “Palmyra-base Parameter Autoregressive LanguageModel,” https://dev.writer.com, 2023.[107] ——, “Camel-5b instructgpt,” https://dev.writer.com, 2023.[108] Yandex. Yalm. [Online]. Available: https://github.com/yandex/YaLM-100B[109] M. Team et al., “Introducing mpt-7b: a new standard for open-source,commercially usable llms,” 2023.[110] A. Mitra, L. D. Corro, S. Mahajan, A. Codas, C. Simoes, S. Agarwal,X. Chen, A. Razdaibiedina, E. Jones, K. Aggarwal, H. Palangi,G. Zheng, C. Rosset, H. Khanpour, and A. Awadallah, “Orca 2:Teaching small language models how to reason,” 2023.[111] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, andG. Neubig, “Pal: Program-aided language models,” in InternationalConference on Machine Learning. PMLR, 2023, pp. 10 764–10 799.[112] Anthropic. claude. [Online]. Available: https://www.anthropic.com/news/introducing-claude[113] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou,“Codegen2: Lessons for training llms on programming and naturallanguages,” arXiv preprint arXiv:2305.02309, 2023.[114] L. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y. Belkada,S. Huang, L. von Werra, C. Fourrier, N. Habib et al., “Zephyr: Directdistillation of lm alignment,” arXiv preprint arXiv:2310.16944, 2023.[115] X. team. Grok. [Online]. Available: https://grok.x.ai/[116] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou,and J. Zhou, “Qwen-vl: A frontier large vision-language model withversatile abilities,” arXiv preprint arXiv:2308.12966, 2023.[117] mixtral. mixtral. [Online]. Available: https://mistral.ai/news/mixtral-of-experts/[118] D. Wang, N. Raman, M. Sibue, Z. Ma, P. Babkin, S. Kaur, Y. Pei,A. Nourbakhsh, and X. Liu, “Docllm: A layout-aware generativelanguage model for multimodal document understanding,” 2023.[119] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi,Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang, “Deepseek-coder:When the large language model meets programming – the rise of codeintelligence,” 2024.[120] F. Wan, X. Huang, D. Cai, X. Quan, W. Bi, and S. Shi, “Knowledgefusion of large language models,” 2024.[121] P. Zhang, G. Zeng, T. Wang, and W. Lu, “Tinyllama: An open-sourcesmall language model,” 2024.[122] C. Wu, Y. Gan, Y. Ge, Z. Lu, J. Wang, Y. Feng, P. Luo, and Y. Shan,“Llama pro: Progressive llama with block expansion,” 2024.[123] X. Amatriain, A. Sankar, J. Bing, P. K. Bodigutla, T. J. Hazen, andM. Kazi, “Transformer models: an introduction and catalog,” 2023.[124] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli,H. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay, “The refined-web dataset for falcon llm: outperforming curated corpora with webdata, and web data only,” arXiv preprint arXiv:2306.01116, 2023.[125] D. Hernandez, T. Brown, T. Conerly, N. DasSarma, D. Drain, S. El-Showk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume et al.,“Scaling laws and interpretability of learning from repeated data,”arXiv preprint arXiv:2205.10487, 2022.[126] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relativeposition representations,” arXiv preprint arXiv:1803.02155, 2018.[127] J. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu, “Roformer: En-hanced transformer with rotary position embedding,” arXiv preprintarXiv:2104.09864, 2021.[128] O. Press, N. A. Smith, and M. Lewis, “Train short, test long: Attentionwith linear biases enables input length extrapolation,” arXiv preprintarXiv:2108.12409, 2021.[129] G. Ke, D. He, and T.-Y. Liu, “Rethinking positional encoding inlanguage pre-training,” arXiv preprint arXiv:2006.15595, 2020.[130] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,and J. Dean, “Outrageously large neural networks: The sparsely-gatedmixture-of-experts layer,” arXiv preprint arXiv:1701.06538, 2017.[131] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scalingto trillion parameter models with simple and efficient sparsity,” TheJournal of Machine Learning Research, vol. 23, no. 1, pp. 5232–5270,2022.[132] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson,“Parameter-efficient multi-task fine-tuning for transformers via sharedhypernetworks,” 2021.[133] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu,T. Zhang, F. Wu, and G. Wang, “Instruction tuning for large languagemodels: A survey,” 2023.[134] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, “Cross-taskgeneralization via natural language crowdsourcing instructions,” arXivpreprint arXiv:2104.08773, 2021.[135] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi,and H. Hajishirzi, “Self-instruct: Aligning language model with selfgenerated instructions,” arXiv preprint arXiv:2212.10560, 2022.[136] K. Ethayarajh, W. Xu, D. Jurafsky, and D. Kiela. Kto. [Online].Available: https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf[137] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, andD. Amodei, “Deep reinforcement learning from human preferences,”Advances in neural information processing systems, vol. 30, 2017.[138] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V. Car-bune, and A. Rastogi, “Rlaif: Scaling reinforcement learning fromhuman feedback with ai feedback,” arXiv preprint arXiv:2309.00267,2023.[139] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, andC. Finn, “Direct preference optimization: Your language model issecretly a reward model,” arXiv preprint arXiv:2305.18290, 2023.[140] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, “Zero: Memoryoptimizations toward training trillion parameter models,” in SC20: In-ternational Conference for High Performance Computing, Networking,Storage and Analysis. IEEE, 2020, pp. 1–16.[141] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao,X. Cheng, M. Chung, M. Grella, K. K. GV et al., “Rwkv: Reinventingrnns for the transformer era,” arXiv preprint arXiv:2305.13048, 2023.[142] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,and W. Chen, “Lora: Low-rank adaptation of large language models,”arXiv preprint arXiv:2106.09685, 2021.[143] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in aneural network,” arXiv preprint arXiv:1503.02531, 2015.[144] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation:A survey,” International Journal of Computer Vision, vol. 129, pp.1789–1819, 2021.[145] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J.Bang, A. Madotto, and P. Fung, “Survey of hallucination in naturallanguage generation,” ACM Comput. Surv., vol. 55, no. 12, mar 2023.[Online]. Available: https://doi.org/10.1145/3571730[146] N. McKenna, T. Li, L. Cheng, M. J. Hosseini, M. Johnson, andM. Steedman, “Sources of hallucination by large language models oninference tasks,” 2023.[147] C.-Y. Lin, “ROUGE: A package for automatic evaluation ofsummaries,” in Text Summarization Branches Out. Barcelona, Spain:Association for Computational Linguistics, Jul. 2004, pp. 74–81.[Online]. Available: https://aclanthology.org/W04-1013[148] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method forautomatic evaluation of machine translation,” in Proceedings of the40th Annual Meeting of the Association for Computational Linguistics,P. Isabelle, E. Charniak, and D. Lin, Eds. Philadelphia, Pennsylvania,USA: Association for Computational Linguistics, Jul. 2002, pp. 311–318. [Online]. Available: https://aclanthology.org/P02-1040[149] B. Dhingra, M. Faruqui, A. Parikh, M.-W. Chang, D. Das, andW. Cohen, “Handling divergent reference texts when evaluatingtable-to-text generation,” in Proceedings of the 57th Annual Meetingof the Association for Computational Linguistics, A. Korhonen,D. Traum, and L. Màrquez, Eds. Florence, Italy: Associationfor Computational Linguistics, Jul. 2019, pp. 4884–4895. [Online].Available: https://aclanthology.org/P19-1483[150] Z. Wang, X. Wang, B. An, D. Yu, and C. Chen, “Towards faithfulneural table-to-text generation with content-matching constraints,”in Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics, D. Jurafsky, J. Chai, N. Schluter,and J. Tetreault, Eds. Online: Association for ComputationalLinguistics, Jul. 2020, pp. 1072–1086. [Online]. Available: https://aclanthology.org/2020.acl-main.101[151] H. Song, W.-N. Zhang, J. Hu, and T. Liu, “Generating persona consis-tent dialogues by exploiting natural language inference,” Proceedingsof the AAAI Conference on Artificial Intelligence, vol. 34, no. 05, pp.8878–8885, Apr. 2020.[152] O. Honovich, L. Choshen, R. Aharoni, E. Neeman, I. Szpektor,and O. Abend, “q2: Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering,”in Proceedings of the 2021 Conference on Empirical Methods inNatural Language Processing, M.-F. Moens, X. Huang, L. Specia,and S. W.-t. Yih, Eds. Online and Punta Cana, Dominican Republic:Association for Computational Linguistics, Nov. 2021, pp. 7856–7870.[Online]. Available: https://aclanthology.org/2021.emnlp-main.619[153] N. Dziri, H. Rashkin, T. Linzen, and D. Reitter, “Evaluating attributionin dialogue systems: The BEGIN benchmark,” Transactions of theAssociation for Computational Linguistics, vol. 10, pp. 1066–1083,2022. [Online]. Available: https://aclanthology.org/2022.tacl-1.62[154] S. Santhanam, B. Hedayatnia, S. Gella, A. Padmakumar, S. Kim,Y. Liu, and D. Z. Hakkani-Tür, “Rome was built in 1776: A case studyon factual correctness in knowledge-grounded response generation,”ArXiv, vol. abs/2110.05456, 2021.[155] S. Min, K. Krishna, X. Lyu, M. Lewis, W. tau Yih, P. W. Koh, M. Iyyer,L. Zettlemoyer, and H. Hajishirzi, “Factscore: Fine-grained atomicevaluation of factual precision in long form text generation,” 2023.[156] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner,V. Chaudhary, and M. Young, “Machine learning: The high interestcredit card of technical debt,” in SE4ML: Software Engineering forMachine Learning (NIPS 2014 Workshop), 2014.[157] Z. Zhang, A. Zhang, M. Li, and A. Smola, “Automatic chain of thoughtprompting in large language models,” 2022.[158] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, andK. Narasimhan, “Tree of thoughts: Deliberate problem solving withlarge language models,” 2023.[159] P. Manakul, A. Liusie, and M. J. F. Gales, “Selfcheckgpt: Zero-resource black-box hallucination detection for generative large lan-guage models,” 2023.[160] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan,and S. Yao, “Reflexion: Language agents with verbal reinforcementlearning,” 2023.[161] S. J. Zhang, S. Florin, A. N. Lee, E. Niknafs, A. Marginean, A. Wang,K. Tyser, Z. Chin, Y. Hicke, N. Singh, M. Udell, Y. Kim, T. Buonassisi,A. Solar-Lezama, and I. Drori, “Exploring the mit mathematics andeecs curriculum using large language models,” 2023.[162] T. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and C. J.Cai, “Promptchainer: Chaining large language model prompts throughvisual programming,” 2022.[163] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, andJ. Ba, “Large language models are human-level prompt engineers,”2023.[164] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin,N. Goyal, H. Küttler, M. Lewis, W. Yih, T. Rocktäschel, S. Riedel, andD. Kiela, “Retrieval-augmented generation for knowledge-intensiveNLP tasks,” CoRR, vol. abs/2005.11401, 2020. [Online]. Available:https://arxiv.org/abs/2005.11401[165] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, andH. Wang, “Retrieval-augmented generation for large language models:A survey,” arXiv preprint arXiv:2312.10997, 2023.[166] A. W. Services. (Year of publication, e.g., 2023) Question answeringusing retrieval augmented generation with foundation models inamazon sagemaker jumpstart. Accessed: Date of access, e.g.,December 5, 2023. [Online]. Available: https://shorturl.at/dSV47[167] S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, “Unifying largelanguage models and knowledge graphs: A roadmap,” arXiv preprintarXiv:2306.08302, 2023.[168] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang,J. Callan, and G. Neubig, “Active retrieval augmented generation,”2023.[169] T. Schick, J. Dwivedi-Yu, R. Dessı̀, R. Raileanu, M. Lomeli, L. Zettle-moyer, N. Cancedda, and T. Scialom, “Toolformer: Language modelscan teach themselves to use tools,” 2023.[170] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer,and M. T. Ribeiro, “Art: Automatic multi-step reasoning and tool-usefor large language models,” 2023.[171] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, “Hugginggpt:Solving ai tasks with chatgpt and its friends in huggingface,” arXivpreprint arXiv:2303.17580, 2023.[172] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang,S. Jin, E. Zhou et al., “The rise and potential of large language modelbased agents: A survey,” arXiv preprint arXiv:2309.07864, 2023.[173] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,J. Tang, X. Chen, Y. Lin et al., “A survey on large language modelbased autonomous agents,” arXiv preprint arXiv:2308.11432, 2023.[174] Z. Durante, Q. Huang, N. Wake, R. Gong, J. S. Park, B. Sarkar,R. Taori, Y. Noda, D. Terzopoulos, Y. Choi, K. Ikeuchi, H. Vo, L. Fei-Fei, and J. Gao, “Agent ai: Surveying the horizons of multimodalinteraction,” arXiv preprint arXiv:2401.03568, 2024.[175] B. Xu, Z. Peng, B. Lei, S. Mukherjee, Y. Liu, and D. Xu, “Rewoo:Decoupling reasoning from observations for efficient augmented lan-guage models,” 2023.[176] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao,“React: Synergizing reasoning and acting in language models,” 2023.[177] V. Nair, E. Schumacher, G. Tso, and A. Kannan, “Dera: Enhanc-ing large language model completions with dialog-enabled resolvingagents,” 2023.[178] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi,C. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang,and X. Xie, “A survey on evaluation of large language models,” 2023.[179] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova,L. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit,Q. Le, and S. Petrov, “Natural questions: A benchmark forquestion answering research,” Transactions of the Association forComputational Linguistics, vol. 7, pp. 452–466, 2019. [Online].Available: https://aclanthology.org/Q19-1026[180] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, andJ. Steinhardt, “Measuring massive multitask language understanding,”2021.[181] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,E. Jiang, C. Cai, M. Terry, Q. Le et al., “Program synthesis with largelanguage models,” arXiv preprint arXiv:2108.07732, 2021.[182] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y. Choi, P. Liang,and L. Zettlemoyer, “QuAC: Question answering in context,” inProceedings of the 2018 Conference on Empirical Methods in NaturalLanguage Processing, E. Riloff, D. Chiang, J. Hockenmaier, andJ. Tsujii, Eds. Brussels, Belgium: Association for ComputationalLinguistics, Oct.-Nov. 2018, pp. 2174–2184. [Online]. Available:https://aclanthology.org/D18-1241[183] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt, “Measuringcoding challenge competence with apps,” NeurIPS, 2021.[184] V. Zhong, C. Xiong, and R. Socher, “Seq2sql: Generating structuredqueries from natural language using reinforcement learning,” arXivpreprint arXiv:1709.00103, 2017.[185] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:A large scale distantly supervised challenge dataset for readingcomprehension,” in Proceedings of the 55th Annual Meeting of theAssociation for Computational Linguistics (Volume 1: Long Papers),R. Barzilay and M.-Y. Kan, Eds. Vancouver, Canada: Associationfor Computational Linguistics, Jul. 2017, pp. 1601–1611. [Online].Available: https://aclanthology.org/P17-1147[186] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy, “RACE: Large-scaleReAding comprehension dataset from examinations,” in Proceedingsof the 2017 Conference on Empirical Methods in Natural LanguageProcessing, M. Palmer, R. Hwa, and S. Riedel, Eds. Copenhagen,Denmark: Association for Computational Linguistics, Sep. 2017, pp.785–794. [Online]. Available: https://aclanthology.org/D17-1082[187] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “SQuAD: 100,000+questions for machine comprehension of text,” in Proceedings ofthe 2016 Conference on Empirical Methods in Natural LanguageProcessing, J. Su, K. Duh, and X. Carreras, Eds. Austin, Texas:Association for Computational Linguistics, Nov. 2016, pp. 2383–2392.[Online]. Available: https://aclanthology.org/D16-1264[188] C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, andK. Toutanova, “Boolq: Exploring the surprising difficulty of naturalyes/no questions,” CoRR, vol. abs/1905.10044, 2019. [Online].Available: http://arxiv.org/abs/1905.10044[189] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth,“Looking beyond the surface:a challenge set for reading compre-hension over multiple sentences,” in Proceedings of North AmericanChapter of the Association for Computational Linguistics (NAACL),2018.[190] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, andJ. Schulman, “Training verifiers to solve math word problems,”CoRR, vol. abs/2110.14168, 2021. [Online]. Available: https://arxiv.org/abs/2110.14168[191] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang,D. Song, and J. Steinhardt, “Measuring mathematical problem solvingwith the MATH dataset,” CoRR, vol. abs/2103.03874, 2021. [Online].Available: https://arxiv.org/abs/2103.03874[192] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hellaswag:Can a machine really finish your sentence?” 2019.[193] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,and O. Tafjord, “Think you have solved question answering? tryarc, the AI2 reasoning challenge,” CoRR, vol. abs/1803.05457, 2018.[Online]. Available: http://arxiv.org/abs/1803.05457[194] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, “PIQA:reasoning about physical commonsense in natural language,” CoRR,vol. abs/1911.11641, 2019. [Online]. Available: http://arxiv.org/abs/1911.11641[195] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi, “Socialiqa:Commonsense reasoning about social interactions,” CoRR, vol.abs/1904.09728, 2019. [Online]. Available: http://arxiv.org/abs/1904.09728[196] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit ofarmor conduct electricity? A new dataset for open book questionanswering,” CoRR, vol. abs/1809.02789, 2018. [Online]. Available:http://arxiv.org/abs/1809.02789[197] S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how modelsmimic human falsehoods,” arXiv preprint arXiv:2109.07958, 2021.[198] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov,and C. D. Manning, “Hotpotqa: A dataset for diverse, explainablemulti-hop question answering,” CoRR, vol. abs/1809.09600, 2018.[Online]. Available: http://arxiv.org/abs/1809.09600[199] Y. Zhuang, Y. Yu, K. Wang, H. Sun, and C. Zhang, “Toolqa: Adataset for llm question answering with external tools,” arXiv preprintarXiv:2306.13304, 2023.[200] D. Chen, J. Bolton, and C. D. Manning, “A thorough examinationof the cnn/daily mail reading comprehension task,” in Association forComputational Linguistics (ACL), 2016.[201] R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang et al., “Abstractive textsummarization using sequence-to-sequence rnns and beyond,” arXivpreprint arXiv:1602.06023, 2016.[202] Y. Bai and D. Z. Wang, “More than reading comprehension: A surveyon datasets and metrics of textual question answering,” arXiv preprintarXiv:2109.12264, 2021.[203] H.-Y. Huang, E. Choi, and W.-t. Yih, “Flowqa: Grasping flow inhistory for conversational machine comprehension,” arXiv preprintarXiv:1810.06683, 2018.[204] S. Lee, J. Lee, H. Moon, C. Park, J. Seo, S. Eo, S. Koo, and H. Lim, “Asurvey on evaluation metrics for machine translation,” Mathematics,vol. 11, no. 4, p. 1006, 2023.[205] J. Li, X. Cheng, W. X. Zhao, J.-Y. Nie, and J.-R. Wen, “Halueval:A large-scale hallucination evaluation benchmark for large languagemodels,” in Proceedings of the 2023 Conference on Empirical Methodsin Natural Language Processing, 2023, pp. 6449–6464.[206] Simon Mark Hughes, “Hughes hallucination evaluation model(hhem) leaderboard,” 2024, https://huggingface.co/spaces/vectara/Hallucination-evaluation-leaderboard, Last accessed on 2024-01-21.[207] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, andR. McHardy, “Challenges and applications of large language models,”arXiv preprint arXiv:2307.10169, 2023.[208] S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. Del Giorno,S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi et al.,“Textbooks are all you need,” arXiv preprint arXiv:2306.11644, 2023.[209] Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T.Lee, “Textbooks are all you need ii: phi-1.5 technical report,” arXivpreprint arXiv:2309.05463, 2023.[210] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus,Y. Bengio, S. Ermon, and C. Ré, “Hyena hierarchy: Towards largerconvolutional language models,” 2023.[211] M. Poli, J. Wang, S. Massaroli, J. Quesnelle, E. Nguyen, andA. Thomas, “StripedHyena: Moving Beyond Transformers withHybrid Signal Processing Models,” 12 2023. [Online]. Available:https://github.com/togethercomputer/stripedhyena[212] D. Y. Fu, S. Arora, J. Grogan, I. Johnson, S. Eyuboglu, A. W. Thomas,B. Spector, M. Poli, A. Rudra, and C. Ré, “Monarch mixer: A simplesub-quadratic gemm-based architecture,” 2023.[213] G. J. McLachlan, S. X. Lee, and S. I. Rathnayake, “Finite mixturemodels,” Annual review of statistics and its application, vol. 6, pp.355–378, 2019.[214] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” arXivpreprint arXiv:2304.08485, 2023.[215] S. Liu, H. Cheng, H. Liu, H. Zhang, F. Li, T. Ren, X. Zou,J. Yang, H. Su, J. Zhu, L. Zhang, J. Gao, and C. Li, “Llava-plus:Learning to use tools for creating multimodal agents,” arXiv preprintarXiv:2311.05437, 2023.[216] S. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, “Next-gpt: Any-to-anymultimodal llm,” arXiv preprint arXiv:2309.05519, 2023.[217] N. N. Khasmakhi, M. Asgari-Chenaghlu, N. Asghar, P. Schaer, andD. Zühlke, “Convgenvismo: Evaluation of conversational generativevision models,” 2023.[218] N. Alshahwan, J. Chheda, A. Finegenova, B. Gokkaya, M. Harman,I. Harper, A. Marginean, S. Sengupta, and E. Wang, “Automated unittest improvement using large language models at meta,” arXiv preprintarXiv:2402.09171, 2024.[219] L. Sun, Y. Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y. Huang,W. Lyu, Y. Zhang, X. Li et al., “Trustllm: Trustworthiness in largelanguage models,” arXiv preprint arXiv:2401.05561, 2024.[220] Microsoft. Deepspeed. [Online]. Available: https://github.com/microsoft/DeepSpeed[221] HuggingFace. Transformers. [Online]. Available: https://github.com/huggingface/transformers[222] Nvidia. Megatron. [Online]. Available: https://github.com/NVIDIA/Megatron-LM[223] BMTrain. Bmtrain. [Online]. Available: https://github.com/OpenBMB/BMTrain[224] EleutherAI. gpt-neox. [Online]. Available: https://github.com/EleutherAI/gpt-neox[225] microsoft. Lora. [Online]. Available: https://github.com/microsoft/LoRA[226] ColossalAI. Colossalai. [Online]. Available: https://github.com/hpcaitech/ColossalAI[227] FastChat. Fastchat. [Online]. Available: https://github.com/lm-sys/FastChat[228] skypilot. skypilot. [Online]. Available: https://github.com/skypilot-org/skypilot[229] vllm. vllm. [Online]. Available: https://github.com/vllm-project/vllm[230] huggingface. text-generation-inference. [Online]. Available: https://github.com/huggingface/text-generation-inference[231] langchain. langchain. [Online]. Available: https://github.com/langchain-ai/langchain[232] bentoml. Openllm. [Online]. Available: https://github.com/bentoml/OpenLLM[233] embedchain. embedchain. [Online]. Available: https://github.com/embedchain/embedchain[234] microsoft. autogen. [Online]. Available: https://github.com/microsoft/autogen[235] babyagi. babyagi. [Online]. Available: https://github.com/yoheinakajima/babyagi[236] guidance. guidance. [Online]. Available: https://github.com/guidance-ai/guidance[237] prompttools. prompttools. [Online]. Available: https://github.com/hegelai/prompttools[238] promptfoo. promptfoo. [Online]. Available: https://github.com/promptfoo/promptfoo[239] facebook. faiss. [Online]. Available: https://github.com/facebookresearch/faiss[240] milvus. milvus. [Online]. Available: https://github.com/milvus-io/milvus[241] qdrant. qdrant. [Online]. Available: https://github.com/qdrant/qdrant[242] weaviate. weaviate. [Online]. Available: https://github.com/weaviate/weaviate[243] llama index. llama-index. [Online]. Available: https://github.com/run-llama/llama indexAPPENDIX1. Open Source Toolkits For LLM Development andDeploymentThere are various frameworks and libraries developed forLLM training, evaluation, and deployment, and covering everysingle framework is out of this paper’s scope. But we try toprovide a brief introduction of some of the most popular ones,grouped into different categories.A. LLM Training/Inference FrameworksSome of the popular frameworks which are useful for LLMtraining includes (note that some of them can be used beyondLLM training too):DeepSpeed [220] is a deep learning optimization librarythat makes distributed training and inference easy, efficient,and effective. DeepSpeed enables world’s most powerful lan-guage models like MT-530B and BLOOM. It is an easy-to-use deep learning optimization software suite that powersunprecedented scale and speed for both training and inference.With DeepSpeed you can:Transformers [221] is library by HuggingFace whichprovides thousands of pretrained models to perform tasks ondifferent modalities such as text, vision, and audio. Usingpretrained models one can reduce compute costs, carbonfootprint, and save the time and resources required to traina model from scratch.Megatron-LM [222] is a large, powerful transformerdeveloped by the Applied Deep Learning Research teamat NVIDIA. It contains efficient, model-parallel (tensor, se-quence, and pipeline), and multi-node pre-training of trans-former based models such as GPT, BERT, and T5 using mixedprecision.BMTrain [223] is an efficient large model training toolkitthat can be used to train large models with tens of billions ofparameters. It can train models in a distributed manner whilekeeping the code as simple as stand-alone training.GPT-NeoX [224] leverages many of the same features andtechnologies as the popular Megatron-DeepSpeed library butwith substantially increased usability and novel optimizations.LoRA [225] library provides the support for Low-RankAdaptation of Large Language Models. It reduces the numberof trainable parameters by learning pairs of rank-decompostionmatrices while freezing the original weights. This vastlyreduces the storage requirement for large language modelsadapted to specific tasks and enables efficient task-switchingduring deployment all without introducing inference latency.LoRA also outperforms several other adaptation methods in-cluding adapter, prefix-tuning, and fine-tuning.ColossalAI library [226] provides a collection of parallelcomponents. It aims to support developers to write theirdistributed deep learning models just like how they write theirmodel on their laptop. They provide user-friendly tools tokickstart distributed training and inference in a few lines. Interms of Parallelism strategies, they support: Data Parallelism,Pipeline Parallelism, Sequence Parallelism, Zero RedundancyOptimizer (ZeRO) [140], and Auto-Parallelism.B. Deployment ToolsWe provide an overview of some of the most popular LLMdeployment tools here.FastChat [227] is an open platform for training, serv-ing, and evaluating large language model based chatbots.FastChat’s core features include: The training and evaluationcode for state-of-the-art models (e.g., Vicuna, MT-Bench), anda distributed multi-model serving system with web UI andOpenAI-compatible RESTful APIs.Skypilot [228] is a framework for running LLMs, AI,and batch jobs on any cloud, offering maximum cost savings,highest GPU availability, and managed execution.vLLM [229] is a fast and easy-to-use library for LLM in-ference and serving. vLLM seamlessly supports many HuggingFace models, including the following architectures: Aquila,Baichuan, BLOOM, ChatGLM, DeciLM, Falcon, GPT Big-Code, LLaMA, LLaMA 2, Mistral, Mixtral, MPT, OPT, Qwen,Yi, and many more.text-generation-inference [230] is a toolkit for deployingand serving Large Language Models (LLMs). TGI enableshigh-performance text generation for the most popular open-source LLMs, including Llama, Falcon, StarCoder, BLOOM,GPT-NeoX, and more.LangChain [231] is a framework for developing applica-tions powered by language models. It enables applications that:• Are context-aware: connect a language model tosources of context (prompt instructions, few shot ex-amples, content to ground its response in, etc.)• Reason: rely on a language model to reason (abouthow to answer based on provided context, what ac-tions to take, etc.)OpenLLM [232] is an open-source platform designed tofacilitate the deployment and operation of large language mod-els (LLMs) in real-world applications. With OpenLLM, youcan run inference on any open-source LLM, deploy them onthe cloud or on-premises, and build powerful AI applications.Embedchain [233] is an Open Source RAG Frameworkthat makes it easy to create and deploy AI apps. Embedchainstreamlines the creation of RAG applications, offering a seam-less process for managing various types of unstructured data.It efficiently segments data into manageable chunks, generatesrelevant embeddings, and stores them in a vector database foroptimized retrieval.Autogen [234] is a framework that enables the devel-opment of LLM applications using multiple agents that canconverse with each other to solve tasks. AutoGen agentsare customizable, conversable, and seamlessly allow humanparticipation. They can operate in various modes that employcombinations of LLMs, human inputs, and tools.BabyAGI [235] is an autonomous Artificial Intelligenceagent, that is designed to generate and execute tasks based ongiven objectives. It harnesses cutting-edge technologies fromOpenAI, Pinecone, LangChain, and Chroma to automate tasksand achieve specific goals. In this blog post, we will diveinto the unique features of BabyAGI and explore how it canstreamline task automation.C. Prompting LibrariesGuidance [236] is a programming paradigm that offerssuperior control and efficiency compared to conventionalprompting and chaining. It allows users to constrain generation(e.g. with regex and CFGs) as well as to interleave control(conditional, loops) and generation seamlessly.PromptTools [237] offers a set of open-source, self-hostable tools for experimenting with, testing, and evaluatingLLMs, vector databases, and prompts. The core idea is toenable developers to evaluate using familiar interfaces likecode, notebooks, and a local playground.PromptBench [?] is a Pytorch-based Python package forEvaluation of Large Language Models (LLMs). It providesuser-friendly APIs for researchers to conduct evaluation onLLMs.Promptfoo [238] is a tool for testing and evaluating LLMoutput quality. It systematically test prompts, models, andRAGs with predefined test cases.D. VectorDBFaiss [239] is a library developed by Facebook AI Re-search that provides efficient similarity search and clusteringof dense vectors. It is designed for use with large-scale,high-dimensional data and supports several index types andalgorithms for various use cases.Milvus [240] is an open-source vector database built topower embedding similarity search and AI applications. Mil-vus makes unstructured data search more accessible, and pro-vides a consistent user experience regardless of the deploymentenvironment.Qdrant [241] is a vector similarity search engine andvector database. It provides a production-ready service with aconvenient API to store, search, and manage points—vectorswith an additional payload Qdrant is tailored to extendedfiltering support. environment.Weaviate [242] is an open-source, GraphQL-based vec-tor search engine that enables similarity search on high-dimensional data. While it is open-source, the commercial ver-sion offers additional features, support, and managed services.Some of the other popular options includes LlamaIndex[243] and Pinecone.",
    "Link": "http://arxiv.org/abs/2402.06196"
}