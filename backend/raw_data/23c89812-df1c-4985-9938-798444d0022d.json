{
    "Title": "Pytrec_eval: An Extremely Fast Python Interface to trec_eval",
    "Authors": "Koepke H., ST., Tague J.",
    "Year": "No year available",
    "Abstract": "We introduce pytrec_eval, a Python interface to the tree_eval information\nretrieval evaluation toolkit. pytrec_eval exposes the reference implementations\nof trec_eval within Python as a native extension. We show that pytrec_eval is\naround one order of magnitude faster than invoking trec_eval as a sub process\nfrom within Python. Compared to a native Python implementation of NDCG,\npytrec_eval is twice as fast for practically-sized rankings. Finally, we\ndemonstrate its effectiveness in an application where pytrec_eval is combined\nwith Pyndri and the OpenAI Gym where query expansion is learned using\nQ-learning.Comment: SIGIR '18. The 41st International ACM SIGIR Conference on Research &\n  Development in Information Retrieva",
    "Keywords": "No keywords available",
    "Publisher": "'Association for Computing Machinery (ACM)'",
    "Publication Date": "No publication date available",
    "Journal": "No journal available",
    "Citation Count": 0,
    "Full Text": "Pytrec_eval: An Extremely Fast Python Interface to trec_eval∗\nChristophe Van Gysel\nUniversity of Amsterdam\nAmsterdam, The Netherlands\nchris@stophr.be\nMaarten de Rijke\nUniversity of Amsterdam\nAmsterdam, The Netherlands\nderijke@uva.nl\nABSTRACT\nWe introduce pytrec_eval, a Python interface to the trec_eval\ninformation retrieval evaluation toolkit. pytrec_eval exposes the\nreference implementations of trec_eval within Python as a na-\ntive extension. We show that pytrec_eval is around one order of\nmagnitude faster than invoking trec_eval as a sub process from\nwithin Python. Compared to a native Python implementation of\nNDCG, pytrec_eval is twice as fast for practically-sized rankings.\nFinally, we demonstrate its effectiveness in an application where\npytrec_eval is combined with Pyndri and the OpenAI Gym where\nquery expansion is learned using Q-learning.\nCCS CONCEPTS\n• Information systems→ Evaluation of retrieval results;\nKEYWORDS\nIR evaluation, toolkits\nACM Reference Format:\nChristophe Van Gysel and Maarten de Rijke. 2018. Pytrec_eval: An Ex-\ntremely Fast Python Interface to trec_eval. In SIGIR ’18: The 41st Interna-\ntional ACM SIGIR Conference on Research & Development in Information\nRetrieval, July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA,\n4 pages. https://doi.org/10.1145/3209978.3210065\n1 INTRODUCTION\nEvaluation is a crucial component of any information retrieval (IR)\nsystem [2]. Reusable test collections and off-line evaluation mea-\nsures [7] have been the dominating paradigm for experimentally\nvalidating IR research for the last 30 years. The popularity and\nubiquity of off-line IR evaluation measures is partly due to the Text\nREtrieval Conference (TREC) [5]. TREC led to the development\nof the trec_eval1 software package that is the standard tool for\nevaluating a collection of rankings. The trec_eval tool allows IR re-\nsearchers to easily compute a large number of evaluation measures\nusing standardized input and output formats. For a document col-\nlection, a test collection of queries with query/document relevance\ninformation (i.e., qrel) and a set of rankings generated by a par-\nticular IR system (i.e., a system run) for the test collection queries,\n∗Open-source implementation is available at https://github.com/cvangysel/pytrec_eval.\n1https://github.com/usnistgov/trec_eval\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA\n© 2018 Copyright held by the owner/author(s). Publication rights licensed to Associa-\ntion for Computing Machinery.\nACM ISBN 978-1-4503-5657-2/18/07. . . $15.00\nhttps://doi.org/10.1145/3209978.3210065\ntrec_eval outputs a standardized output format containing eval-\nuation measure values. The adoption of trec_eval as an integral\npart of IR research has led to the following benefits: (a) standardized\nformats for system rankings and query relevance information such\nthat different research groups can exchange experimental results\nwith minimal communication, and (b) open-source reference imple-\nmentations of evaluation measures—provided by a third party (i.e.,\nNIST)—that promotes transparent and consistent evaluation.\nWhile the availability of trec_eval has brought many benefits\nto the IR community, it has the downside that it is available only\nas a standalone executable that is interfaced by passing files with\nrankings and ground truth information. In recent years, the Python\nprogramming language has risen in popularity due to its feature\nrichness (i.e., scientific libraries and data structures) and holistic\nlanguage design [3]. Research progresses at a rate proportional to\nthe time it takes to implement an idea, and consequently, scripting\nlanguages (e.g., Python) are preferred over conventional program-\nming languages [6]. Within IR research, retrieval systems are often\nimplemented and optimized using Python (e.g., [4, 9]) and for their\nevaluation trec_eval is used. However, invoking trec_eval from\nPython is expensive as it involves (1) serializing the internal rank-\ning structures to disk files, (2) invoking trec_eval through the\noperating system, and (3) parsing the trec_eval evaluation output\nfrom the standard output stream. This workflow is unnecessarily\ninefficient as it incurs (a) a double I/O cost when the ranking is\nfirst serialized by the Python script and subsequently parsed by\ntrec_eval, and (b) a context-switching overhead as the invocation\nof trec_eval needs to be processed by the operating system.\nWe introduce pytrec_eval to counter these excessive efficiency\ncosts and avoid a wild growth of ad-hoc Python-based evaluation\nmeasure implementations. pytrec_eval builds upon the trec_eval\nsource code and exposes a Python-first interface to the trec_eval\nevaluation toolkit as a native Python extension. Rankings con-\nstructed in Python can directly be passed to the evaluation proce-\ndure, without incurring disk I/O costs; evaluation is performed us-\ning the original trec_eval implementation. Due to pytrec_eval’s\nimplementation as a native Python extension, context-switching\noverheads are avoided as the evaluation procedure and its invoca-\ntion reside within the same process. Next to improved efficiency,\npytrec_eval brings the following benefits: (a) current and future\nreference trec_eval implementations of IR evaluation measures\nare available within Python, and (b) as the evaluation measures are\nimplemented in C, their execution are typically faster than native\nPython-based alternatives. The main purpose of this paper is to\ndescribe pytrec_eval, provide empirical evidence of the speedup\nthat pytrec_eval delivers, and showcase the use of pytrec_eval\nin a reinforcement learning application. We ask the following ques-\ntions: (RQ1)What speedup do we obtain when using pytrec_eval\nover trec_eval (serialize-invoke-parse workflow)? (RQ2) How\nfast is pytrec_eval compared to native Python implementations\nof IR evaluation measures? We also present a demo application\nthat combines Pyndri [9] and pytrec_eval in a query formulation\nar\nX\niv\n:1\n80\n5.\n01\n59\n7v\n2 \n [c\ns.I\nR]\n  5\n Ju\nn 2\n01\n8\nqrel = {'q1': {'d1': 1, 'd2': 0},\n'q2': {'d2': 1}}\nrun = {'q1': {'d1': 0.5, 'd2': 2.0},\n'q2': {'d1': 0.5, 'd2': 0.6}}\nevaluator = pytrec_eval.RelevanceEvaluator(\nqrel, {'map', 'ndcg'})\nresult = evaluator.evaluate(run)\n# result equals\n# {'q1': {'map': 0.5, 'ndcg': 0.6309297535714575},\n# 'q2': {'map': 1.0, 'ndcg': 1.0}}\nCode snippet 1:Minimal example of how pytrec_eval can be\nused to compute IR evaluation measures. Evaluation mea-\nsures (NDCG, MAP) are computed for two queries—q1 and\nq2—and two documents—d1 and d2—where for q2 we only\nhave partial relevance (d1 is assumed to be non-relevant).\nreinforcement learning setting and provide the environment and\nthe reward signal, integrated within the OpenAI Gym [1].\n2 EVALUATING USING PYTREC_EVAL\nThe pytrec_eval library has a minimalistic design. Its main inter-\nface is the RelevanceEvaluator class. The RelevanceEvaluator\nclass takes as arguments (1) query relevance ground truth, a dic-\ntionary of query identifiers to a dictionary of document identifiers\nand their integral relevance level, and (2) a set of evaluation mea-\nsures to compute (e.g., ndcg, map). Code snippet 1 shows a minimal\nexample on how pytrec_eval can be used to evaluate a ranking.\nRankings are encoded by a mapping from document identifiers to\ntheir retrieval scores. Internally, pytrec_eval sorts the documents\nin decreasing order of retrieval score. This behavior mimics the im-\nplementation of trec_eval, which ignores the order of documents\nwithin the user-provided file, and only considers the document\nscores. Similar to trec_eval, document ties, which occur when\ntwo documents are assigned the same score, are broken by secondar-\nily sorting on document identifier. Query relevance ground truth is\npassed to pytrec_eval in a similar way to document scores, where\nrelevance is encoded as an integer rather than a floating point value.\nBeyond measures computed over the full ranking of documents,\npytrec_eval also supports measures computed up to a particular\nrank k . The values of k are the same as the ones used by trec_eval.\nFor example, measures ndcg_cut and P correspond to NDCG@k\nand precision@k , respectively, with k = 5, 10, 15, 20, 30, 100, 200,\n500, 1000. The set of supported evaluation measures is stored in the\npytrec_eval.supported_measures property and the identifiers\nare the same as used by trec_eval (i.e., running trec_eval with\narguments -m ndcg_cut --help will show documentation for\nthe NDCG@k measure). To mimic the behavior of trec_eval to\ncompute all known evaluation measures (i.e., passing argument\n-m all_trec to trec_eval), just instantiate RelevanceEvaluator\nwith pytrec_eval.supported_measures as the second argument.\n3 BENCHMARK RESULTS\nAs demonstrated above, pytrec_eval conveniently exposes pop-\nular IR evaluation measures within Python. However, the same\nfunctionality could be exposed by invoking trec_eval in a serialize-\ninvoke-parse workflow—or—by implementing the evaluation mea-\nsure natively in Python. In this section we provide empirical bench-\nmark results that show that pytrec_eval, beyond its convenience,\nis also faster at computing evaluation measures than these two\nalternatives (i.e., invoking trec_eval or native Python).\nExperimental setup. For every hyperparameter configuration,\nthe runtime measurement was repeated 20 times and the average\nruntime is reported. Speedup denotes the ratio of the runtime of\nthe alternative method (i.e., trec_eval or native Python) over the\nruntime of pytrec_eval and consequently, a speedup of 1.0means\nthat both methods are equally fast. When invoking trec_eval us-\ning the serialize-invoke-parse workflow, rankings are written from\nPython to storage without sorting, as trec_eval itself sorts the\nrankings internally. The resulting evaluation output is read from\nstdout to a Python string and we do not extract the measure val-\nues, as different parsing strategies can lead to large variance in\nruntime. For the native Python implementation, we experimented\nwith different open-source implementations of the NDCG measure\nand adapted the fastest implementation as our baseline. The imple-\nmentation does not make use of NumPy or other scientific Python\nlibraries as (a) we wish to compare to native Python directly and\n(b) the NumPy-based implementations we experimented with were\nless efficient than the native implementation we settled with, as\nNumPy-based implementations require that the rankings are en-\ncoded in dense arrays before computing evaluation measures. The\nevaluated rankings and ground-truth were synthesized by assign-\ning every document a distinct ranking score in N and a relevance\nlevel of 1. This allows us to evaluate different evaluation measure\nimplementations with rankings and query sets of different sizes.\nExperiments were run using a single Intel Xeon CPU (E5-2630 v3)\nclocked at 2.4GHz, DDR4 RAM clocked at 2.4GHz, an Intel SSD (DC\nS3610) with sequential read/write speeds of 550MB/s and 450MB/s,\nrespectively, and a hard disk drive (Seagate ST2000NX0253) with a\nrotational speed of 7200 rpm. All code used to run our experiments\nis available under the MIT open-source license.2\nResults.We now answer our research questions by comparing the\nruntime performance of pytrec_eval to trec_eval (RQ1) and a\nnative Python implementation (RQ2).\nRQ1 What speedup do we obtain when using pytrec_eval over\ntrec_eval (serialize-invoke-parse workflow)?\nFig. 1 shows matrices of speedups of pytrec_eval over trec_eval\nobtained using different storage types (increasing order of through-\nput capacity): a regular hard disk drive (HDD), a solid state drive\n(SSD) and a memory-mapped file system (tmpfs). For the degen-\nerate case where we have a single query and a single returned\ndocument, we observe that there is a clear difference between the\ndifferent storages. In particular, we can see that tmpfs is faster than\nSSD, and in turn, SSD is faster than the HDD. However, for larger\nconfigurations (upper right box in every grid; 10,000 queries with\n1,000 documents) we see that the difference between the storage\ntypes fades away and that pytrec_eval always achieves a speedup\nof at least 17 over trec_eval. This is because (a) starting the seri-\nalization (e.g., disk seek time) is expensive (as can be seen in the\nleft-lower box of every grid), but that cost is quickly overshadowed\nby (b) the cost of context switching between processes. In the case\nof pytrec_eval, however, context switching is avoided as all logic\nruns as part of the same process. Consequently, we can conclude\nthat pytrec_eval is at least one order of magnitude faster than\ninvoking trec_eval using a serialize-invoke-parse workflow.\n2The benchmark code can be found in the benchmarks sub-directory of the\npytrec_eval repository; see the footnote on the first page.\n1 5 10 20 50 100 500 1000\nDocuments per query\n1\n10\n50\n250\n1000\n5000\n10000\nQ\nue\nrie\ns\n951 780 627 443 229 125 38 28\n229 194 140 96 57 40 22 19\n54 50 49 41 33 25 19 17\n16 24 28 29 27 22 18 18\n14 21 23 19 19 21 18 18\n25 20 19 21 21 22 19 17\n34 34 30 25 24 22 18 18\n(a) HDD\n1 5 10 20 50 100 500 1000\nDocuments per query\n1\n10\n50\n250\n1000\n5000\n10000\nQ\nue\nrie\ns\n781 653 565 389 196 113 32 24\n190 142 106 81 46 35 20 19\n45 44 42 37 30 29 21 18\n15 22 26 29 27 28 18 17\n13 21 26 24 24 23 18 18\n31 30 28 23 21 22 19 18\n49 36 31 26 23 23 18 18\n(b) SSD\n1 5 10 20 50 100 500 1000\nDocuments per query\n1\n10\n50\n250\n1000\n5000\n10000\nQ\nue\nrie\ns\n771 511 525 333 192 102 32 24\n185 144 109 76 47 34 21 19\n45 43 41 37 31 28 20 18\n15 22 26 29 29 27 18 17\n12 21 25 25 24 21 19 17\n34 29 26 23 22 22 19 18\n55 41 31 28 23 23 18 17\n(c) Memory-mapped (tmpfs)\nFigure 1: Speedup of pytrec_eval (down-rounded speedup in each box; runtime measured as average over 20 repetitions) com-\npared to invoking trec_eval using a serialize-invoke-parse workflow (§1) for different numbers of queries, different numbers\nof ranked documents per query, and using different types of storage (hard disk drive, solid state drive and random access\nmemory) for serializing the rankings and query relevance ground truth.\n1 3 5 10 20 30 40 50 102 103 104 105\nNumber of documents\n0\n1\n2\n3\nS\npe\ned\nup\nFigure 2: Speedup of pytrec_eval over a native Python im-\nplementation of the NDCG evaluation measure (we report\naverage speedup and its standard deviation over 20 repeti-\ntions). For practically-sized rankings, pytrec_eval is consis-\ntently faster than the native Python implementation.\nRQ2 How fast is pytrec_eval compared to native Python imple-\nmentations of IR evaluation measures?\nFig. 2 shows the speedup of pytrec_eval over a Python-native\nimplementation of NDCG for a single query and a varying number\nof documents. Here we see that for extremely short rankings (1–3\ndocuments), the native implementation outperforms pytrec_eval.\nHowever, for rankings consisting of 5 documents or more, we can\nsee that pytrec_eval provides a consistent performance boost over\nthe native implementation. The reason for the sub-native perfor-\nmance of pytrec_eval for very short rankings is because—before\npytrec_eval computes evaluation measures—rankings need to\nbe converted into the internal C format used by trec_eval. The\nPython-native implementation does not require this transformation,\nand consequently, can thus be slightly faster when rankings are\nvery short. However, it is important to note that short rankings are\nuncommon in IR and that the average ranking consists of around\n100 to 1,000 documents. We conclude that pytrec_eval is faster\nthan native Python implementations for practically-size rankings.\n4 EXAMPLE: Q-LEARNING\nWe showcase the integration of the Pyndri indexing library [9]\nand pytrec_eval within the OpenAI Gym [1], a reinforcement\nlearning library, for the task of query expansion. In particular, we\nuse Pyndri to rank documents according to a textual query and\nsubsequently evaluate the obtained ranking using pytrec_eval.\nThe reinforcement learning agent navigates an environment where\nactions correspond to adding a term to the query. Rewards are given\nby an increase or decrease in evaluation measure (i.e., NDCG). The\ngoal is for the agent to learn a policy π∗ that optimizes the expected\nvalue of the total reward. For the purpose of this demonstration of\nsoftware interoperability, we synthesize a test collection in order to\n(1) limit the computational complexity that arises from real-world\ncollections, and (2) to give us the ability to create an unlimited\nnumber of training queries and relevance judgments.\nDocument collection.We construct a synthetic document collec-\ntion D, of a given size |D | = 100, following the principles laid out\nby Tague et al. [8]. For a given vocabulary size |V | = 10,000, we\nconstruct vocabulary V consisting of symbolic tokens. We sample\ncollection-wide unigram (|V | parameters) and bigram (|V | 2 param-\neters) pseudo counts from an exponential distribution (λ = 1.0).\nThis incorporates term specificity within our synthetic collection, as\nonly few term uni- and bigrams will be frequent and most will be in-\nfrequent. These pseudo counts will then serve as the concentration\nparameters of Dirichlet distributions from which we will sample\na uni- and bigram language model for every document. We create\n|D | documents as follows. For every document d , given the average\ndocument length µd = 200, we sample its document size, |d |, from a\nPoisson with mean µd . We then sample two language models—one\nfor unigrams P (w | d ) and another for bigrams P ((x ,y) | d )—from\na Dirichlet distribution where the concentration parameters we\ndefined earlier for the whole collection. The document is then con-\nstructed as follows. Until we have reached |d | tokens, we repeat the\nfollowing: (a) sample an n-gram size from a predefined probability\ndistribution (P (n = 1) = 0.9, P (n = 2) = 0.1), and subsequently,\n(b) sample an n-gram from the corresponding language model. We\ntruncate a document if it exceeds its pre-defined length |d |.\nQuery collection. Once we obtained our synthetic document col-\nlection D, we proceed by constructing our query set Q , of a given\nsize |Q | = 100,000, as follows. For every query q to be constructed,\nwe select r = 5 documents uniformly at random from D and de-\nnote these as the set of relevant documents Rq ⊂ D for query q.\nGiven the average query length µq = 3, the length of query q, |q |, is\nthen sampled from a Poisson distribution with mean µq . We write\nP\n(\nw | Rq\n)\nand P (w | D) to denote the empirical language models\n00h 00m 00s 02h 46m 40s 05h 33m 20s 08h 20m 00s 11h 06m 40s 13h 53m 20s 16h 40m 00s\nWall-clock time\n0.00\n0.05\n0.10\nA\nvg\n.\nre\nw\nar\nd\n(∆\nN\nD\nC\nG\n)\nFigure 3: Average reward (∆NDCG) obtained by the Q-learning algorithm over time while training the reinforcement learning\nagent. The agent learns to select vocabulary terms that improve retrieval effectiveness for the set of 100k training queries.\nestimated from concatenating the relevant documents for query\nq and from concatenating all documents in the collection D (i.e.,\nthe collection language model), respectively. The |q | terms of query\nq are sampled with replacement from P\n(\nw | Rq\n)\n(1.0 − P (w | D)),\nsuch that terms specific to Rq and uncommon in D are selected.\nEnvironment. For each query q, the environment is initialized to\nthe state where only the query terms are present. At any given\nstate, the agent can then choose to expand the query terms with\nany unigram term from the vocabulary V in addition to a null op-\neration action. Rankings are obtained by querying the Indri search\nengine using Pyndri, using a Dirichlet language model (µ = 2,500),\nand obtaining a ranking of the top-10 documents. The reward of\nchoosing an action is the ∆NDCG that is obtained by expanding\nthe query with the chosen term. As observation, the agent receives\na binary vector indicating which terms of the vocabulary V occur\nat least once in the current expanded query. After 5 actions—or a\nperfect NDCG (i.e., 1.0) is achieved—the episode terminates.\nReinforcement learning agent.We learn an optimal policy tab-\nular π∗ using Q-learning where the initial values of the Q (·) are\ninitialized to zero. We set the learning rate α = 0.1 and the discount\nfactor γ = 0.95. During learning, we maintain an ϵ-greedy strategy\nwith ϵ = 0.05. Fig. 3 shows the average reward obtained while train-\ning an agent on the reinforcement learning problem defined above.\nThe average reward obtained by the agent increases over time. In\nparticular, this example showcases that different IR libraries (Pyn-\ndri, pytrec_eval) can easily be integrated with machine learning\nlibraries (OpenAI Gym) to quickly prototype ideas. An essential\npart here is that expensive operations (i.e., ranking and evaluation)\nare performed in efficient low-level languages, whereas prototyping\noccurs in the high-level Python scripting language. All code used\nin this example is available under the MIT open-source license.3\n5 CONCLUSIONS\nIn this paper we introduced pytrec_eval, a Python interface to\ntrec_eval. pytrec_eval builds upon the trec_eval source code\nand exposes a Python-first interface to the trec_eval evaluation\ntoolkit as a native Python extension. This allows for convenient and\nfast invocation of IR evaluation measures directly from Python. We\nshowed that pytrec_eval is around one order of magnitude faster\nthan invoking trec_eval in a serialize-invoke-parse workflow as it\navoids the costs associated with (1) the serialization of the rankings\nto storage, and (2) operation system context switching. Compared\nto a native Python implementation of NDCG, pytrec_eval is ap-\nproximately twice as fast for practically-sized rankings (100 to 1,000\n3The reinforcement learning code can be found in the examples sub-directory of the\npytrec_eval repository; see the footnote on the first page.\ndocuments). In addition, we showcased the integration of Pyndri\n[9] and pytrec_eval within the OpenAI Gym [1] and showed that\nall three modules can be combined to quickly prototype ideas.\nIn this paper, we used a tabular function during Q-learning; other\nfunctional forms—such as a deep neural network—can also be used.\nPyndri and pytrec_eval expose common IR operations through\na convenient Python interface. Beyond the convenience that both\nmodules provide, an important design principle is that expensive\noperations (e.g., indexing, ranking) are performed using efficient\nlow-level languages (e.g., C), while Python takes on the role of an in-\nstructor that links the expensive operations. Future work consists of\nexposing more IR operations as Python libraries and allowing more\ninteroperability amongst modules. For example, currently Pyndri\nconverts its internal Indri structures to Python structures, which\nare then again converted back to internal trec_eval structures\nby pytrec_eval. A closer integration of Pyndri and pytrec_eval\ncould result in even faster execution times as both can communicate\ndirectly—in cases where one is only interested in the evaluation\nmeasures and not the rankings—rather than through Python.\nAcknowledgements. This research was supported by Ahold Del-\nhaize, Amsterdam Data Science, the Bloomberg Research Grant\nprogram, the China Scholarship Council, the Criteo Faculty Re-\nsearch Award program, Elsevier, the European Community’s Sev-\nenth Framework Programme (FP7/2007-2013) under grant agree-\nment nr 312827 (VOX-Pol), the Google Faculty Research Awards\nprogram, the Microsoft Research Ph.D. program, the Netherlands\nInstitute for Sound and Vision, the Netherlands Organisation for\nScientific Research (NWO) under project nrs CI-14-25, 652.002.001,\n612.001.551, 652.001.003, and Yandex. All content represents the\nopinion of the authors, which is not necessarily shared or endorsed\nby their respective employers and/or sponsors.\nREFERENCES\n[1] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and\nW. Zaremba. OpenAI gym, 2016.\n[2] D. Harman. Information retrieval evaluation. Synthesis Lectures on Information\nConcepts, Retrieval, and Services, 3(2):1–119, 2011.\n[3] H. Koepke. Why python rocks for research. https://www.stat.washington.edu/\n~hoytak/_static/papers/why-python.pdf, 2010. Accessed February 12, 2018.\n[4] D. Li and E. Kanoulas. Bayesian optimization for optimizing retrieval systems. In\nWSDM. ACM, February 2018.\n[5] NIST. Text retrieval conference, 1992–2017.\n[6] L. Prechelt. An empirical comparison of seven programming languages. Computer,\n33(10):23–29, Oct. 2000.\n[7] M. Sanderson. Test collection based evaluation of information retrieval systems.\nFoundations and Trends in Information Retrieval, 4(4):247–375, 2010.\n[8] J. Tague, M. Nelson, and H. Wu. Problems in the simulation of bibliographic\nretrieval systems. In SIGIR, pages 236–255. ACM, June 1980.\n[9] C. Van Gysel, E. Kanoulas, and M. de Rijke. Pyndri: a python interface to the indri\nsearch engine. In ECIR, pages 744–748. Springer, April 2017.\n",
    "Link": "http://arxiv.org/abs/1805.01597"
}